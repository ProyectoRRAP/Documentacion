<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
           "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta name="GENERATOR" content="TtH 4.03" />
 <style type="text/css"> div.p { margin-top: 7pt;}</style>
 <style type="text/css"><!--
 td div.comp { margin-top: -0.6ex; margin-bottom: -1ex;}
 td div.comb { margin-top: -0.6ex; margin-bottom: -.6ex;}
 td div.hrcomp { line-height: 0.9; margin-top: -0.8ex; margin-bottom: -1ex;}
 td div.norm {line-height:normal;}
 span.roman {font-family: serif; font-style: normal; font-weight: normal;} 
 span.overacc2 {position: relative;  left: .8em; top: -1.2ex;}
 span.overacc1 {position: relative;  left: .6em; top: -1.2ex;} --></style>
 <style type="text/css"><!--
 .tiny {font-size:30%;}
 .scriptsize {font-size:xx-small;}
 .footnotesize {font-size:x-small;}
 .smaller {font-size:smaller;}
 .small {font-size:small;}
 .normalsize {font-size:medium;}
 .large {font-size:large;}
 .larger {font-size:x-large;}
 .largerstill {font-size:xx-large;}
 .huge {font-size:300%;}
 --></style>
 
                 
<title>No Title</title>
</head>
<body><div>

<div class="p"><!----></div>

<h2>References</h2>

<dl>
 <dt>[]</dt><dd> (2014a).
 Beacon Controller.
 url: <a href="https://openflow.stanford.edu/display/Beacon/Home"><tt>https://openflow.stanford.edu/display/Beacon/Home</tt></a>, último
  acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014b).
 Floodlight Controller.
 url: <a href="http://www.projectfloodlight.org/floodlight/"><tt>http://www.projectfloodlight.org/floodlight/</tt></a>, último
  acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014c).
 Flowvisor Controller.
 url: <a href="https://github.com/OPENNETWORKINGLAB/flowvisor/wiki"><tt>https://github.com/OPENNETWORKINGLAB/flowvisor/wiki</tt></a>,
  último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014d).
 Helios Controller.
 url: <a href="http://www.nec.com/"><tt>http://www.nec.com/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014e).
 Jaxon Controller.
 url: <a href="http://jaxon.onuos.org/wp/?page_id=12"><tt>http://jaxon.onuos.org/wp/?page_id=12</tt></a>, último acceso
  Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014f).
 Maestro Controller.
 url: <a href="https://code.google.com/p/maestro-platform/"><tt>https://code.google.com/p/maestro-platform/</tt></a>, último
  acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014g).
 NodeFlow Controller.
 url: <a href="https://github.com/dreamerslab/node.flow"><tt>https://github.com/dreamerslab/node.flow</tt></a>, último acceso
  Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014h).
 NOX Controller.
 url: <a href="http://www.noxrepo.org/nox/about-nox/"><tt>http://www.noxrepo.org/nox/about-nox/</tt></a>, último acceso
  Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014i).
 OpenDaylight Controller.
 url: <a href="http://www.opendaylight.org/"><tt>http://www.opendaylight.org/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014j).
 OpenMUL Controller.
 url: <a href="http://kulcloud.wordpress.com/"><tt>http://kulcloud.wordpress.com/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014k).
 POX Controller.
 url: <a href="http://www.noxrepo.org/pox/about-pox/"><tt>http://www.noxrepo.org/pox/about-pox/</tt></a>, último acceso
  Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014l).
 Routeflow Controller.
 url: <a href="http://cpqd.github.io/RouteFlow/"><tt>http://cpqd.github.io/RouteFlow/</tt></a>, último acceso Junio
  2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014m).
 SNAC Controller.
 url: <a href="http://www.openflowhub.org/display/Snac/SNAC+Home"><tt>http://www.openflowhub.org/display/Snac/SNAC+Home</tt></a>, último
  acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014n).
 SNAC Controller.
 url: <a href="http://osrg.github.io/ryu/"><tt>http://osrg.github.io/ryu/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2014o).
 Trema Controller.
 url: <a href="http://trema.github.io/trema/"><tt>http://trema.github.io/trema/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 Asia Pacific Advanced Network(APAN).
 url: <a href="https://www.apan.net/"><tt>https://www.apan.net/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 BIRD internet routing daemon.
 url: <a href="http://www.nongnu.org/quagga/"><tt>http://www.nongnu.org/quagga/</tt></a>, último acceso Junio 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 CANARIE sitio oficial, CA*net4.
 url: <a href="http://www.canarie.ca/"><tt>http://www.canarie.ca/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 Centec Networks whitebox solutions for sdn.
 url: <a href="https://www.opennetworking.org/listings/centec-networks"><tt>https://www.opennetworking.org/listings/centec-networks</tt></a>,
  ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 GÉANT Project sitio oficial.
 url: <a href="http://www.geant.net/Pages/default.aspx"><tt>http://www.geant.net/Pages/default.aspx</tt></a>, último acceso
  Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 HP productos compatibles con openflow.
 url:
  <a href="http://h17007.www1.hp.com/uy/es/solutions/technology/openflow/index.aspx"><tt>http://h17007.www1.hp.com/uy/es/solutions/technology/openflow/index.aspx</tt></a>,
  ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 Internet 2 sitio oficial.
 url: <a href="http://www.internet2.edu/"><tt>http://www.internet2.edu/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015a).
 Lista de correos netfpga-nf10g-beta.
 url: <a href="cl-netfpga-nf10g-beta@lists.cam.ac.uk"><tt>cl-netfpga-nf10g-beta@lists.cam.ac.uk</tt></a>, ultimo acceso Mayo
  2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 Lista de productos compatibles con sdn.
 url: <a href="https://www.opennetworking.org/products-listing"><tt>https://www.opennetworking.org/products-listing</tt></a>, ultimo
  acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015b).
 Manual de ejecucion test rldram netfpga-10g.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/RLDRAM%20Test%20Manual"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/RLDRAM%20Test%20Manual</tt></a>,
  ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 Mininet - Entorno de emulacion virtual.
 url: <a href="http://mininet.org/"><tt>http://mininet.org/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 NetFPGA - PCIE Programming.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/PCIE-Programming"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/PCIE-Programming</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 NetFPGA - Production Test Manual.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/Production%20Test%20Manual"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/Production%20Test%20Manual</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015c).
 NetFPGA - Publicaciones academicas.
 url: <a href="http://netfpga.org/site/#/publications/"><tt>http://netfpga.org/site/#/publications/</tt></a>, último acceso
  Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015d).
 NetFPGA-1G - Lista de proyectos para la plataforma.
 url:
  <a href="http://netfpga.org/site/#/systems/4netfpga-1g/applications/"><tt>http://netfpga.org/site/#/systems/4netfpga-1g/applications/</tt></a>, último
  acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015e).
 NetFPGA Descripción de RLDRAM Test NetFPGA-10G.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G%20RLDRAM%20Test"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G%20RLDRAM%20Test</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015f).
 NetFPGA Descripción de Test de Producción NetFPGA-10.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Production-Tes"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Production-Tes</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015a).
 NetFPGA Learning CAM Switch.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Learning-CAM-Switch"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Learning-CAM-Switch</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015g).
 NetFPGA Manual de Test de Produción NetFPGA-10.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/Production%20Test%20Manual"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/Production%20Test%20Manual</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015h).
 NetFPGA Plataform.
 url: <a href="http://netfpga.org/index.htm"><tt>http://netfpga.org/index.htm</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015b).
 NetFPGA Reference NIC.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Reference-NIC"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Reference-NIC</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015c).
 NetFPGA Reference Router.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Reference-Router"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Reference-Router</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 Ofelia openflow in europe: Linking infrastructure and applications.
 url: <a href="http://www.fp7-ofelia.eu/"><tt>http://www.fp7-ofelia.eu/</tt></a>, ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 Open vSwitch Repositorio de código fuente.
 url: <a href="https://github.com/openvswitch/ovs"><tt>https://github.com/openvswitch/ovs</tt></a>, último acceso Mayo
  2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 Pica8 whitebox sdn.
 url: <a href="http://www.pica8.com/"><tt>http://www.pica8.com/</tt></a>, ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd> (2015).
 Quagga routing suite.
 url: <a href="http://bird.network.cz/"><tt>http://bird.network.cz/</tt></a>, último acceso Junio 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Andersson, L., AB, A., and Minei, I. (1996).
 LDP Specification rfc5036.
 <em>Network Working Group</em>.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Azodolmolky, S. (October 2013).
 <em>Software Defined Networking with OpenFlow, Get hands on withe
  the platforms and development tools used to build OpenFlow network
  applications.</em>
 Packt Publishing, 35 Livery Street, published by packt publishing
  ltd. edition.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Caesar, M., Caldwell, D., Feamster, N., Rexford, J., Shaikh, A., and van&nbsp;der
  Merwe, J. (2005).
 Design and implementation of a routing control platform.
 In <em>Proceedings of the 2nd conference on Symposium on Networked
  Systems Design &amp; Implementation-Volume 2</em>, pages 15-28. USENIX Association.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Campbell, A.&nbsp;T., Katzela, I., Miki, K., and Vicente, J. (1999).
 Open signaling for atm, internet and mobile networks (opensig'98).
 <em>ACM SIGCOMM Computer Communication Review</em>, 29(1):97-108.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Casado, M., Freedman, M.&nbsp;J., Pettit, J., Luo, J., McKeown, N., and Shenker, S.
  (2007).
 Ethane: taking control of the enterprise.
 In <em>ACM SIGCOMM Computer Communication Review</em>, volume&nbsp;37, pages
  1-12. ACM.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Case, J., Fedor, M., Schoffstall, M., and Davin, C. (1989).
 A simple network management protocol (snmp).

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Doria, A., Hellstrand, F., Sundell, K., and Worster, T. (2002).
 General switch management protocol (gsmp) v3.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Doria, A., Salim, J.&nbsp;H., Haas, R., Khosravi, H., Wang, W., Dong, L., Gopal, R.,
  and Halpern, J. (2010).
 Forwarding and control element separation (forces) protocol
  specification.
 <em>Internet Requests for Comments, RFC Editor, RFC</em>, 5810.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>E.&nbsp;Rosen, Y.&nbsp;R. (2006).
 Bgp/mpls ip virtual private networks (vpns).

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Feamster, N., Balakrishnan, H., Rexford, J., Shaikh, A., and Van Der&nbsp;Merwe, J.
  (2004).
 The case for separating routing from routers.
 In <em>Proceedings of the ACM SIGCOMM workshop on Future directions
  in network architecture</em>, pages 5-12. ACM.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Feamster, N., Rexford, J., and Zegura, E. (2013).
 The road to sdn.
 <em>Queue</em>, 11(12):20.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Greenberg, A., Hjalmtysson, G., Maltz, D.&nbsp;A., Myers, A., Rexford, J., Xie, G.,
  Yan, H., Zhan, J., and Zhang, H. (2005).
 A clean slate 4d approach to network control and management.
 <em>ACM SIGCOMM Computer Communication Review</em>, 35(5):41-54.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Gude, N., Koponen, T., Pettit, J., Pfaff, B., Casado, M., McKeown, N., and
  Shenker, S. (2008).
 Nox: towards an operating system for networks.
 <em>ACM SIGCOMM Computer Communication Review</em>, 38(3):105-110.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Hassas&nbsp;Yeganeh, S. and Ganjali, Y. (2012).
 Kandoo: a framework for efficient and scalable offloading of control
  applications.
 In <em>Proceedings of the first workshop on Hot topics in software
  defined networks</em>, pages 19-24. ACM.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Heller, B., Sherwood, R., and McKeown, N. (2012).
 The controller placement problem.
 In <em>Proceedings of the first workshop on Hot topics in software
  defined networks</em>, pages 7-12. ACM.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Jain, S., Kumar, A., Mandal, S., Ong, J., Poutievski, L., Singh, A., Venkata,
  S., Wanderer, J., Zhou, J., Zhu, M., et&nbsp;al. (2013).
 B4: Experience with a globally-deployed software defined wan.
 In <em>ACM SIGCOMM Computer Communication Review</em>, volume&nbsp;43, pages
  3-14. ACM.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Kompella, K. and Rekhter, Y. (2007).
 Virtual private lan service (vpls) using bgp for auto-discovery and
  signaling.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Koponen, T., Casado, M., Gude, N., and Stribling, J. (2014).
 Distributed control platform for large-scale production networks.
 US Patent 8,830,823.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Lasserre, M. and Kompella, V. (2007).
 Virtual private lan service (vpls) using label distribution protocol
  (ldp) signaling.
 Technical report, RFC 4762, January.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Levin, D., Wundsam, A., Heller, B., Handigol, N., and Feldmann, A. (2012).
 Logically centralized?: state distribution trade-offs in software
  defined networks.
 In <em>Proceedings of the first workshop on Hot topics in software
  defined networks</em>, pages 1-6. ACM.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Malkin, G. (1994).
 Rip version 2-carrying additional information.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>McKeown, N., Anderson, T., Balakrishnan, H., Parulkar, G., Peterson, L.,
  Rexford, J., Shenker, S., and Turner, J. (2008).
 Openflow: enabling innovation in campus networks.
 <em>ACM SIGCOMM Computer Communication Review</em>, 38(2):69-74.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Migration Working Group (2014).
 <em>ONF Migration Use Cases and Methods</em>.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Moore, J.&nbsp;T. and Nettles, S.&nbsp;M. (2001).
 Towards practical programmable packets.
 In <em>Proceedings of the 20th Conference on Computer Communications
  (INFOCOM). Citeseer</em>. Citeseer.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Moy, J. (1998).
 rfc 2328: Ospf version 2.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Nadeau, T., Srinivasan, C., Bloomberg, L., and Viswanathan, A. (2004).
 Multiprotocol label switching mpls forwarding equivalence class to
  next hop label forwarding entry fec-to-nhlfe management information base mib.
 Technical report, RFC 3814.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>NetFPGA (2015a).
 Compendio de referencia de SDN.
 url: <a href="https://sites.google.com/site/sdnreadinglist/"><tt>https://sites.google.com/site/sdnreadinglist/</tt></a>, último
  acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>NetFPGA (2015b).
 NetFPGA sitio del proyecto.
 url: <a href="http://www.netfpga.org/"><tt>http://www.netfpga.org/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Nunes, B., Mendonca, M., Nguyen, X., Obraczka, K., and Turletti, T. (2014).
 A survey of software-defined networking: Past, present, and future of
  programmable networks.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>ONF (Agosto 2012).
 Specification openflow switch, pages 70-71.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Rexford, J., Greenberg, A., Hjalmtysson, G., Maltz, D.&nbsp;A., Myers, A., Xie, G.,
  Zhan, J., and Zhang, H. (2004).
 Network-wide decision making: Toward a wafer-thin control plane.
 In <em>Proc. HotNets</em>, pages 59-64.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Rose, M.&nbsp;T. and McCloghrie, K. (1990).
 Structure and identification of management information for
  tcp/ip-based internets.
 <em>Structure</em>.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Rosen, E., Rekhter, Y., Tappan, D., Farinacci, D., Fedorkow, G., Li, T., and
  Conta, A. (2001a).
 Mpls label stack encoding.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Rosen, E., Viswanathan, A., Callon, R., et&nbsp;al. (2001b).
 Multiprotocol label switching architecture.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Rosen, E.&nbsp;C. and Rekhter, Y. (1999).
 Bgp/mpls vpns.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Routing, O. I.-I. I.-d. (1990).
 Protocol, 1990. internet engineering task force.
 Technical report, RFC 1142.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Sherwood, R., Chan, M., Covington, A., Gibb, G., Flajslik, M., Handigol, N.,
  Huang, T.-Y., Kazemian, P., Kobayashi, M., Naous, J., et&nbsp;al. (2010).
 Carving research slices out of your production networks with
  openflow.
 <em>ACM SIGCOMM Computer Communication Review</em>, 40(1):129-130.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Tennenhouse, D.&nbsp;L., Smith, J.&nbsp;M., Sincoskie, W.&nbsp;D., Wetherall, D.&nbsp;J., and
  Minden, G.&nbsp;J. (1997).
 A survey of active network research.
 <em>Communications Magazine, IEEE</em>, 35(1):80-86.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Tennenhouse, D.&nbsp;L. and Wetherall, D.&nbsp;J. (2002).
 Towards an active network architecture.
 In <em>DARPA Active NEtworks Conference and Exposition, 2002.
  Proceedings</em>, pages 2-15. IEEE.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Tootoonchian, A. and Ganjali, Y. (2010).
 Hyperflow: A distributed control plane for openflow.
 In <em>Proceedings of the 2010 internet network management
  conference on Research on enterprise networking</em>, pages 3-3. USENIX
  Association.

<div class="p"><!----></div>
</dd>
 <dt>[]</dt><dd>Yu, M., Rexford, J., Freedman, M.&nbsp;J., and Wang, J. (2011).
 Scalable flow-based networking with difane.
 <em>ACM SIGCOMM Computer Communication Review</em>, 41(4):351-362.</dd>
</dl>
 
<div class="p"><!----></div>



<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>




<div class="p"><!----></div>




<div class="p"><!----></div>

<div class="p"><!----></div>
 

<h1 align="center">Routers Reconfigurables de Altas Prestaciones </h1>

<div class="p"><!----></div>


<div class="p"><!----></div>

<div class="p"><!----></div>

<h3 align="center">Autores: <br /><span class="roman">Emiliano Viotti <br />Rodrigo Amaro </span> <br /><br /><br /> Supervisores: <br /><span class="roman">Eduardo Grampin <br />Martin Giachino</span> </h3>

<div class="p"><!----></div>


<div class="p"><!----></div>


<div class="p"><!----></div>

<div class="p"><!----></div>


<div class="p"><!----></div>
 

<div class="p"><!----></div>
 


<div class="p"><!----></div>

<div class="p"><!----></div>
   

<div class="p"><!----></div>

<div class="p"><!----></div>
                 
<div class="p"><!----></div>

<div class="p"><!----></div>

<h1>Índice general </h1> empty


<a href="#tth_chAp1"
>1&nbsp; Introducci&#243;n</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc1.1"
>1.1&nbsp; Motivaci &#243;n</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc1.2"
>1.2&nbsp; Definici &#243;n del Problema</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc1.3"
>1.3&nbsp; Objetivos</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc1.4"
>1.4&nbsp; Resultados Esperados</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc1.5"
>1.5&nbsp; Estructura del documento</a><br />
<a href="#tth_chAp2"
>2&nbsp; Estado del Arte</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.1"
>2.1&nbsp; Antecedentes de SDN</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.1.1"
>2.1.1&nbsp; Open Signaling</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.1.2"
>2.1.2&nbsp; Active Networking</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.1.3"
>2.1.3&nbsp; Routing Control Plataorm</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.1.4"
>2.1.4&nbsp; ForCES Protocol</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.1.5"
>2.1.5&nbsp; 4D Project</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.2"
>2.2&nbsp; Software Defined Networking</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.3"
>2.3&nbsp; Arquitecturas basadas en SDN</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.3.1"
>2.3.1&nbsp; OpenFlow</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.3.2"
>2.3.2&nbsp; Reglas OpenFlow</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.3.3"
>2.3.3&nbsp; Acciones OpenFlow</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.3.4"
>2.3.4&nbsp; Controlador OpenFlow</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.4"
>2.4&nbsp; Herramientas SDN/OpenFlow disponibles</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.4.1"
>2.4.1&nbsp; Controlador</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.4.2"
>2.4.2&nbsp; Mininet y Mini Next</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.5"
>2.5&nbsp; Aplicaciones de SDN</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.6"
>2.6&nbsp; Casos de  &#233;xito de SDN</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.7"
>2.7&nbsp; Red Privada Virtual</a><br />


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.7.1"
>2.7.1&nbsp; Redes Privadas en Uruguay y proyecciones para la RAU</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.8"
>2.8&nbsp; Multiprotocol Label Switching</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc2.9"
>2.9&nbsp; NetFPGA</a><br />
<a href="#tth_chAp3"
>3&nbsp; An&#225;lisis del problema</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc3.1"
>3.1&nbsp; Definici &#243;n de requerimientos</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc3.2"
>3.2&nbsp;   Por qu&#233; utilizar SDN?</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc3.3"
>3.3&nbsp;   Por qu&#233; utilizar NetFPGA?</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc3.4"
>3.4&nbsp; Qu&#233; arquitectura basada en SDN utilizar</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc3.5"
>3.5&nbsp;   Qu&#233; implementaci&#243;n de Controlador OpenFlow utilizar?</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc3.6"
>3.6&nbsp; Alternativas de dise&#241;o para el router</a><br />
<a href="#tth_chAp4"
>4&nbsp; Dise&#241;o e Implementaci&#243;n del prototipo</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.1"
>4.1&nbsp; Lineamientos generales</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.1.1"
>4.1.1&nbsp; Algoritmo de Ruteo</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.1.2"
>4.1.2&nbsp; Algoritmo de distribuci &#243;n de etiquetas</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.2"
>4.2&nbsp; Arquitectura del Prototipo</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.3"
>4.3&nbsp; RAU-Switch</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.3.1"
>4.3.1&nbsp; Plataforma de la PC</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.3.2"
>4.3.2&nbsp; Sistema Operativo</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.3.3"
>4.3.3&nbsp; Hardware NetFPGA</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.3.4"
>4.3.4&nbsp; Open vSwitch</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.3.5"
>4.3.5&nbsp; Quagga</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.3.6"
>4.3.6&nbsp; Agente SNMP</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.4"
>4.4&nbsp; Entidad Controlador</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.4.1"
>4.4.1&nbsp; Software de Control Ryu</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.4.2"
>4.4.2&nbsp; Quagga</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.4.3"
>4.4.3&nbsp; LSDB Sync</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc4.4.4"
>4.4.4&nbsp; Administrador SNMP</a><br />
<a href="#tth_chAp5"
>5&nbsp; RAUFlow</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.1"
>5.1&nbsp; An&#225;lisi de requerimientos</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.2"
>5.2&nbsp; Modelado de la realidad</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.3"
>5.3&nbsp; Relevamiento de casos de uso</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.4"
>5.4&nbsp; Arquitectura de RauFlow</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.4.1"
>5.4.1&nbsp; Capa de aplicaciones Ryu</a><br />

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.4.2"
>5.4.2&nbsp; Capa de Negocios</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.4.3"
>5.4.3&nbsp; Capa de Presentaci &#243;n</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.5"
>5.5&nbsp; Implementaci&#243;n</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.5.1"
>5.5.1&nbsp; Clasificaci &#243;n de tr&#225;fico</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.5.2"
>5.5.2&nbsp; Implementaci &#243;n de Servicio</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.5.3"
>5.5.3&nbsp; Algoritmo de ruteo</a><br />

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.5.4"
>5.5.4&nbsp; Algoritmo de distribuci &#243;n de etiquetas</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.5.5"
>5.5.5&nbsp; Actualizaci&#243;n de la topologia</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc5.5.6"
>5.5.6&nbsp; Ciclo de vida de un Nodo</a><br />
<a href="#tth_chAp6"
>6&nbsp; Laboratorio de pruebas</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc6.1"
>6.1&nbsp; Definici &#243;n del Laboratorio</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc6.2"
>6.2&nbsp; VPN de capa 3</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc6.2.1"
>6.2.1&nbsp; Escenario 1 - Red Privada Multipunto</a><br />




&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc6.2.2"
>6.2.2&nbsp; Escenario 2</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc6.3"
>6.3&nbsp; Pruebas</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc6.3.1"
>6.3.1&nbsp; Asignaci &#243;n de etiquetas</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc6.3.2"
>6.3.2&nbsp; Clasificaci &#243;n de tr&#225;fico</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc6.3.3"
>6.3.3&nbsp; Algoritmo de ruteo din &#225;mico</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc6.3.4"
>6.3.4&nbsp; Numeraciones superpuestas</a><br />
<a href="#tth_chAp7"
>7&nbsp; Ejecuci &#243;n del proyecto</a><br />
<a href="#tth_chAp8"
>8&nbsp; Conclusiones</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc8.1"
>8.1&nbsp; Conclusiones</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEc8.2"
>8.2&nbsp; Trabajo a futuro</a><br />


&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcA.1"
>A.1&nbsp; Problema con la herramienta pcieprog y ReferenceNIC</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcA.2"
>A.2&nbsp; Error en el driver para el proyecto ReferenceNIC</a><br />

&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcB.1"
>B.1&nbsp; Problemas con SFPs y Patchcoords</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcB.2"
>B.2&nbsp; Desprogramaci&#243;n del hardware NetFPGA</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcB.3"
>B.3&nbsp; Falta de licencias para suite de Xilinx ISE SDK</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcB.4"
>B.4&nbsp; Falta de driver para cable JTAG Xilinx</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcB.5"
>B.5&nbsp; Desaf&#237;os con Open vSwitch</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcB.6"
>B.6&nbsp; MPLS Linux y Quagga LDP</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcB.7"
>B.7&nbsp; Instalaci&#243;n de Sistema Operativo</a><br />

&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcC.1"
>C.1&nbsp; Cabezal OpenFlow versi&#243;n 1.3</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcC.2"
>C.2&nbsp; OpenFlow v1.3 atributos soportados para definir reglas</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcC.3"
>C.3&nbsp; OpenFlow v1.3 atributos soportados para definir acciones</a><br />

&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcD.1"
>D.1&nbsp; Archivos de configuraci&#243;n Quagga</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcD.2"
>D.2&nbsp; Archivos de configuraci&#243;n de Open vSwitch</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcD.3"
>D.3&nbsp; Otros scripts de configuraci&#243;n</a><br />
<a href="#tth_chApI"
>I&nbsp; Manual de construcci &#243;n de RAU-Switch</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.1"
>I.1&nbsp; Resumen</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.2"
>I.2&nbsp; Plataforma utilizada</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.3"
>I.3&nbsp; Instalaci &#243;n y configuraci &#243;n</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.3.1"
>I.3.1&nbsp; Instalaci &#243;n de un sistema operativo</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.3.2"
>I.3.2&nbsp; Instalaci &#243;n de librer &#237;as y dependencias</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.3.3"
>I.3.3&nbsp; Instalaci &#243;n de suite de desarrollo Xilinx ISE SDK</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.3.4"
>I.3.4&nbsp; Configuraci &#243;n del entorno de desarrollo NetFPGA</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.3.5"
>I.3.5&nbsp; Pruebas de aceptaci &#243;n del hardware NetFPGA</a><br />

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.3.6"
>I.3.6&nbsp; Programaci &#243;n de la tarjeta</a><br />


&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.3.7"
>I.3.7&nbsp; Instalaci &#243;n de Open vSwitch</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_sEcI.3.8"
>I.3.8&nbsp; Instalaci &#243;n de software de ruteo Quagga</a><br />



<div class="p"><!----></div>

<h1>Índice de figuras </h1>

&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.1"
>2.1&nbsp;  Capas de la arquitectura 4D</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.2"
>2.2&nbsp;  Arquitectura de SDN - Capas l &#243;gicas</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.3"
>2.3&nbsp;  Estructura de un switch OpenFlow</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.4"
>2.4&nbsp;  Visi &#243;n esquem &#225;tica del funcionamiento de un Switch OpenFlow</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.5"
>2.5&nbsp;  Ciclo de vida de un paquete en pipe OpenFlow</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.6"
>2.6&nbsp;  OF 1.0 Matching Fields</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.7"
>2.7&nbsp;  OF 1.3.3 Matching Fields</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.8"
>2.8&nbsp;  OF 1.3.3 Matching Fields</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.9"
>2.9&nbsp;  Esquema de una VPN Punto a Punto</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg2.10"
>2.10&nbsp;  Esquema de una VPN Multipunto</a><br />


&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg4.1"
>4.1&nbsp;  OpenSourceRArch0</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg4.2"
>4.2&nbsp;  RAU-Switch - diagrama de componentes</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg4.3"
>4.3&nbsp;  OpenSourceRArch3</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg4.4"
>4.4&nbsp;  Vista l&#243;gica ampliada del prototipo</a><br />

&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg5.1"
>5.1&nbsp;  Modelo de datos</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg5.2"
>5.2&nbsp;  Casos de Uso de RAUFlow</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg5.3"
>5.3&nbsp;  Vista l&#243;gica</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg5.4"
>5.4&nbsp;  Algoritmo de ruteo Centralizado sin restricciones (SPF)</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg5.6"
>5.6&nbsp;  Algoritmo de distribuci &#243;n de etiquetas</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg5.8"
>5.8&nbsp;  Algoritmo de actualizaci &#243;n de la topolog&#237;a </a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg5.11"
>5.11&nbsp;  Ciclo de vida de un Nodo</a><br />

&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg6.1"
>6.1&nbsp;  Laboratorio de pruebas - Topolog&#237;a</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg6.2"
>6.2&nbsp;  Laboratorio de pruebas - Costos de la topolog&#237;a</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg6.3"
>6.3&nbsp;  VPN de capa 3 - Escenario 1</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg6.4"
>6.4&nbsp;  Escenario 1 - Caminos para servicios</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg6.5"
>6.5&nbsp;  Cambiar por imagen correcta</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg6.6"
>6.6&nbsp;  Escenario 1 - Caminos para servicios recalculados</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg6.7"
>6.7&nbsp;  Cambiar por imagen correcta</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIg6.8"
>6.8&nbsp;  VPN de capa 3 - Escenario 2</a><br />




&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgB.1"
>B.1&nbsp;  OVSInterfaces</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgB.2"
>B.2&nbsp;  OVSInterfaces2</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.1"
>I.1&nbsp;  Jumper J16</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.2"
>I.2&nbsp;  SW9 en posicion PCIe</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.3"
>I.3&nbsp;  Posiciones DIP SW1 SW2 SW6 SW10</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.4"
>I.4&nbsp;  Conector ATX</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.5"
>I.5&nbsp;  Cable samtec twinax</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.6"
>I.6&nbsp;  Cables samtec twinax conectados</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.7"
>I.7&nbsp;  Cable serial puerto COM</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.8"
>I.8&nbsp;  Cable JTAG</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.9"
>I.9&nbsp;  Tarjeta NetFPGA esquema de instalaci &#243;n</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.10"
>I.10&nbsp;  Tarjeta NetFPGA instalada en una PC</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.11"
>I.11&nbsp;  Reserva de bloque de memoria</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.12"
>I.12&nbsp;  Programaci &#243;n Impact</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.13"
>I.13&nbsp;  Reserva bloque de memoria</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.14"
>I.14&nbsp;  Test de Producci &#243;n salida esperada</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_fIgI.15"
>I.15&nbsp;  Configuraci &#243;n SW3 JTag/PCIe programing</a><br />


<div class="p"><!----></div>

<h1>Índice de cuadros </h1>

&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_tAb2.1"
>2.1&nbsp;  Controladores disponibles y sus caracter &#237;sticas (Basada en una tabla similar y m &#225;s reducida en [<a href="#StateOfArt1" name="CITEStateOfArt1">702014Nunes et&nbsp;al.</a>]) </a><br />

&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_tAb3.1"
>3.1&nbsp;  OpenFlow NetFPGA vs ReferenceNIC - Ventajas</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_tAb3.2"
>3.2&nbsp;  OpenFlow NetFPGA vs ReferenceNIC - Desventajas</a><br />



&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_tAb6.1"
>6.1&nbsp;  CU1 - Escenario 1, servicios creados</a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_tAb6.2"
>6.2&nbsp;  CU1 - Escenario 1, servicios extra</a><br />







&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_tAbI.1"
>I.1&nbsp;  RAU-Switch, especificaciones de hardware </a><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="#tth_tAbI.2"
>I.2&nbsp;  RAU-Switch, especificaciones de software </a><br />


<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>


<div class="p"><!----></div>

<div class="p"><!----></div>
 <a name="tth_chAp1"></a><h1>
Capítulo 1 <br />Introducci&#243;n</h1>

<div class="p"><!----></div>
     <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Motivación</h2>

En la actualidad, sobre la red de Internet conviven aplicaciones académicas, comerciales y particulares con iguales prioridades. Este escenario no es apropiado para actividades de experimentación, investigación y estudio de nuevas herramientas a gran escala. A su vez los proveedores de servicios sobre Internet, contando con que no todos sus clientes hacen uso de su infraestructura a la vez, venden m&#225;s servicios de los que su infraestructura realmente puede soportar, si se tienen por ejemplo que garantizar un ancho de banda para cada servicio. Esta situación en particular es comúnmente conocida como “sobreventa” del ancho de banda y se agrava en horas de mayor uso de Internet (horas pico), lo cual es crítico cuando se piensa en aplicaciones que requieren de niveles de calidad de servicio garantizados.

<div class="p"><!----></div>
Por estas razones Internet no es la red más apropiada para su utilización en el contexto académico. Es necesaria otra red diferente, que permita proveer de conectividad y a su vez soportar actividades experimentales como desplegar nuevos servicios y protocolos.<br />

<div class="p"><!----></div>
En este contexto y en todo el mundo se vienen desarrollando desde mediados de la década de los 90’s las redes académicas avanzadas de alta velocidad, con el objetivo de proveer de una red la cual posibilite a docentes e investigadores colaborar en aplicaciones altamente demandantes de ancho de banda (educación a distancia, transferencia de grandes cantidades de información, acceso a equipos remotos, telemedicina, etc.), sin competir por este recurso con aplicaciones de naturaleza comercial.

<div class="p"><!----></div>
En Estados Unidos por ejemplo, el proyecto que lidera este desarrollo es Internet2[<a href="#Internet2" name="CITEInternet2">Int, 2015</a>], en Canadá es el proyecto CA*net4[<a href="#Canarie" name="CITECanarie">Can, 2015</a>], en Europa el proyectos GÉANT[<a href="#GEANT" name="CITEGEANT">GEA, 2015</a>] y en Asia el proyecto APAN[<a href="#APAN" name="CITEAPAN">APA, 2015</a>]. Todas estas redes se encuentran a su vez conectadas entre sí, formando una gran red avanzada de alta velocidad de alcance mundial. En Latinoamérica, las redes académicas de Argentina, Brasil y Chile se encuentran integradas a Internet2.<br />

<div class="p"><!----></div>
En las &#250;ltimas dos d&#233;cadas no solo se han hecho esfuerzos para desarrollar las redes académicas nacionales, sino que también se ha trabajado arduamente en la construcción de acuerdos que permitan interconectar cada una de estas redes en una gran red académica de alcance global. En la historia de estos tratados, uno de los sucesos m&#225;s importantes puede ser lo que se denomin&#243; CAESAR (Connecting All European and South(Latin) American Researchers). Esta iniciativa surge tras la unión de las Redes Nacionales de Educación e Investigación (RNEI) de Portugal y España (FCNN y RedIRIS) y DANTE con el objetivo de analizar la viabilidad de interconectar directamente la red académica paneuropea GÉANT y sus equivalentes nacionales en América Latina.

<div class="p"><!----></div>

<div class="p"><!----></div>
Tras meses de trabajo esta unión decanta en lo que se denomin&#243; “Declaración de Toledo”, en donde se reconoce la necesidad de crear una red troncal regional en América Latina que eventualmente se pueda conectar a GÉANT. Apenas dos semanas después, las redes latinoamericanas se unen bajo su propia agrupación denominada CLARA(Cooperación Latino Americana de Redes Avanzadas), y bajo esta nueva figura se reúnen en Río de Janeiro (Brasil) para avanzar en los acuerdos adoptados en el marco de la reunión de Toledo. El día 16 de Julio del año 2002 todas las redes involucradas en CLARA adscribieron a la ya denominada “Declaración de Toledo”.<br />

<div class="p"><!----></div>
En Uruguay, la Universidad de la República a través del Servicio Central de Informática(SeCIU), es la fundadora de la Red Académica Uruguaya(RAU). Este proyecto persigue en el contexto de las redes académicas mundiales los siguientes objetivos: unir las Instituciones Nacionales Académicas, Universidades (pública y privadas) y Centros de Investigación del Uruguay, promover el desarrollo de Redes Académicas y Científicas donde ellas hagan falta, planificar y desarrollar una red nacional, incentivar la colaboración con iniciativas similares y conectar la RAU con Latinoamérica. 

<div class="p"><!----></div>
Actualmente entre facultades, escuelas, institutos y servicios de la Universidad de la República, así como diversas instituciones educativas y de investigación, son 37 las instituciones que forman parte de la RAU, las cuales hacen uso de los 153 nodos que la conforman para brindar servicios a mas de 6.500 docentes, 1.000 técnicos y 60.000 estudiantes en diferentes partes del país.

<div class="p"><!----></div>
Por otro lado, hace un tiempo ya que se viene trabajando en un proyecto para remplazar la infraestructura actual de la RAU por una red avanzada de altas prestaciones, que pueda brindar de mayores y mejores servicios a las instituciones que forman parte de la misma, así como conectarse a las demás redes académicas de latinoam&#233;rica. Este proyecto se denomin&#243; RAU2.  

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Definición del Problema</h2>

<div class="p"><!----></div>
El problema elegido como eje para el desarrollo de este trabajo es la 
construcción de lo que se dio en llamar RAU2, una modernización de la actual Red Académica Uruguaya (RAU), que estaría dotada de funciones de virtualización de redes que permitan una gran flexibilidad en su definición y uso. Como red universitaria pretende no solo mejorar en el ancho de banda disponible para desarrollar las tareas comunes con cualquier otra red de esta magnitud, sino que además busca virtualizar la red como mecanismo de asignar recursos independientes a cada Universidad o Facultad y poder utilizarla al mismo tiempo como laboratorio de pruebas sin que esto interfiera con el funcionamiento normal de la red.<br />

<div class="p"><!----></div>
Interesa entonces trabajar en el desarrollo de un prototipo para la RAU2 utilizando como plataforma PCs con placas de red aceleradas en hardware reconfigurable, fruto de un desarrollo de la Universidad de Stanford denominado NetFPGA(Field Programmable Gate Arrays)[<a href="#NetFPGA" name="CITENetFPGA">NetFPGA, 2015b</a>] y el enfoque de las Redes Definidas por Software(SDN)[<a href="#gude2008nox" name="CITEgude2008nox">Gude et&nbsp;al., 2008</a>][<a href="#SDNReadingList" name="CITESDNReadingList">NetFPGA, 2015a</a>].

<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;Objetivos</h2>
El objetivo principal de este trabajo es la implementación de un prototipo RAU2 de altas prestaciones, utilizando como punto de partida el enfoque de SDN y el hardware NetFPGA, para luego ejecutar un conjunto de pruebas que permitan validar estas tecnologías para su utilización en  la posible construcción de la RAU2.<br />

<div class="p"><!----></div>
A su vez dado la ausencia de experiencias similares  trabajando con estas tecnologías a nivel local en la órbita de la academia, se buscaba conocer a fondo estas tecnologías y en particular la arquitectura de SDN y los lenguajes de programación relacionados, así como generar conocimiento y experiencias que puedan contribuir en trabajos a futuro. Por ello se hizo especial hincapié en la generación de exhaustivo material sobre el estado del arte de las redes definidas por software, y una buena documentación de la experiencia obtenida utilizando el hardware NetFPGA.

<div class="p"><!----></div>
 <a name="tth_sEc4"></a><h2>
4&nbsp;&nbsp;Resultados Esperados</h2>
Dentro de los resultados esperados de este proyecto, uno de ellos es el estado del arte en el enfoque de las Redes Definidas por Software(SDN) y el hardware reconfigurable NetFPGA. En particular interesa conocer en profundidad ambas tecnologías, relevando el nivel de madurez y desarrollo de ellas, y reunir suficiente información para evaluar su utilización en la construcción de un prototipo que sea equiparable en prestaciones y rendimiento a un producto comercial sustituto. Para fijar ideas un posible sustituto comercial podría ser un equipo enrutador de backbone MPLS marca HP o Cisco por ejemplo.<br />

<div class="p"><!----></div>
El trabajo busca obtener una implementación de un prototipo de gestión y control de red utilizando estrategias de SDN, en particular enfocado en las funcionalidades y capacidades que se pretenden de la RAU2.<br />

<div class="p"><!----></div>
Finalmente, otro objetivo y no menos importante debe resultar de este trabajo el diseño e implementación de pruebas funcionales que permitan validar el prototipo desarrollado y evaluar el potencial de las tecnologías utilizadas.

<div class="p"><!----></div>
 <a name="tth_sEc5"></a><h2>
5&nbsp;&nbsp;Estructura del documento</h2>

<div class="p"><!----></div>
A continuación se describe la forma en que esta estructurado este documento y las temáticas asociadas a cada capitulo.<br />

<div class="p"><!----></div>
El cap&#237;tulo 2 presenta los resultados obtenidos en la investigación del estado del arte en SDN, NetFPGA y conceptos esenciales para el entendimiento de este trabajo. Se recomienda al lector fuertemente la lectura de las secciones <a href="#section2.2">2.2</a>, <a href="#section2.3">2.3</a>, <a href="#section2.7">2.7</a>, <a href="#section2.8">2.8</a> y <a href="#section2.9">2.9</a> para una mejor comprensión de este trabajo.<br />

<div class="p"><!----></div>
En el cap&#237;tulo 3 se presenta el análisis de la problemática realizado, donde se incluyen desde las principales ventajas en la utilización del enfoque SDN y hardware NetFPGA, los requerimientos relevados para el prototipo, el estudio de soluciones alternativas para la construcción del mismo y algunas de las decisiones de diseño m&#225;s importantes.<br />

<div class="p"><!----></div>
En el cap&#237;tulo 4 se muestra el diseño general del prototipo, profundizándose en la arquitectura del plano de control, la arquitectura de un nodo del prototipo y la interacción entre ambas componentes.<br />

<div class="p"><!----></div>
En el cap&#237;tulo 5 se presenta el diseño de RAUFlow, la aplicaci&#243;n que implementa el plano de control. Aquí se presentan desde los requerimientos relevados para RAUFlow, los casos de uso implementados, el modelo de datos constru&#237;do, detalles de la arquitectura e implementaci&#243;n de los algoritmos m&#225;s importantes.<br />

<div class="p"><!----></div>
En el cap&#237;tulo 6 se presenta el laboratorio de pruebas diseñado para validar funcionalmente el prototipo as&#237; como los casos de uso utilizados para ello y los resultados obtenidos.<br />

<div class="p"><!----></div>
En el cap&#237;tulo 7 se ofrece una evaluación de los resultados m&#225;s importantes, así como un breve estudio de la ejecuci&#243;n del proyecto, identificando las principales etapas, contratiempos y objetivos alcanzados.<br />

<div class="p"><!----></div>
En el cap&#237;tulo 8 se presentan las conclusiones obtenidas a partir del desarrollo de este trabajo y las principales l&#237;neas de trabajo e investigaci&#243;n identificadas como trabajo a futuro, dentro de las cuales se encuentran algunas de las mejoras que pueden incorporarse al prototipo.<br />

<div class="p"><!----></div>
Luego se incorpora el cap&#237;tulo de referencias con la inormaci&#243;n bibliográfica del material utilizado en el desarrollo de este trabajo.<br />

<div class="p"><!----></div>
Por &#250;ltimo se incluye un capitulo de ap&#233;ndices con contenidos que por varias razones no se incluyeron en el documento principal y que fueron debidamente referenciados en el desarrollo del mismo para complementar la lectura en caso de que el lector así lo desee.<br />

<div class="p"><!----></div>
Adicionalmente se anexa a este trabajo el manual generado para la construcci&#243;n de un nodo del prototipo, lo que se dio a llamar RAU-Switch, orientado a la reproducci&#243;n y posible mejora a futuro del trabajo realizado.

<div class="p"><!----></div>
 
<div class="p"><!----></div>
 <a name="tth_chAp2"></a><h1>
Capítulo 2 <br />Estado del Arte</h1>  
<div class="p"><!----></div>
    En este capítulo se presentan los resultados obtenidos en el trabajo de investigación sobre el estado
del arte de las Redes Definidas por Software(Software Defined Networking), lenguajes
de programación y herramientas para el desarrollo sobre dicha arquitectura, hardware NetFPGA y otros conceptos esenciales para el desarrollo de este trabajo. En caso de que el lector desee profundizar en alguno de los conceptos presentados en este cap&#237;tulo o consultar enfoques alternativos
y/o complementarios se recomiendan los siguientes trabajos&nbsp;[<a href="#StateOfArt1">Nunes et&nbsp;al., 2014</a>]&nbsp;[<a href="#StateOfArt2" name="CITEStateOfArt2">Feamster et&nbsp;al., 2013</a>]. Para el caso particular en que el lector este familiarizándose con el enfoque de las Redes Definidas por Software, se sugiere también la siguiente lista de lecturas[<a href="#SDNReadingList">NetFPGA, 2015a</a>] y el libro[<a href="#SDNBook1" name="CITESDNBook1">Azodolmolky, 2013</a>].<br />

<div class="p"><!----></div>
En relación a como se estructura este cap&#237;tulo, primero se introduce el concepto de Software Defined Netoworking(SDN) analizando sus principales antecedentes y finalmente brindando una definición. Luego se presenta y analiza en profundidad una de las arquitecturas existentes basadas en este enfoque y en particular utilizada por este trabajo para la construcción del prototipo. Se contin&#250;a con la introducción de algunos conceptos esenciales para el desarrollo de este trabajo como el concepto de Redes Privadas Virtuales y Multilabel Protocol Switch entre otros. Finalmente se presenta el relevamiento en el estado del arte realizado sobre la plataforma NetFPGA.  

<div class="p"><!----></div>

 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Antecedentes de SDN</h2> Software Defined Networking, Redes Definidas por Software en español o simplemente SDN por su sigla en ingl&#233;s, es un concepto relativamente nuevo ya que bajo este nombre puede contextualizarse en la &#250;ltima década. No obstante muchas de las ideas que yacen detrás de este enfoque han sido introducidas por diferentes propuestas en el pasado. Por ello vale la pena repasar los principales antecedentes que desde nuestra perspectiva contribuyeron al enfoque de SDN. 

<div class="p"><!----></div>
     <a name="tth_sEc1.1"></a><h3>
1.1&nbsp;&nbsp;Open Signaling</h3>
En el año 1995 el grupo de investigación OPENSIG[<a href="#campbell1999open" name="CITEcampbell1999open">451999Campbell et&nbsp;al.</a>] dentro del IETF, comenzó a trabajar en alternativas para hacer ATM, Internet y las redes móviles "más" abiertas, extensibles y programables.

<div class="p"><!----></div>
El principal objetivo de este grupo fue proveer de un acceso al hardware de red(routers, switches, etc) mediante un conjunto de interfaces abiertas. Con esto se buscaba permitir el despliegue de nuevos servicios de red a través de un ambiente de programación distribuido.

<div class="p"><!----></div>
El principal desafío con el que tuvo que lidiar esta iniciativa fue la estricta arquitectura en forma vertical que presenta el hardware de red(división en capas del stack OSI), cuya naturaleza a su vez solía ser cerrada. Esto dificultaba una estricta separación entre el software de control y el hardware.<br />

<div class="p"><!----></div>
Los esfuerzos de esta iniciativa desembocaron en la especificación del General Switch Managment Protocol(GSMP); un protocolo de propósito general para control de etiquetas en switches.
GSMP permitía a un controlador establecer y liberar conexiones en un switch, agregar, eliminar
y dejar una conexión multicast, manejar los puertos de un switch, obtener información acerca de la configuración, obtener y eliminar recursos reservados en un switch y obtener información estadística.<br />

<div class="p"><!----></div>
Tras años de trabajo, el grupo considero concluido su labor, siendo su último trabajo publicado la especificación de GSMPv3 en Junio del 2002[<a href="#doria2002general" name="CITEdoria2002general">482002Doria et&nbsp;al.</a>].

<div class="p"><!----></div>
     <a name="tth_sEc1.2"></a><h3>
1.2&nbsp;&nbsp;Active Networking</h3>
Active Networking[<a href="#tennenhouse1997survey" name="CITEtennenhouse1997survey">Tennenhouse et&nbsp;al., 1997</a>][<a href="#tennenhouse2002towards" name="CITEtennenhouse2002towards">Tennenhouse and Wetherall, 2002</a>][<a href="#moore2001towards" name="CITEmoore2001towards">Moore and Nettles, 2001</a>] &#243; Redes Activas en español es otra iniciativa que surge en la década de los 90's con la propuesta de una red programable que posibilitara la configuración de servicios sobre la misma. Fue propulsada por la USA Defense Advanced Research Projects Agency (DARPA), con el principal objetivo de acelerar la innovación en una área de redes marcada por la lentitud en incorporar nuevas tecnologías y servicios.<br />

<div class="p"><!----></div>
Se propusieron dos enfoques: (1) enfoque de Switches Programables en donde el tr&#225;fico es encaminado a través de nodos que ejecutan programas y cada paquete o su segmento correspondiente es derivado al programa apropiado en el nodo y (2) enfoque de C&#225;psulas en donde cada paquete enviado en la red transporta un programa, evaluándose en cada nodo en su ambiente de ejecución correspondiente.<br />

<div class="p"><!----></div>
Como tal, este proyecto no alcanz&#243; la suficiente masa cr&#237;tica como para imponerse en la industria, entre otras razones por la necesidad de un hardware diferente m&#225;s caro (tradicionalmente se utilizaba hardware de tipo ASIC y se requería de tipo TCAM, FPGA &#243; NPU) adem&#225;s de interrogantes de seguridad y desarrollo de lenguajes de programación apropiados.

<div class="p"><!----></div>
     <a name="tth_sEc1.3"></a><h3>
1.3&nbsp;&nbsp;Routing Control Plataorm</h3>
Routing Control Plataform (RCP)[<a href="#feamster2004case" name="CITEfeamster2004case">Feamster et&nbsp;al., 2004</a>][<a href="#caesar2005design" name="CITEcaesar2005design">Caesar et&nbsp;al., 2005</a>] o Plataforma de Control de Ruteo, propone una entidad de control(Routing Control Plataform) encargada de computar las rutas en una red, que luego utilizando protocolos estándares como iBGP traslada dicha configuración a los diferentes dispositivos.<br />

<div class="p"><!----></div>
Esta estrategia ten&#237;a como ventajas que al utilizar protocolos existentes estándares facilitaba notablemente la transición entre el paradigma actual y el propuesto. No obstante su potencial 
se encontraba limitado justamente a las capacidades de los protocolos de comunicación existentes.

<div class="p"><!----></div>
     <a name="tth_sEc1.4"></a><h3>
1.4&nbsp;&nbsp;ForCES Protocol</h3>
Otro grupo de trabajo dentro del IETF denominado ForCES(Forwarding and Control Element Separation) comenzó a trabajar a mediados del año 2007 en un protocolo que permita la separación entre los dispositivos de red y la inteligencia que los gobierna. Dicho protocolo fue especificado como una propuesta de estándar y publicado en el año 2010 bajo el nombre ForCES Protocol[<a href="#doria2010forwarding" name="CITEdoria2010forwarding">Doria et&nbsp;al., 2010</a>].

<div class="p"><!----></div>
Este enfoque plantea dos entidades lógicas denominadas Elementos de Control(CE) y Elementos de Reenv&#237;o(FE), así como un protocolo de comunicación entre ambas entidades.

<div class="p"><!----></div>
     <a name="tth_sEc1.5"></a><h3>
1.5&nbsp;&nbsp;4D Project</h3>
En el año 2004 el proyecto 4D [<a href="#rexford2004network" name="CITErexford2004network">Rexford et&nbsp;al., 2004</a>][<a href="#greenberg2005clean" name="CITEgreenberg2005clean">Greenberg et&nbsp;al., 2005</a>] propone una re-estructuración arquitectónica de una red basada en tres principios claves: 

<div class="p"><!----></div>

<ul>
<li> Políticas de red (network-level objectives): conjunto de objetivos o requerimientos sobre una red tales como performance, confiabilidad entre otras.
<div class="p"><!----></div>
</li>

<li> Vistas de la red (network-wide views): vistas globales sobre la topolog&#237;a de red, tr&#225;fico y eventos, con ciertos niveles de frescura y exactitud en los datos.
<div class="p"><!----></div>
</li>

<li> Control directo: se debe proveer de una interfaz directa para el control de los dispositivos de red
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Basándose en estos principios se construye el esquema denominado "4D" que merece su nombre por los cuatro planos presentes en su arquitectura: decisión(decision), diseminación(dissemination), descubrimiento(discovery) y datos (data).<br />

<div class="p"><!----></div>
Dicha arquitectura hace un fuerte énfasis en una completa separación de lo que son las decisiones lógicas que toma un Sistema Autónomo(AS), del conjunto de protocolos que gobiernan las redes e interacciones entre sus diferentes componentes.<br />

<div class="p"><!----></div>
Los objetivos al nivel de un AS son especificados en el plano de decisiones y mediante los servicios de los planos de diseminación y descubrimiento se ejerce un control directo sobre como se encamina y redirigen paquetes en el plano de datos.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg1">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 1: Capas de la arquitectura 4D</div>
<a name="fig:4DProject">
</a>
</div>
<div class="p"><!----></div>
En otras palabras el plano de decisión es responsable de crear la configuración de la red, mientras que el plano de diseminación se encarga de recolectar información sobre el estado de la red para el plano de decisión y distribuir la salida del plano de decisión. Mientras tanto el plano de descubrimiento permite a dispositivos descubrir vecinos directamente conectados en la red. Finalmente el plano de datos se encarga de redirigir el tráfico de red.<br /><br />

<div class="p"><!----></div>
Habiendo introducido a los principales antecedentes de las redes definidas por software, en la siguiente secci&#243;n se propone abordar el abordaje de dicho concepto.

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Software Defined Networking</h2>
<a name="section2.2">
</a>

<div class="p"><!----></div>
Definir el concepto de Software Defined Networking no es una tarea simple, puesto que no se encuentra por completo estandarizado a pesar de la existencia de varias organizaciones trabajando en la temática. En particular se pueden destacar el grupo de trabajo Software-Defined Networking Research Group(SDNRG) del IRTF, quien se encuentra trabajando en la redacción de las definiciones y estándares para este concepto, la Open Networking Foundation(ONF) organización que concentra gran cantidad de material y documentación sobre SDN y se encuentra trabajando en el desarrollo del protocolo OpenFlow el cual se explica mas adelante en este trabajo.<br />

<div class="p"><!----></div>
Acorde a la Open Networking Foundation, SDN puede resumirse como:

<div class="p"><!----></div>

<blockquote><div><i>"La separación física del plano de control de la red del plano de datos, y donde el plano de control controla varios dispositivos."</i>
</div></blockquote>

<div class="p"><!----></div>
Más en detalle, SDN es una arquitectura de red que desacopla los planos de control y de datos, moviendo el plano de control (inteligencia de la red, construcción y c&#225;lculo de políticas de red) hacia una entidad denominada Controlador. Esto habilita al control de la red a ser directamente programable y a la infraestructura subyacente ser abstraída por las aplicaciones y servicios de red.<br />

<div class="p"><!----></div>
La ONF presenta a SDN como una arquitectura emergente, dinámica, escalable, rentable y adaptable, haciéndola ideal para aplicaciones de naturaleza dinámica y exigentes de los recursos de red disponibles. En particular destaca las cualidades de presentar una gestión centralizada y de estar basada en estándares abiertos neutrales a los vendedores.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg2">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 2: Arquitectura de SDN - Capas lógicas</div>
<a name="fig:SDNArchitecture">
</a>
</div>
<div class="p"><!----></div>
El enfoque de SDN propone tres capas lógicas(ver imagen &nbsp;<a href="#fig:SDNArchitecture">2.2</a>): (1) capa de aplicaciones (Application Layer), (2) capa de control (Control Layer) y (3) capa de infraestructura (Infraestructure Layer).<br />

<div class="p"><!----></div>
La capa de infraestructura se compone por los dispositivos de red tradicionales como switches y routers, en particular compatibles con la arquitectura SDN. La inteligencia de dichos dispositivos a diferencia de switches y routers convencionales es retirada de los mismos para ser trasladada a la capa de control.

<div class="p"><!----></div>
Cada dispositivo implementa una API de operaciones bien definida mediante la cual es manipulado por la capa de control. Esta api de operaciones implementa lo que se denomina Interfaz Sur.
<br />

<div class="p"><!----></div>
En la capa de control se encuentra el software encargado de implementar el plano de control en dicho modelo, usualmente denominado Controlador.

<div class="p"><!----></div>
El Controlador implementa una API de operaciones a ser expuestas a las aplicaciones de la capa de Aplicación para la manipulación de dispositivos en la capa de infraestructura. Esta API de operaciones implementa lo que se conoce en el modelo como Interfaz Norte.<br />

<div class="p"><!----></div>
Por otro lado la capa de aplicaciones contiene implementados en software en forma de aplicaciones toda la inteligencia que originalmente se agregaba en un dispositivo de la capa de infraestructura, por ejemplo la implementaci&#243;n de protocolos de red.

<div class="p"><!----></div>
En dicha capa se pueden implementar desde protocolos de ruteo como OSPF y RIP, backbones sobre MPLS, políticas de seguridad y hasta aplicaciones de ingeniería de tráfico.<br />

<div class="p"><!----></div>
De esta forma Software Defined Networking centraliza el control de la red en una aplicación de software (Controlador) y transforma algoritmos, procesos de control, políticas de seguridad y toda la inteligencia antiguamente acoplados a los dispositivos de red, en aplicaciones de controlador.<br />

<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;Arquitecturas basadas en SDN</h2>
<a name="section2.3">
</a>

<div class="p"><!----></div>
Existen varias arquitecturas o implementaciones en las que de una forma u otra puede encontrarse argumentos para afirmar que siguen el enfoque de SDN. Si bien este trabajo se basa en la utilización de la arquitectura OpenFlow, existen otras arquitecturas basadas en este enfoque como el protocolo ForCES y alternativas m&#225;s comerciales como OpFlex.

<div class="p"><!----></div>
A continuaci&#243;n se explican los principales conceptos relacionados a la arquitectura OpenFlow.

<div class="p"><!----></div>
     <a name="tth_sEc3.1"></a><h3>
3.1&nbsp;&nbsp;OpenFlow</h3>

<div class="p"><!----></div>
Orientado por el enfoque en tres capas de SDN y bajo la misma premisa de desacoplar completamente los planos de datos y control, OpenFlow[<a href="#mckeown2008openflow" name="CITEmckeown2008openflow">632008McKeown et&nbsp;al.</a>] brinda una implementaci&#243;n estándar para el mecanismo de comunicación entre las capas de control e infraestructura; Interfaz Sur. En otras palabras, provee de un protocolo de comunicación estándar para manipular los distintos dispositivos de la red y así el plano de datos.<br />

<div class="p"><!----></div>
OpenFlow se basa en tres componentes(ver figura &nbsp;<a href="#fig:OpenFlowArch">2.3</a>): (1) Controlador OpenFlow compatible, (2) Protocolo OpenFlow y (3) Switch OpenFlow compatible.

<div class="p"><!----></div>
  
<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg3">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 3: Estructura de un switch OpenFlow. Imagen extraída de [<a href="#mckeown2008openflow">632008McKeown et&nbsp;al.</a>]</div>
<a name="fig:OpenFlowArch">
</a>
</div>
<div class="p"><!----></div>
A partir de que cada dispositivo es completamente configurado y controlado por el Controlador, en la arquitectura de OpenFlow todos los dispositivos son iguales, denominados simplemente switches OpenFlow. Un switch OpenFlow se compone de tres componentes principales(ver figura &nbsp;<a href="#fig:OpenFlowArch">2.3</a>): (1) Tabla de flujos, (2) Canal Seguro (Secure Channel) que conecta el switch con el Controlador habilitando el intercambio de paquetes y comandos entre estos últimos dos utilizando (3) el protocolo OpenFlow.<br />

<div class="p"><!----></div>
Cada switch OpenFlow presenta una o m&#225;s tablas de flujos, compuestas por entradas denominadas flujos. Cada uno de estos flujos determina c&#243;mo una clase de paquetes deben ser procesados y reenviados.<br />

<div class="p"><!----></div>
Cada entrada o flujo en esta tabla(ver figura &nbsp;<a href="#fig:OpenFlowArch2">2.4</a>) se compone de: (1) campos de selección(matching fields), utilizados para identificar a un conjunto de paquetes a ser tratados de una forma en particular, basándose en información relacionada a campos en los cabezales del paquete para esto; (2) contadores para la recolección de información estadística en relación al flujo (n&#250;mero de paquetes recibidos, cantidad de bytes, duración de un flujo, etc) y (3) un conjunto de instrucciones y acciones que son aplicadas a cada paquete que coincide con el flujo descrito. Estos campos establecen la forma en que un paquete es procesado y reenviado.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg4">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 4: Visión esquemática del funcionamiento de un Switch OpenFlow. Imagen extraída de [<a href="#mckeown2008openflow">632008McKeown et&nbsp;al.</a>]</div>
<a name="fig:OpenFlowArch2">
</a>
</div>
<div class="p"><!----></div>
Cuando un paquete arriba a un switch(ver figura &nbsp;<a href="#fig:OFPacketProcessing">2.5</a>), se extraen y comparan cabezales del paquete acorde a las reglas definidas en el matching field del flujo, comparando los campos utilizados. En caso que el paquete coincida con las reglas definidas, se aplican el conjunto de instrucciones y acciones asociadas a dicho flujo. En caso que el paquete no coincida con ningún flujo, el accionar a desempeñar dependerá del conjunto de instrucciones y acciones definidas por una entrada especial en la tabla de flujos, denominada Table-miss Flow Entry.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg5">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 5: Ciclo de vida de un paquete en el pipe OpenFlow</div>
<a name="fig:OFPacketProcessing">
</a>
</div>
<div class="p"><!----></div>
La Table-miss Flow Entry esta pensada para contemplar el tr&#225;fico que no es definido por ningún otro flujo, por ello en esencia es el ultimo flujo a ser considerado(menor prioridad) al momento de procesar un paquete. Generalmente tiene asociada la acción de arrojar el paquete (Drop), procesar el paquete en la siguiente tabla de flujos(GoTo Table) &#243; reenviar el paquete al Controlador.<br />

<div class="p"><!----></div>
Otra caracter&#237;stica importante de un switch OpenFlow, es la capacidad para procesar paquetes utilizando protocolos tradicionales de red adem&#225;s del procesamiento normal de OpenFlow. Esta característica a su vez da lugar a la clasificaci&#243;n en switches OpenFlow puros y switches OpenFlow h&#237;bridos. Mientras que los switches OpenFlow puros son aquellos que solamente soportan el protocolo de igual nombre, los switches OpenFlow h&#237;bridos son aquellos que adicionalmente permiten procesar paquetes de la misma forma en que lo har&#237;a un hardware de red legado.

<div class="p"><!----></div>
Aprovechando las capacidades de un switch OpenFlow h&#237;brido, se puede definir por ejemplo la Table-Flow-Entry de forma que se procese todo paquete contemplado por esta entrada como un hardware de red tradicional, aplicando por ejemplo esquemas de reenvío IP. 

<div class="p"><!----></div>
     <a name="tth_sEc3.2"></a><h3>
3.2&nbsp;&nbsp;Reglas OpenFlow</h3>
Cada entrada en la tabla de flujos, distingue un tipo de tr&#225;fico en particular y la forma en que se procesa(acciones e instrucciones). Es uno de los pilares del protocolo la expresividad disponible para escribir las reglas de cada flujo; es decir, la capacidad de agrupar tipos de tr&#225;fico acorde a diferentes propiedades en el mismo.<br />

<div class="p"><!----></div>
Las reglas OpenFlow se basan en los valores de cabezales asociados a diferentes capas del stack OSI. Estos cabezales a su vez varían con la versi&#243;n del protocolo, incorporándose m&#225;s opciones en cada versi&#243;n del mismo. Por ejemplo en la versi&#243;n 1.0 del protocolo se tiene un soporte mínimo para cabezales de capas 1 a 4 (ver imagen &nbsp;<a href="#fig:OF10MatchingFields">2.6</a>).

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg6">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 6: OpenFlow v1.0 Matching Fields</div>
<a name="fig:OF10MatchingFields">
</a>
</div>
<div class="p"><!----></div>
Por otro lado en la version 1.3.3 del protocolo se cuenta soporte para utilizar cabezales de IPv6, ICMPv6 y MPLS en la definición de los flujos (ver imagen &nbsp;<a href="#fig:OF13MatchingFields">2.7</a>).

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg7">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 7: OF 1.3.3 Matching Fields</div>
<a name="fig:OF13MatchingFields">
</a>
</div>
<div class="p"><!----></div>
     <a name="tth_sEc3.3"></a><h3>
3.3&nbsp;&nbsp;Acciones OpenFlow</h3>

<div class="p"><!----></div>
Como se menciona anteriormente la acci&#243;n de un flujo OpenFlow determina la forma en que un paquete es procesado en un switch. Las tres acciones principales que todo switch OpenFlow debe implementar son:

<div class="p"><!----></div>

<ol type="1">
<li> <b>[Output:Puerto]:</b> Reenviar un paquete hacia un puerto determinado (o conjunto de puertos). Esto permite encaminar paquetes a través de la red.
<div class="p"><!----></div>
</li>

<li> <b>[Output:Controller]:</b> Encapsular y reenviar un paquete hacia el Controlador. Utilizando el canal de comunicación con el Controlador y el protocolo OpenFlow, se reenvía el paquete al plano de Control. Esto permite procesar un paquete no identificado en ningún flujo en cualquier momento o en una etapa de configuración de la red. Luego el Controlador decide si descartar el paquete o instalar un nuevo flujo que lo contemple.
<div class="p"><!----></div>
</li>

<li> <b>[Drop]:</b> Descartar un paquete. Puede ser utilizado por razones de seguridad, para prevenir ataques de negación de servicios (DoS), eliminar paquetes sospechosos, disminuir el impacto de paquetes de descubrimiento en redes de difusión(broadcast), o simplemente descartar paquetes no contemplados.
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
     <a name="tth_sEc3.4"></a><h3>
3.4&nbsp;&nbsp;Controlador OpenFlow</h3>
Basicamente un controlador SDN brinda implementaciones a las Interfaces Sur y Norte. Un controlador OpenFlow/SDN requiere adem&#225;s de esto la compatibilidad con el protocolo OpenFlow en la interfaz sur. 

<div class="p"><!----></div>
Basándose solamente en estos principios, las implementaciones de controladores OpenFlow pueden ser muy variadas. Desde controladores muy simples como NOX[<a href="#ControllersNOX" name="CITEControllersNOX">82014hCon</a>] y POX[<a href="#ControllersPOX" name="CITEControllersPOX">112014kCon</a>] que siguen estrictamente el enfoque de SDN/OpenFlow, hasta controladores m&#225;s elaborados como OpenDaylight[<a href="#ControllersOpendaylight" name="CITEControllersOpendaylight">92014iCon</a>] (ver imagen &nbsp;<a href="#fig:OpenDayLightHydrogen">2.8</a>) que implementan OpenFlow como una característica mas de su implementaci&#243;n de interfaz Sur.

<div class="p"><!----></div>
Sin embargo existen aspectos relacionados a la entidad Controlador dentro del enfoque SDN/OpenFlow que son afines a cualquier implementacion y arquitectura. A continuaci&#243;n se mencionan los principales:

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg8">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 8: Arquitectura de OpenDaylight Hydrogen</div>
<a name="fig:OpenDayLightHydrogen">
</a>
</div>
<div class="p"><!----></div>

<h4>Granularidad en el control</h4>
Tradicionalmente, la unidad básica de tráfco en una red es el paquete. Cada paquete contiene información de direccionamiento necesaria para la toma de decisiones dentro del switch a nivel de reenvío. Sin embargo muchas aplicaciones de red envían datos como un flujo de muchos paquetes individuales, mientras que las redes que desean proveer de calidad de servicios(QoS) se benefician utilizando flujos. Por ello resulta beneficioso adoptar un nivel de abstracción superior en el control del plano de datos, distinguiendo tr&#225;fico por flujos y a través de paquetes individuales.<br />

<div class="p"><!----></div>
Por otro lado uno de los puntos vulnerables del enfoque SDN, es la comunicación entre switch y Controlador. Sobrecargar este canal de comunicación, pensando en redes a gran escala en la cantidad de dispositivos, supone una debilidad importante. Basar la granularidad de control en paquetes individuales, profundiza esta debilidad; mientras que el enfoque basado en flujos atenúa esta debilidad. 

<div class="p"><!----></div>
Para fijar ideas, en el caso en que un paquete no es contemplado por un flujo particular y es reenviado al Controlador, puede resolverse instalar un flujo a posteriori para contemplar tal paquete. En el enfoque de flujos, cualquier otro paquete contemplado por el flujo no es m&#225;s reenviado al Controlador, mientras que en el enfoque de paquetes individuales la regla instalada contempla solamente al paquete inicial.

<div class="p"><!----></div>
A su vez se pueden utilizar las mismas prácticas que se utilizan con el control a nivel
de paquetes individuales, como agrupar flujos relacionados al tráfico entre dos hosts, tomando
decisiones sobre los flujos agregados. 

<div class="p"><!----></div>

<h4>Centralizado vs Distribuido</h4>
SDN no establece ninguna restricción en cuanto a si el plano de control debe ser tanto lógica como físicamente centralizado o distribuido. De hecho resulta conveniente pensar en enfoques distribuidos cuando se piensa en arquitecturas robustas y escalables. No es el objetivo de este trabajo profundizar en esta área de investigación por lo cual se recomienda para profundizar sobre esta temática [<a href="#heller2012controller" name="CITEheller2012controller">562012Heller et&nbsp;al.</a>][<a href="#levin2012logically" name="CITElevin2012logically">612012Levin et&nbsp;al.</a>]. 

<div class="p"><!----></div>
En relacion a OpenFlow, si bien el protocolo no establece mecanismos para la comunicación entre controladores, habilita a un switch a conectarse a múltiples controladores.<br />

<div class="p"><!----></div>
Algunos proyectos como Onix[<a href="#koponen2014distributed" name="CITEkoponen2014distributed">592014Koponen et&nbsp;al.</a>] e HyperFlow[<a href="#tootoonchian2010hyperflow" name="CITEtootoonchian2010hyperflow">812010Tootoonchian and Ganjali</a>] toman la idea de mantener un plano de control lógicamente centralizado, pero físicamente distribuido. Este enfoque tiene como principal beneficio disminuir la sobrecarga en el proceso de búsqueda sobre una tabla de flujos, permitiendo la comunicación con controladores locales.<br />

<div class="p"><!----></div>
Kandoo[<a href="#hassas2012kandoo" name="CITEhassas2012kandoo">552012Hassas&nbsp;Yeganeh and Ganjali</a>], proponen utilizar controladores locales para el manejo de aplicaciones y redirigir hacia un Controlador global flujos que requieran de una visión global del estado de la red, bastante similar a lo anterior. Esto tiene como principal beneficio la reducción de la carga sobre el Controlador global, debido al filtrado de solicitudes de flujo que realizan los controladores locales, y a la vez proveen al plano de datos de una rápida respuesta para las solicitudes que pueden ser manejadas por los controladores locales.<br />

<div class="p"><!----></div>
Concluyendo con este apartado, otro de los enfoques que buscan generar una distribución del plano de control en SDN es el asumido por FlowVisor[<a href="#sherwood2010carving" name="CITEsherwood2010carving">782010Sherwood et&nbsp;al.</a>]. Este enfoque propone la virtualización de una red de switches OpenFlow con múltiples controladores, construyendo lo que denominan Slices de la red. De esta forma brindan a cada controlador una visión local a su slice; mientras que un controlador especial actuá de proxy, manteniendo una visión global de toda la red.

<div class="p"><!----></div>

<h4>Políticas reactivas vs proactivas</h4>
Existen dos enfoques para la intervención del controlador en el plano de datos: (1) reactivo y (2) proactivo.

<div class="p"><!----></div>

<ol type="1">
<li> <b>Instalación reactiva de políticas:</b> En este enfoque, para fijar ideas, un switch se configura con la tabla de flujos vac&#237;a o al menos una configuración mínima al encenderse. Luego, cada vez que un paquete arriba al switch y no es contemplado por ning&#250;n flujo, es reenviado al Controlador. Este decide que hacer con el paquete e instala un flujo en el dispositivo para procesar de igual forma paquetes similares.

<div class="p"><!----></div>
Este modelo adoptado en particular por Ethane[<a href="#casado2007ethane" name="CITEcasado2007ethane">462007Casado et&nbsp;al.</a>], presenta como principal desventaja la sobrecarga en el canal de comunicación Switch-Controlador. Por ello juega un rol sumamente importante en este esquema la definición de los flujos (flujos m&#225;s generales disminuyen la sobrecarga con el controlador y flujos muy específicos aumentan esta sobrecarga), tiempo de vida de los flujos en la tabla(flujos con tiempo de vida de un solo paquete aumentan la sobrecarga), la ubicación física del controlador y las características del canal de comunicación. Utilizando una jerarquía de controladores, canales de comunicación de alta velocidad o redundancia de enlaces, se puede mejorar la penalizaci&#243;n en los tiempos de respuesta por la sobrecarga del canal de comunicación.
<div class="p"><!----></div>
</li>

<li> <b>Instalación proactiva de políticas:</b> En este enfoque, para fijar ideas, un switch se configurar con la mayor cantidad de flujos posibles, contemplando la mayor cantidad de tr&#225;fico en la red al momento de encender el switch. Luego, cuando un paquete ingresa al switch, en caso de no ser contemplado por ninguno de los flujos instalados, se reenvía al Controlador para decidir c&#243;mo actuar. Luego el Controlador puede instalar un flujo para contemplar el caso asociado a este paquete. 

<div class="p"><!----></div>
En [<a href="#yu2011scalable" name="CITEyu2011scalable">822011Yu et&nbsp;al.</a>] se analiza el impacto de utilizar un enfoque de políticas proactivas en la performance de una red.
<div class="p"><!----></div>
</li>
</ol> 

<div class="p"><!----></div>

<div class="p"><!----></div>
 <a name="tth_sEc4"></a><h2>
4&nbsp;&nbsp;Herramientas SDN/OpenFlow disponibles</h2>

<div class="p"><!----></div>
La principal herramienta para el desarrollo de aplicaciones SDN/OpenFlow es el software de control o Controlador. No obstante también existen entornos de emulación compatibles con estas tecnologías y que merecen la pena ser mencionados en este trabajo por su utilidad.

<div class="p"><!----></div>
     <a name="tth_sEc4.1"></a><h3>
4.1&nbsp;&nbsp;Controlador</h3>
Existe un cantidad interesante de propuestas para la implementaci&#243;n del plano de control. En este trabajo se ha mencionado algunos como OpenDaylight, NOX y POX, pero existen varias alternativas. Pueden encontrarse desde controladores comerciales y académicos, en código abierto y propietario, compatibles solamente con OpenFlow y compatibles con múltiples protocolos incluyendo OpenFlow, etc.<br />

<div class="p"><!----></div>
Estas características son utilizadas en este proyecto para la elección de la mejor alternativa de software de control, basándose en las necesidades y restricciones del prototipo. Por ello se cree conveniente incluir el siguiente cuadro comparativo, en el que se presentan las principales características utilizadas en la elecci&#243;n del controlador m&#225;s apropiado. Cabe destacar que esta tabla esta basada en una tabla m&#225;s reducida presentada en [<a href="#StateOfArt1">702014Nunes et&nbsp;al.</a>]. 

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb1">
</a> <div class="tiny">
<table border="1">
<tr><td align="left"><b>Controlador</b> </td><td align="left"><b>Implementación</b> </td><td align="left"><b>Opensource</b> </td><td width="79"><b>Desarrollador</b>         </td><td align="left"><b>OpenFlow</b> </td><td width="237"><b>Detalle</b>                                                                                                                                                                                                                                                                                                          </td></tr>
<tr><td align="left">POX[<a href="#ControllersPOX">112014kCon</a>]                  </td><td align="left">Python                  </td><td align="left">Si                  </td><td width="79">Nicira                         </td><td align="left">1.0               </td><td width="237">Controlador de proposito general implementado en Python, compatible con Linux, Mac OS, y Windows.                                                                                                                                                                                                                         </td></tr>
<tr><td align="left">NOX[<a href="#ControllersNOX">82014hCon</a>]                  </td><td align="left">C++                     </td><td align="left">Si                  </td><td width="79">Nicira                         </td><td align="left">1.0               </td><td width="237">Uno de los primeros controladores ampliamente utilizado. Implementa funcionalidades para descubrimiento de topologías, learning switch, y Network-wide switch. Esta probado con Linux 12.04 inclusive.                                                                                                                    </td></tr>
<tr><td align="left">OpenMUL[<a href="#ControllersOpenMUL" name="CITEControllersOpenMUL">102014jCon</a>]          </td><td align="left">C                       </td><td align="left">Si                  </td><td width="79">Kulcloud                       </td><td align="left">1.4               </td><td width="237">Controlador basado en OpenFlow con compatibilidad hacia atrás (OpenFow v1.0, v1.2, etc) con soporte multi-thread basado en lenguaje C, soporte SSL, soporte a múltiples interfaces norte, API REST                                                                                                                        </td></tr>
<tr><td align="left">Maestro[<a href="#ControllersMaestro" name="CITEControllersMaestro">62014fCon</a>]              </td><td align="left">Java                    </td><td align="left">Si                  </td><td width="79">Rice University                </td><td align="left">-                 </td><td width="237">Maestro is an &#246;perating system" for orchestrating network control applications. Maestro provides interfaces for implementing modular network control applications to access and modify state of the network, and coordinate their interactions                                                                           </td></tr>
<tr><td align="left">Trema[<a href="#ControllersTrema" name="CITEControllersTrema">152014oCon</a>]                </td><td align="left">Ruby/C                  </td><td align="left">Si                  </td><td width="79">NEC                            </td><td align="left">1.0               </td><td width="237">Framework full-stack para el desarrollo de Controladores OpenFlow en Ruby y C, compatible con Ubuntu 13.04 y Fedora 16-19 entre otras distibuciones Linux.                                                                                                                                                                </td></tr>
<tr><td align="left">Beacon[<a href="#ControllersBeacon" name="CITEControllersBeacon">12014aCon</a>]           </td><td align="left">Java                    </td><td align="left">Si                  </td><td width="79">Stanford                       </td><td align="left">1.0?              </td><td width="237">Controlador multi-plataforma, rápido y modular basado en Openflow con soporte a programación orientada a eventos y concurrente (threads). Incorpora la plataforma web Jetty y un modulo GUI extensible.                                                                                                                   </td></tr>
<tr><td align="left">Jaxon[<a href="#ControllersJaxon" name="CITEControllersJaxon">52014eCon</a>]                </td><td align="left">Java                    </td><td align="left">Si                  </td><td width="79">Desarrolladores independientes </td><td align="left">1.0?              </td><td width="237">Controlador OpenFlow basado en NOX para desarrollo en  Java.                                                                                                                                                                                                                                                              </td></tr>
<tr><td align="left">Helios[<a href="#ControllersHelios" name="CITEControllersHelios">42014dCon</a>]               </td><td align="left">C                       </td><td align="left">No                  </td><td width="79">NEC                            </td><td align="left">-                 </td><td width="237">Controlador basado en OpenFlow orientado a lenguaje C, extensible. Interfaz de comunicación Norte en formato Shell.                                                                                                                                                                                                       </td></tr>
<tr><td align="left">Floodlight[<a href="#ControllersFloodlight" name="CITEControllersFloodlight">22014bCon</a>]           </td><td align="left">Java                    </td><td align="left">Si                  </td><td width="79">BigSwitch                      </td><td align="left">1.3               </td><td width="237">Controlador SDN basado en OpenFlow, soporte para switches virtuales y físicos, maneja redes OpenFlow y redes no OpenFlo así como islas OpenFlow, soporte a OpenStack.                                                                                                                                                     </td></tr>
<tr><td align="left">SNAC[<a href="#ControllersSNAC" name="CITEControllersSNAC">132014mCon</a>]                 </td><td align="left">C++                     </td><td align="left">No                  </td><td width="79">Nicira                         </td><td align="left">-                 </td><td width="237">Controlador basado en OpenFlow para redes LAN con GUI y un lenguaje definido para declarar reglas. Esta basado en el controlador NOX más un módulo para un lenguaje de modelado formal (FML).                                                                                                                             </td></tr>
<tr><td align="left">Ryu[<a href="#ControllersRyu" name="CITEControllersRyu">142014nCon</a>]                  </td><td align="left">Python                  </td><td align="left">Si                  </td><td width="79">NTT, OSRG group                </td><td align="left">1.4               </td><td width="237">Framework de programación para SDN, provee Controladores basados en OpenFlow, Netconf, Of-config entre otros. Brinda soporte para OpenFlow v1.0 v1.2 v1.3 y v1.4 y extenciones Nicira.                                                                                                                                    </td></tr>
<tr><td align="left">Nodeflow[<a href="#ControllersNodeFlow" name="CITEControllersNodeFlow">72014gCon</a>]             </td><td align="left">Javascript              </td><td align="left">Si                  </td><td width="79">NTT, OSRG group                </td><td align="left"></td><td width="237">Controlador basado en OpenFlow orientado a programación en Javascript, basado en el framework Node.js.                                                                                                                                                                                                                    </td></tr>
<tr><td align="left">OVS-Controller[]       </td><td align="left">C                       </td><td align="left">Si                  </td><td width="79">Desarrolladores independientes </td><td align="left">-                 </td><td width="237">Controlador basado en OpenFlow de referencia con soporte a Open vSwitch y gran parte de otros tipos de switches. Como resultado los switches funcionan como un Switch de capa 2 MAC-learning.                                                                                                                             </td></tr>
<tr><td align="left">FlowVisor[<a href="#ControllersFlowvisor" name="CITEControllersFlowvisor">32014cCon</a>]            </td><td align="left">C                       </td><td align="left">Si                  </td><td width="79">Stanford/Nicira                </td><td align="left">-                 </td><td width="237">Controlador de propósitos particulares, que actua como proxy de forma transparente entre una red OpenFLow y múltiples Controladores OpenFlow.                                                                                                                                                                             </td></tr>
<tr><td align="left">RouteFlow[<a href="#ControllersRouteflow" name="CITEControllersRouteflow">122014lCon</a>]            </td><td align="left">C++                     </td><td align="left">Si                  </td><td width="79">CPqD                           </td><td align="left">1.3               </td><td width="237">Proyecto opensource que provee de servicios de routing virtualizados sobre hardware OpenFlow. Un escenario de uso común puede ser su utilización en conjunto con otro Controlador com Ryu.                                                                                                                                </td></tr>
<tr><td align="left">OpenDaylight[<a href="#ControllersOpendaylight">92014iCon</a>]         </td><td align="left">Python                   </td><td align="left">Si                  </td><td width="79">Linux Foundation               </td><td align="left">1.3               </td><td width="237">Open Daylight es un framework abierto a la comunidad y apoyado por fabricantes, orientado a mejorar y agilizar la adopción e innovación. Provee un Controlador basado en OpenFlow orientado a Java?, entre otras implementaciones de interfaz SDN sur provistas. Brinda soporte a OpenFlow hasta versión 1.3 actualmente.</td></tr></table>


<div style="text-align:center">Table 1: Controladores disponibles y sus características (Basada en una tabla similar y más reducida en [<a href="#StateOfArt1">702014Nunes et&nbsp;al.</a>])</div>
<a name="table:Controladores">
</a>
</div>
<div class="p"><!----></div>
     <a name="tth_sEc4.2"></a><h3>
4.2&nbsp;&nbsp;Mininet y Mini Next</h3>
Mininet[<a href="#Mininet1" name="CITEMininet1">262015Min</a>] es un emulador de red, que habilita a crear hosts, switches, links y controladores, en un entorno virtual. Se encuentra disponible tanto para instalarse nativamente en un entorno Linux, como a través de una m&#225;quina virtual configurada con todo el software de desarrollo necesario para iniciarse en el desarrollo de aplicaciones OpenFlow/SDN(Mininet Kit Started). 

<div class="p"><!----></div>
Soporta diferentes versiones del protocolo OpenFlow y es una de las herramientas m&#225;s utilizadas para la prototipaci&#243;n y experimentación de aplicaciones desarrolladas en dicha arquitectura; puesto que habilita a investigadores y desarrolladores, aprender, prototipar, probar y debuggear aplicaciones rápidamente utilizando una computadora convencional.<br />

<div class="p"><!----></div>
Mininext es una extension de Mininet que permite entre otras cosas asignar a cada host su propio sistema de ficheros, permitiendo as&#237; modificar la configuraci&#243;n por defecto de cada nodo virtual e instalar diferentes herramientas en ellos. Esta orientado al despliegue virtual de arquitecturas m&#225;s complejas que las posibles con Mininet.<br />

<div class="p"><!----></div>
En este trabajo Mininet es utilizado para realizar pruebas de experimentaci&#243;n y benchmark con las alternativas de controladores disponibles, con el objetivo de determinar el controlador OpenFlow a ser utilizado en la arquitectura del prototipo.<br />

<div class="p"><!----></div>
  <a name="tth_sEc5"></a><h2>
5&nbsp;&nbsp;Aplicaciones de SDN</h2>
SDN presenta un campo de aplicación bastante extenso, y es posible que a medida que el concepto siga evolucionando y asentándose se irán descubriendo nuevos casos de uso y campos de aplicación. A continuación se muestran algunos casos de uso:

<div class="p"><!----></div>

<ul>
<li> <b>Monitoreo de Red:</b>
En una arquitectura OpenFlow por ejemplo, los switches OpenFlow proveen de información estadística(similar a SNMP) de cada flujo en tiempo real. De esta forma un Controlador puede recolectar información estadística sobre el tráfico en el plano de datos, lo cual es de gran interés para operadores de red y aplicaciones, con la granularidad exacta que estos requieran. Esta información puede ser agregada de diferentes formas: por dirección IP, por dirección MAC, etiquetas VLAN, por aplicación, etc. De esta forma se logra una gran versatilidad en la entrega de información estadística sobre el tráfico en una red.
<div class="p"><!----></div>
</li>

<li> <b>Redes Tap programables:</b>
Utilizando OpenFlow se pueden implementar redes taps programables y replicar el tráfico para su monitoreo de una forma más eficiente que los métodos tradicionales de SPAN/RSPAN. También pueden filtrarse los datos que son enviados por ejemplo a un sistema de detección de intrusos(IDS), reduciendo la carga de trabajo a los dispositivos de procesamiento de datos utilizados, filtrando previamente los datos que son realmente necesarios y evitando replicar datos innecesarios.<br />
Además desde que el control de los dispositivos intermediarios puede realizarse desde un controlador, se facilita enormemente a gestión de los mismos, permitiendo una configuración dinámica en caso de fallas o averías técnicas.
<div class="p"><!----></div>
</li>

<li> <b>Balanceo de carga y QoS:</b>
Contar con una visión global de los dispositivos del plano de datos, habilita al Controlador a implementar diferentes políticas de balanceo de carga en función del estado de la red y permite un uso óptimo de todos los recursos disponibles. A su vez se pueden priorizar diferentes tipos de tráfico, encaminar tráfico dependiendo de un tipo de servicio o contenido y otras técnicas de QoS.
<div class="p"><!----></div>
</li>

<li> <b>Herramientas para mitigación de ataques DoS:</b>
Herramientas para la mitigación de ataques de negación de servicios (DoS) pueden beneficiarse de las estadísticas provistas por un switch OpenFlow por ejemplo para detectar anomalías. Además utilizando la acción de redirección de un flujo al Controlador, se puede analizar y detectar tráfico sospechoso de ataques DoS, permitiendo a posteriori insertar un flujo en el switch de ingreso específico para bloquear el tipo de ataque o tráfico malicioso detectado.
<div class="p"><!----></div>
</li>

<li> <b>Inserción de Servicios:</b>
Con una arquitectura basada en SDN como openFlow, resulta sumamente sencillo la incorporación de servicios al plano de datos de una infraestructura de red dada. Independientemente de la marca del hardware utilizado, sus características y funcionalidades, siempre que sean compatibles con OpenFlow puede incorporarse nuevos servicios como autenticación, cortafuegos, almacenamiento secundario, etc., simplemente programando una nueva aplicación en el Controlador que implemente dichos servicios.
<div class="p"><!----></div>
</li>

<li> <b>Experimentación:</b>
El desarrollo, verificación y puesta en producción de nuevos servicios y protocolos resulta sumamente sencillo puesto que consta simplemente de desplegar una nueva aplicación en un entorno de software (Controlador). Por ello resulta sumamente útil el enfoque de SDN para la academia y la industria en lo que refiere al desarrollo de nuevos protocolos de red, nuevos servicios, mejoras a servicios existentes, entre otros.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Por estas razones soluciones basadas en el enfoque SDN con OpenFlow por ejemplo, u otras arquitecturas basadas en SDN resultan de gran utilidad en escenarios como:

<div class="p"><!----></div>

<ol type="1">
<li> Redes empresariales
<div class="p"><!----></div>
</li>

<li> Centros de cómputo(Data Centers)
<div class="p"><!----></div>
</li>

<li> Academia
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
 <a name="tth_sEc6"></a><h2>
6&nbsp;&nbsp;Casos de éxito de SDN</h2>
OpenFlow en particular se ha hecho con un cierto protagonismo recientemente. Tal es el hecho que se pueden nombrar casos de éxito en la migración de servicios sobre esquemas tradicionales de redes a SDN/OpenFlow.<br />

<div class="p"><!----></div>
La Open Networking Foundation en particular elabor&#243; un articulo[<a href="#ONFSuccessCase" name="CITEONFSuccessCase">642014Migration Working Group</a>] en donde se recomiendan métodos y se proveen guías para la migración de servicios desde un esquema tradicional a SDN, nombrando en particular tres casos de éxito en este tipo de migraciones, en escenarios reales: (1) Google InterDatacenter
WAN, (2) NTT Provider Edge y (3) Stanford Campus Network.

<div class="p"><!----></div>

<ol type="1">
<li> Google InterDatacenter:

<div class="p"><!----></div>
Los servicios de Google orientados a usuarios de Internet como su motor de búsquedas, Google+, GMail, Youtube, Google Maps, entre otros, requieren que una gran cantidad de datos sean movidas desde una región a otra todo el tiempo, haciendo a estas aplicaciones y servicios consumidores intensivos de la WAN. Debido a esto Google concluy&#243; que la oferta de dichos servicios no sería escalable con las tecnologías disponibles actualmente, debido a la complejidad en la configuración y manejo de dichas tecnologías las cuales no son lineales con el crecimiento en la demanda de dichos servicios. Como resultado, Google decidió apostar por el enfoque de SDN para el manejo de su WAN. Si el lector est&#225; interesado en esta experiencia, puede encontrar una descripción completa en [<a href="#jain2013b4" name="CITEjain2013b4">572013Jain et&nbsp;al.</a>].
<div class="p"><!----></div>
</li>

<li> NTT Provider Edge:

<div class="p"><!----></div>
En el modelo de despliegue tradicional de BGP, los routers  proveedores de servicios de borde(Provider Edge Roputer), mantienen numerosas adyacencias BGP; tantas como routers/caminos BGP para diferentes familias de direcciones, como IPv4, IPv6, VPNv4, VPNv6, etc.<br />

<div class="p"><!----></div>
Mantener la máquina de estados de BGP, procesar las actualizaciones BGP y configuraciones de políticas, calcular los mejores caminos para cada familia de direcciones, constituyen una gran carga de procesamiento en el hardware del router. Adicionalmente los servicios por definición están sujetos a modificaciones en el tiempo, ya sea para proveer de servicios a nuevos clientes como para actualizar sus políticas. Por otro lado los recursos disponibles como CPU y memoria son limitados, y la naturaleza de los sistemas operativos y hardware de cada equipo es propietaria de la implementación de cada fabricante. Esto limita la aceleración de servicios y la innovación a las políticas de cada vendedor de hardware.<br />

<div class="p"><!----></div>
Por ello BGP free edge, define un nuevo paradigma que simplifica el encaminamiento eBGP en los routers PE. En este modelo de despliegue, el router PE se convierte en un nodo de reenvío para manejar el plano de datos, mientras que el plano de control BGP se desacopla a una entidad externa. El plano de control de SDN/Openflow encaja perfectamente en el rol de dicha entidad.<br />

<div class="p"><!----></div>
Algunas de las razones por las cuales apostar por BGP free edge:

<ul>
<li> Simplificar y abaratar la arquitectura de un router PE-BGP con una política BGP centralizada.
<div class="p"><!----></div>
</li>

<li> Acelerar el despliegue de nuevos servicios de borde mediante la separación en plano de control y plano de datos
<div class="p"><!----></div>
</li>

<li> Mejor control sobre patrones de tráfico
<div class="p"><!----></div>
</li>

<li> Flexibilidad para calcular mejores caminos configurables
<div class="p"><!----></div>
</li>

<li> Reducción del efecto "BGP Wave", ayudando a la escalabilidad de Internet.
<div class="p"><!----></div>
</li>
</ul>
<div class="p"><!----></div>
</li>

<li> Stanford Campus Network:

<div class="p"><!----></div>
Una parte de la red del campus de la Universidad de Stanford, fue exitosamente migrada para soportar OpenFlow en el año 2010.
Inicialmente esta migración fue orientada a usuarios en particular (wireless), luego se agregaron los usuarios físicamente conectados por cable en el ala 3A del edificio William Gates, y finalmente se expandió en múltiples islas a través de 2 edificios.
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
 <a name="tth_sEc7"></a><h2>
7&nbsp;&nbsp;Red Privada Virtual</h2>
<a name="section2.7">
</a>

<div class="p"><!----></div>
En pocas palabras una Red Privada Virtual &#243; VPN por su sigla en ingl&#233;s, es la extension de una red privada sobre una infraestructura de red p&#250;blica como lo es por ejemplo Internet.

<div class="p"><!----></div>
Este concepto habilita a un equipo en una determinada subred privada a enviar y recibir información a otro equipo en otra subred separadas f&#237;sicamente, utilizando una infraestructura de red p&#250;blica o compartida para la comunicaci&#243;n entre ambas subredes, y de la misma forma que si estuviese directamente conectada a la red privada. Adem&#225;s permite mantener las políticas de seguridad y funcionalidades de la red privada.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
Las implementaciones de redes privadas se caracterizan entre otras cosas por las funcionalidades que son capaces de proveer. En relaci&#243;n a aspectos de seguridad se buscan principalmente funcionalidades de autenticaci&#243;n de usuarios, confidencialidad e integridad de los datos. Por otro lado en relaci&#243;n al procesamiento del tr&#225;fico dentro de la red p&#250;blica, algunas implementaciones de VPN ofrecen funcionalidades de clasificaci&#243;n de tr&#225;fico y calidad de servicios (QoS), permitiendo por ejemplo garantizar un ancho de banda m&#237;nimo para el tr&#225;fico de una organizaci&#243;n cuando es transportado a trav&#233;s de la red p&#250;blica de una red privada a otra. 

<div class="p"><!----></div>
Algunas implementaciones proveen adem&#225;s la capacidad de priorizar tipos de tr&#225;fico como por ejemplo el asociado a una aplicaci&#243;n en particular, o distribu&#237;r la carga entre diferentes caminos, lo que se conoce como balanceo de carga.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
En relaci&#243;n a la implementaci&#243;n de redes privadas, a lo largo de los años han surgido diferentes alternativas; diferenciándose por las funcionalidades que implementan y tambi&#233;n por las capas en el stack OSI en la cual operan. Algunos ejemplos son: 

<div class="p"><!----></div>

<ul>
<li> VPN tradicionales
	
<ul>
<li> Frame Relay (Capa 2)
<div class="p"><!----></div>
</li>

<li> ATM (Capa 2)
<div class="p"><!----></div>
</li>
</ul>
<div class="p"><!----></div>
</li>

<li> VPNs basadas en CPE
	
<ul>
<li> L2TP (Capa 2)
<div class="p"><!----></div>
</li>

<li> IPSec (Capa 2)
<div class="p"><!----></div>
</li>
</ul>
<div class="p"><!----></div>
</li>

<li> VPNs implementadas por proveedores de servicios
	
<ul>
<li> VPN de capa dos con MPLS (Capa 2)
<div class="p"><!----></div>
</li>

<li> BGP/MPLS VPNs de capa tres(Capa 3)
<div class="p"><!----></div>
</li>
</ul>
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Otra de las características por la cual es interesante diferenciar a dichas implementaciones, es la capacidad de conectar dos o m&#225;s puntos de una red privada, lo cual da lugar a redes privadas punto a punto &#243; multipunto.

<div class="p"><!----></div>

<h4>Redes Privadas Punto a Punto</h4>

<div class="p"><!----></div>
Las redes privadas punto a punto son un tipo de implementaci&#243;n orientada a conectar solamente dos extremos de la red de una organizaci&#243;n, utilizando lo que se denominan t&#250;neles sobre la infraestructura de un proveedor de servicios o Internet. T&#237;picamente se utilizan para conectar un cliente con un servidor en una organizacai&#243;n, o dos edificios de una misma organizaci&#243;n de una forma simple. 

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg9">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 9: Esquema de una VPN Punto a Punto</div>
<a name="fig:VPNPuntoAPunto">
</a>
</div>
<div class="p"><!----></div>
En este enfoque el tr&#225;fico de la subred A al llegar al nodo de borde, es encapsulado y enviado a trav&#233;s de un túnel sobre la red p&#250;blica. Al llegar al otro extremo del túnel el tr&#225;fico es desencapsulado y entregado al nodo de borde en la subred B, tomando luego el camino correspondiente dentro de esta subred en forma normal. Esto genera la percepci&#243;n desde ambas subredes que se tiene un “cable” entre ambos nodos de borde, conectando ambas subredes como si fuesen una sola red privada.

<div class="p"><!----></div>

<h4>Redes Privadas Multipunto</h4>

<div class="p"><!----></div>
Las redes privadas multipunto, extienden el enfoque punto a punto con el objetivo de brindar conectividad a una organizaci&#243;n dispersa geogr&#225;ficamente en m&#250;ltiples lugares.

<div class="p"><!----></div>
Permiten establecer dominios de difusi&#243;n compartidos, creando de forma transparente la ilusi&#243;n de que se tiene una gran LAN compuesta por varias subredes. Sin embargo al ser m&#225;s completas en funcionalidades y casos de uso soportados, tambi&#233;n son m&#225;s complejas de implementar.<br />

<div class="p"><!----></div>
Virtual Private LAN Service(VPLS), es una implementaci&#243;n de red privada multipunto, bastante difundida y provista de los dos enfoques definidos en los RFC4761[<a href="#kompella2007virtual" name="CITEkompella2007virtual">Kompella and Rekhter, 2007</a>] y RFC4762[<a href="#lasserre2007virtual" name="CITElasserre2007virtual">602007Lasserre and Kompella</a>]. Esta implementaci&#243;n crea dominios de difusión Ethernet utilizando diferentes tecnologías como IP/MPLS, L2TPv3 y túneles GRE.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg10">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 10: Esquema de una VPN Multipunto</div>
<a name="fig:VPNMulipunto">
</a>
</div>
<div class="p"><!----></div>
 En una red privada multipunto VPLS como la de la figura <a href="#fig:OF10MatchingFields">2.6</a>, compuesta por las subredes A, B y C, cuando se origina un paquete en un equipo de la subred A con destino a otro equipo en la subred B sucede lo siguiente: (1) primero el paquete es encaminado al nodo de borde de la subred A (Router VPN), (2) luego se resuelve dinamicamente a que punto de la VPN debe ser enviado el paquete(subred B &#243; subred C), para lo cual se utiliza un protocolo de ruteo de borde como BGP, despu&#233;s (3) el paquete es encapsulado dentro de MPLS y encaminado mediante conmutaci&#243;n de etiquetas hasta el nodo de borde en la subred B, finalmente (4) el paquete es desencapsulado y entregado a la subred B para su encaminamiento normal hacia el equipo destino.

<div class="p"><!----></div>
     <a name="tth_sEc7.1"></a><h3>
7.1&nbsp;&nbsp;Redes Privadas en Uruguay y proyecciones para la RAU</h3>
En Uruguay ANTEL (el principal proveedor de servicios de telecomunicaciones del país) ofrece servicios de redes privadas basados en tres tecnolog&#237;as principalmente: Lan to Lan con conexiones Ethernet punto a punto, VLAN Hub tmbi&#233;n con conexiones Ethernet y VPN IP/MPLS multipunto con QoS opcionalmente.<br />

<div class="p"><!----></div>
Dentro de las proyecciones existentes para la RAU2, se encuentran funcionalidades como calidad de servicios, clasificaci&#243;n de tr&#225;fico y priorizaci&#243;n del mismo de acuerdo a pol&#237;ticas definidas. A su vez se busca conectar m&#250;ltiples oficinas y organizaciones dispersas geogr&#225;ficamente en todo el país, manteniendo las pol&#237;ticas de seguridad de la red privada de cada organizaci&#243;n. 

<div class="p"><!----></div>
De acuerdo a las implementaciones mencionadas anteriormente, la soluci&#243;n que m&#225;s se adapta a los requerimientos de la RAU2 es una VPN IP/MPLS multipunto con funcionalidades de QoS, en donde el rol de proveedor de servicios lo asume la propia red académica o la organizaci&#243;n encargada de su mantenimiento(SeCIU).<br />

<div class="p"><!----></div>
Para complementar la lectura en relaci&#243;n a la arquitectura de una implementaci&#243;n de VPN IP/MPLS Multipunto recomendamos continuar con RFC2547[<a href="#rosen1999bgp" name="CITErosen1999bgp">Rosen and Rekhter, 1999</a>] y RFC4364[<a href="#rosen2006bgp" name="CITErosen2006bgp">E.&nbsp;Rosen, 2006</a>].

<div class="p"><!----></div>
 <a name="tth_sEc8"></a><h2>
8&nbsp;&nbsp;Multiprotocol Label Switching</h2>
<a name="section2.8">
</a>

<div class="p"><!----></div>
Multiprotocol Label Switching(MPLS) o Multiprotocolo de intercambio de etiquetas, es un mecanismo de transporte de datos desarrollado y estandarizado por el IETF, en particular por los RFC 3031[<a href="#rosen2001multiprotocol" name="CITErosen2001multiprotocol">752001bRosen et&nbsp;al.</a>], 3032[<a href="#rosen2001mpls" name="CITErosen2001mpls">Rosen et&nbsp;al., 2001a</a>] y 3814[<a href="#nadeau2004multiprotocol" name="CITEnadeau2004multiprotocol">Nadeau et&nbsp;al., 2004</a>].<br />

<div class="p"><!----></div>
De acuerdo al modelo OSI de capas, MPLS opera entre las capas de Red y Enlace. Fue diseñado para unificar el servicio de transporte de datos basado en circuitos virtuales y basado en conmutación de paquetes.<br />

<div class="p"><!----></div>
Actualmente cuenta con una gran adopción por parte de vendedores, y poco a poco se ha convertido en el mecanismo por elección para construir plataformas de red empresariales, sustituyendo poco a poco los servicios de Frame Relay y ATM.<br />

<div class="p"><!----></div>
MPLS incluye una gran cantidad de características entre las cuales se distinguen soporte para VPNs, Ingeniería de Tráfico, calidad de servicios (QoS), clases de servicios (CoS), a la vez que es independiente del protocolo de capa de enlace pudiendo operar sobre ATM ó Ethernet por ejemplo.<br />

<div class="p"><!----></div>
En una red MPLS, cuando un paquete ingresa se le coloca un cabezal mpls. Este cabezal esta compuesto por cuatro campos: Label, Exp, S, TTL. De ellos se destacan el valor de la etiqueta mpls, utilizado para encaminar paquetes a través de la red hacia su nodo destino, y eventualmente realizar clasificación de tr&#225;fico.

<div class="p"><!----></div>
En cada nodo de la red, el cabezal mpls es extraído y analizado para determinar el próximo nodo al que se debe enviar dicho paquete. Luego del procesamiento un nuevo cabezal mpls con otra etiqueta es colocado en el paquete y se reenvía hacia el siguiente nodo en la red.

<div class="p"><!----></div>
De esta forma el paquete es encaminado dentro de la red mpls mediante un proceso de extracción y colocación de etiquetas, hasta que el paquete arriba a un nodo de salida de la red, en donde el paquete ya no contiene etiquetas.<br />

<div class="p"><!----></div>
Por otro lado, un paquete mpls puede presentar más de un cabezal MPLS, uno arriba del otro. Esta superposicion de etiquetas se conoce bajo el nombre de pila o stack mpls. De esta forma se pueden agregar más variables al servicio que simplemente encaminar paquetes en la red, como puede ser distinguir entre diferentes tipos de tráfico.<br />

<div class="p"><!----></div>
Otro punto a destacar de este protocolo es la configuración del plano de reenvío. El plano de control de las redes mpls se componen de un algoritmo de ruteo y un algoritmo de distribución de etiquetas. Mientras que con el primero se calculan los mejores caminos que un paquete debe tomar para atravesar la red, el segundo se encarga de la asignación y distribución de etiquetas para que cada switch conmute correctamente el paquete hasta su destino.<br />

<div class="p"><!----></div>
Alguno de los principales términos en la literatura MPLS son:

<div class="p"><!----></div>

<ul>
<li> LER(Label Edge Router): Elemento de la red MPLS por la cual ingresa o egresa tráfico hacia o desde la misma.
<div class="p"><!----></div>
</li>

<li> LSR(Label Switching Router): Elemento de la red MPLS que conmuta etiquetas.
<div class="p"><!----></div>
</li>

<li> LSP(Label Switched Path): Nombre genérico que se le asigna a un camino MPLS para tráfico de una determinada FEC, es decir un túnel MPLS establecido entre dos extremos de la red.
<div class="p"><!----></div>
</li>

<li> LDP(Label Distribution Protocol): Protocolo de distribución de etiquetas MPLS entre los equipos de la red.
<div class="p"><!----></div>
</li>

<li> FEC(Forwarding Equivalence Class): Nombre asignado al tráfico que es encaminado bajo una misma etiqueta. En otras palabras, es una clase de equivalencia de tráfico a la cual se le asigna una etiqueta.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Se ha presentado un resumen del trabajo de estudio en el estado del arte en las redes definidas por software y se han introducido los principales conceptos para el correcto entendimiento de este trabajo. Resta entonces presentar las conclusiones obtenidas en el trabajo de estudio del estado del arte del hardware NetFPGA. La siguiente secci&#243;n esta destinada a esta empresa.

<div class="p"><!----></div>
 <a name="tth_sEc9"></a><h2>
9&nbsp;&nbsp;NetFPGA</h2>
<a name="section2.9">
</a>

<div class="p"><!----></div>
NetFPGA[<a href="#NetFPGA1" name="CITENetFPGA1">352015hNet</a>] es una plataforma de hardware reconfigurable y software opensource, flexible y potente, diseñada para un uso principalmente académico en tareas de investigación y enseñanza.<br />

<div class="p"><!----></div>
El hardware esta basado en un chip FPGA(Field Programmable Gate Array), que puede ser programado  utilizando el lenguaje VHDL. Cuenta con una amplia variedad de proyectos desarrollados tanto por el equipo de NetFPGA(proyectos de referencia), como por instituciones académicas y comerciales involucradas(proyectos comunitarios) que permiten programar el comportamiento del hardware  de diferentes formas. Se pueden encontrar desde proyectos que programan el hardware con una implementación de referencia de una tarjeta de red (Reference NIC), como un Learning CAM Switch, Simple Router, Router OpenFlow, hasta un generador de tr&#225;fico entre otros(por mayor informaci&#243;n ver [<a href="#NetFPGA2" name="CITENetFPGA2">Net, 2015d</a>]).<br />

<div class="p"><!----></div>
Surge en el año 2007 como un proyecto de investigación en la Universidad de Stanford, bajo el nombre de NetFPGA-1G, con la consigna de construir una plataforma de desarrollo e investigación basada en un chip FPGA.

<div class="p"><!----></div>
La plataforma alcanz&#243; una versi&#243;n comercial consistente en una placa PCI con un chip Xilinx Virtex-II proFPGA y cuatro interfaces Ethernet de 1Gigabit, m&#225;s un repositorio de código fuente descargable y abierto a la comunidad, conteniendo librerías IP y unos pocos ejemplos de diseño.<br />

<div class="p"><!----></div>
El proyecto prosper&#243; vendiendo mas de 2.600 tarjetas en 150 instituciones educativas en 15 países diferentes del mundo hasta el momento en que se discontinuo dicho producto.<br />

<div class="p"><!----></div>
Tras el éxito de la NetFPGA-1G, en el año 2009 comenzó a trabajarse en la NetFPGA-10G una version m&#225;s potente que la  anterior basada en un chip Xilinx Virtex5 y cuatro interfaces 10-Gigabit SFP+, que remplazara a la NetFPGA-1G. Tras iniciarse su comercialización en el año 2011 la misma cosech&#243; un éxito similar a su antecesora llegando a vender mas de 470 unidades al año 2014, y contando con una plataforma de al menos 15 proyectos desarrollados específicamente para esta plataforma.<br />

<div class="p"><!----></div>
A la fecha, NetFPGA cuenta con cuatro versiones de plataformas comerciales, la NetFPGA-1G(discontinuada), NetFPGA-10G, NetFPGA CML y NetFPGA-SUME. A su vez la plataforma se ha popularizado entre investigadores y desarrolladores como plataforma ideal para la experimentación e innovación, contando a la fecha con mas de 226 artículos académicos[<a href="#NetFPGA4" name="CITENetFPGA4">292015cNet</a>] en diversas áreas, utilizando como plataforma de trabajo NetFPGA.<br />

<div class="p"><!----></div>
En resumen NetFPGA es una plataforma robusta que propicia a investigadores, docentes y estudiantes en el área de redes de computadoras a la construcción de prototipos sobre sistemas de redes de alta velocidad y acelerados por hardware, en poco tiempo y con un costo inferior a otras alternativas de prototipaci&#243;n.<br />

<div class="p"><!----></div>
En este trabajo, se utiliza la plataforma NetFPGA-10G para la prototipaci&#243;n de un dispositivo compatible con OpenFlow, a partir del cual a su vez implementar una red prototipo basada en el enfoque OpenFlow/SDN. 

<div class="p"><!----></div>
 <a name="tth_chAp3"></a><h1>
Capítulo 3 <br />An&#225;lisis del problema</h1>

<div class="p"><!----></div>

    


<div class="p"><!----></div>
El primer paso en el proceso de construcción del prototipo para la RAU2, es el análisis del problema planteado. Definir en función de requerimientos el alcance del prototipo, investigar qu&#233; alternativas se tienen para construir un nodo del prototipo a partir del hardware NetFPGA, resolver qu&#233; estrategia ser&#225; la utilizada para implementar servicios de VPNs, entre otros aspectos.<br />

<div class="p"><!----></div>
El presente cap&#237;tulo est&#225; destinado al planteo de estos aspectos y los fundamentos sobre los que se basan las decisiones de diseño. 

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>
 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Definición de requerimientos</h2>
<a name="3.1">
</a>

<div class="p"><!----></div>
Para identificar posibles requerimientos para el prototipo de la RAU2 a implementar en este proyecto, inicialmente se trabaja en la identificación de requerimientos sobre la RAU2 en general; contextualizando luego los resultados obtenidos en el alcance, los objetivos y resultados esperados.<br />

<div class="p"><!----></div>
Basándose en [ponemos alguna referencia al borrador que nos paso eduardo...] se esperan las siguientes funcionalidades de la RAU2:

<div class="p"><!----></div>

<ol type="1">
<li> <b>Clasificación de tráfico:</b> Una de las necesidades y objetivos de la futura actualizaci&#243;n de la RAU, es la facilidad para clasificar tráfico. Alineado con las actuales necesidades, en particular se precisa al menos diferenciar las siguientes 3 categorías: (a) público, (b) académico y (c) servicios de contenido.
<div class="p"><!----></div>
</li>

<li> <b>Grandes volúmenes de datos (BIG Data):</b> En la RAU intervienen instituciones como el Instituto Pasteur, Centro Uruguayo de Imagenología Molecular (CUDIM) entre otros, en donde la generación e intercambio de grandes volúmenes de datos como lo son los exámenes PET del Pasteur o una secuenciación de ADN del CUDIM entre otros.
<div class="p"><!----></div>
</li>

<li> <b>Escalabilidad:</b> Se espera alcanzar en un mediano plazo un total de 11.000 docentes, 7.000 funcionarios y 140.000 estudiantes, por lo que el prototipo para la RAU2 debe ser escalable en la cantidad de usuarios en la infraestructura.
<div class="p"><!----></div>
</li>

<li> <b>Red de entrega de contenidos (Content Delivery Network):</b> Resulta sumamente útil tomar un enfoque de red de entrega de contenidos para el diseño de la red académica. Las organizaciones partícipes de la misma tienen y generan grandes volúmenes de información de gran interés por parte de otras organizaciones de la RAU. Una red de distribución de contenidos garantiza un mejor acceso en tiempo real a dicha información por parte de múltiples organizaciones en simultáneo.
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
De por s&#237; este conjunto de funcionalidades o requerimientos representa un problema de dimensiones mayores al que se pretende resolver en este trabajo, por ello pensando en resolver un problema acorde a las dimensiones de un proyecto de fin de carrera, se decide enfocarse en el primer requerimiento, la Clasificación de Tr&#225;fico.

<div class="p"><!----></div>
Partiendo de la clasificación de tr&#225;fico como funcionalidad principal, se relevan los siguientes requerimientos:

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb1">
</a> <div style="text-align:center">
<table border="1">
<tr><td colspan="1" align="center">Requerimientos Funcionales</td></tr>
<tr><td align="left">
<ul>
<li> Dada una organización o aplicación, el Sistema debe tener la capacidad de clasificar y distinguir el tráfico asociada a la misma de cualquier otro tipo de tráfico. Esto se conoce como clasificación de trafico.
<div class="p"><!----></div>
</li>

<li> Dado el tipo de tr&#225;fico asociado a una organización, el Sistema debe poder asignar un porcentaje de los recursos disponibles de la red para el procesamiento del mismo.
<div class="p"><!----></div>
</li>

<li> Dado el tipo de tr&#225;fico asociado a una organización, el Sistema debe poder establecer m&#225;s de un camino en la red para transportar dicho tr&#225;fico implementando lo que se conoce como balanceo de carga.
<div class="p"><!----></div>
</li>

<li> Dadas dos subredes asociadas a una misma organización e interconectadas mediante el Sistema, se debe garantizar que la  numeración IP del tráfico generado por una de las subredes sea mantenida al ser entregado a la segunda subred, manteniendo de esta forma la identidad de los usuarios en ambas subredes.
<div class="p"><!----></div>
</li>
</ul></td></tr></table>

</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb1">
</a> <div style="text-align:center">
<table border="1">
<tr><td colspan="1" align="center">Requerimientos no Funcionales</td></tr>
<tr><td align="left">
<ul>
<li> Open Source: En la medida que sea posible interesa utilizar herramientas y componentes libres y abiertas como software libre y de código abierto, hardware libre, etc.
<div class="p"><!----></div>
</li>
</ul></td></tr></table>

</div>
<div class="p"><!----></div>
Se tiene ya una noción básica del sistema que se pretende construir, interesa entonces analizar por qu&#233; se quiere utilizar el enfoque de SDN en el mismo.

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;¿Por qu&#233; utilizar SDN?</h2>

<div class="p"><!----></div>
El mecanismo estándar y transparente que propone SDN para manipular los diferentes dispositivos de red (se han hecho grandes esfuerzos para estandarizar algunas de las implementaciones de la Interfaz Sur existentes, OpenFlow es un buen ejemplo de ellos) brinda flexibilidad y agilidad para el desarrollo de nuevos protocolos y servicios de redes en una infraestructura de dispositivos heterogénea, facilitando as&#237; la innovaci&#243;n en el &#225;rea. 

<div class="p"><!----></div>
A su vez, genera una independencia sobre la implementaci&#243;n del hardware propuesta por cierto fabricante. Actualmente utilizar productos de un determinado fabricante para la construcci&#243;n de servicios, muchas veces implica configurar equipos y desarrollar aplicaciones sobre la plataforma del mismo, la cual usualmente tiene su propia API de operaciones cerrada y propio lenguaje de desarrollo. Esto implica dificultades para la incorporaci&#243;n de nuevos dispositivos a una red existente, eventualmente de diferente fabricante, dificultades para la incorporaci&#243;n de nuevos servicios y m&#225;s dificultades para migrar servicios existentes, eventualmente a una plataforma diferente.<br />

<div class="p"><!----></div>
En pocas palabras, SDN facilita notablemente la investigaci&#243;n e innovaci&#243;n en el desarrollo de nuevos servicios y protocolos, eliminando barreras en la manipulaci&#243;n de los diferentes dispositivos de red, y ganando libertad de los fabricantes de hardware.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;¿Porque utilizar NetFPGA?</h2>

<div class="p"><!----></div>
Se necesita para el desarrollo de este proyecto una plataforma tecnol&#243;gica compatible con SDN y en particular con el protocolo OpenFlow.

<div class="p"><!----></div>
NetFPGA en especial posibilita la construcci&#243;n de una plataforma de desarrollo y pruebas compatible con OpenFlow a un precio significativamente menor que el de un equipo comercial, con funcionalidades y capacidades suficientes para el desarrollo del prototipo. A su vez se quiere estudiar la aplicabilidad del enfoque OpenFlow/SDN para la construcción de la RAU2, para lo cual es necesario adem&#225;s de la construcción de un prototipo e implementar algunos casos de uso representativos sobre el mismo. 

<div class="p"><!----></div>
Es imperativo entonces la construcción de un laboratorio de pruebas, compuesto por un conjunto de nodos nodos; lo cual a partir del hardware comercial tendría un costo significativamente mayor que a partir del hardware NetFPGA.<br />

<div class="p"><!----></div>
Por otro lado utilizar hardware reconfigurable tiene la ventaja de poder prototipar diferentes dispositivos de red en un mismo equipo. Se puede entonces modificar dinamicamente el laboratorio de pruebas en función de los casos de uso que se quieran probar sin la necesidad de incorporar nuevo hardware. Ademas la capacidad para programar el hardware, permite sacar provecho de las características del mismo para la implementaci&#243;n de funcionalidades especificas para la RAU, que en hardware comercial solo se podrían implementar si el mismo ya tiene incorporadas dichas funcionalidades.<br />

<div class="p"><!----></div>
Finalmente la utilizaci&#243;n de hardware abierto, en conjunci&#243;n con software libre y de código abierto posibilitan la construcci&#243;n de lo que se podría llamar “router open source”, un nodo para la nueva red académica basado en tecnologías abiertas. Esto otorgaría una mayor flexibilidad y capacidades de reutilizaci&#243;n del hardware disponible en la red académica avanzada.

<div class="p"><!----></div>
 <a name="tth_sEc4"></a><h2>
4&nbsp;&nbsp;Qu&#233; arquitectura basada en SDN utilizar</h2>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>
Naturalmente, otra de las decisiones de dise&#241;o importantes a tomar es la elecci&#243;n de una de las arquitecturas existentes basadas en el enfoque SDN.<br />

<div class="p"><!----></div>
Dentro de las alternativas existentes, OpenFlow se presenta como una opci&#243;n madura y probada&nbsp;[<a href="#Ofelia" name="CITEOfelia">Ofe, 2015</a>]. Como se mencion&#243; en el estado del arte OpenFlow  se ha caracterizado por un desarrollo sostenido y una amplia adopci&#243;n tanto por la academia como por la industria, adem&#225;s de ser compatible con una amplia variedad de tecnolog&#237;as. Debido a esto cuenta con una amplia comunidad de usuarios, extensa documentaci&#243;n y es soportado en varios productos comerciales[<a href="#Pica8" name="CITEPica8">Pic, 2015</a>][<a href="#HP" name="CITEHP">HP, 2015</a>][<a href="#Centec" name="CITECentec">Cen, 2015</a>][<a href="#SDNProductlist" name="CITESDNProductlist">SDN, 2015</a>]. Finalmente se puede agregar que OpenFlow est&#225; integramente desarrollado bajo la filosof&#237;a opensource. 

<div class="p"><!----></div>
Por estas razones, se decide utilizar OpenFlow como la implementaci&#243;n de SDN en el desarrollo del prototipo.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>
Como todo protocolo, OpenFlow cuenta con varias versiones en las que se incluyen, conforme el protocolo fue evolucionando funcionalidades nuevas. Se debe decidir entonces que versi&#243;n del protocolo utilizar.<br />

<div class="p"><!----></div>
Al momento de culminar este trabajo la versi&#243;n m&#225;s reciente del protocolo era la 1.5, mientras que al momento de iniciarse este proyecto la versi&#243;n m&#225;s reciente era la 1.4. 

<div class="p"><!----></div>
Por otro lado la versi&#243;n de OpenFlow elegida debe garantizar ciertos requerimientos de dise&#241;o a los que el prototipo est&#225; sujeto. En particular interesa que garantice:

<div class="p"><!----></div>

<ul>
<li> Soporte para MPLS, tanto en la capacidad de reconocer los cabezales como para la manipulaci&#243;n de los mismos mediante las primitivas POP, PUSH y SWAP
<div class="p"><!----></div>
</li>

<li> Soporte para calidad de servicios(QoS)
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Sobre la primer restricci&#243;n, OpenFlow brinda soporte completo para MPLS a partir de la versi&#243;n 1.3.1, mientras que en relaci&#243;n a QoS OpenFlow incorpora el concepto de métricas por flujo a partir de la version 1.3.1, orientado a implementar funcionalidades de QoS.<br />

<div class="p"><!----></div>
De esta forma la versi&#243;n m&#225;s b&#225;sica de OpenFlow que asegura soporte a las restricciones de dise&#241;o mencionadas es la versi&#243;n 1.3.1.<br />

<div class="p"><!----></div>
M&#225;s adelante veremos en la siguiente secci&#243;n, la cual  est&#225; destinada a la programaci&#243;n de la placa NetFPGA, que es de inter&#233;s trabajar con la versi&#243;n de OpenFlow m&#225;s sencilla y minimalista posible que d&#233; soporte a todas las funcionalidades y restricciones impuestas sobre el prototipo.<br />

<div class="p"><!----></div>
En resumen la versi&#243;n de OpenFlow a utilizar es la 1.3.1.

<div class="p"><!----></div>
 <a name="tth_sEc5"></a><h2>
5&nbsp;&nbsp;¿Qu&#233; implementaci&#243;n de Controlador OpenFlow utilizar?</h2>

<div class="p"><!----></div>
 <a name="tth_sEc6"></a><h2>
6&nbsp;&nbsp;Alternativas de dise&#241;o para el router</h2>

<div class="p"><!----></div>
Como se menciona en el cap&#237;tulo 1, una de las premisas para la construcción del prototipo es la utilizaci&#243;n del hardware NetFPGA. El mismo es utilizado en la construcción de cada nodo del prototipo; por lo que asumiendo el uso de OpenFlow en la arquitectura, es necesario partiendo del mismo obtener nodos compatibles con OpenFlow.<br />

<div class="p"><!----></div>
Existen dos estrategias bien definidas para la construcci&#243;n un switch OpenFlow partiendo del hardware mencionado y utilizando los diferentes proyectos de la plataforma. Una de ellas es programar el hardware para que se comporte como un switch compatible con el protocolo OpenFlow. La otra alternativa es programar el hardware para que se comporte como una tarjeta de red estándar, e implementar todo el comportamiento de un switch OpenFlow en software.<br />

<div class="p"><!----></div>
Para la primer estrategia se cuenta con un proyecto de la plataforma y disponible libremente en el repositorio de c&#243;digo fuente. No obstante este proyecto presenta una dificultad y es que a pesar de haber sido dise&#241;ado para soportar en un futuro cualquier versi&#243;n disponible del protocolo OpenFlow, en su veri&#243;n actual solamente soporta un conjunto reducido de funcionalidades de la versi&#243;n 1.0 de dicho protocolo. 

<div class="p"><!----></div>
Como se mencion&#243; anteriormente la versi&#243;n m&#225;s b&#225;sica de este protocolo que permite soportar el conjunto de funcionalidades relevadas como requerimientos es la versi&#243;n 1.3. Esto conlleva a la necesidad de extender el proyecto existente, para soprtar las nuevas caracter&#237;sticas incorporadas en las sucesivas versiones posteriores a la 1.0, &#243; al menos aquellas que son esenciales para soportar las caracter&#237;sticas pretendidas sobre el prototipo.<br />

<div class="p"><!----></div>
Para la segunda estrategia se cuenta con un proyecto de la plataforma NetFPGA denominado ReferenceNIC. Este proyecto habilita a programar el hardware para que se comporte como una placa de red estándar. Adicionalmente se debe incluir o desarrollar herramientas que permitan implementar por software el comportamiento de un switch OpenFlow. En particular sobre este &#250;ltimo punto vale la pena destacar la existencia de Open vSwitch, herramienta que entre otras caracter&#237;sticas realiza esto mismo utilizando hardware convencional como una placa de red estándar.<br />

<div class="p"><!----></div>
A continuación se exponen comparativamente las principales ventajas y desventajas de cada alternativa.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb1">
</a> <div style="text-align:center"><div class="small">
<table border="1">
<tr><td colspan="2" align="center">Ventajas</td></tr><tr><td></td></tr>
<tr><td align="left">Extender proyecto OpenFlow NetFPGA </td><td align="left">ReferenceNIC + Open vSwitch</td></tr>
<tr><td align="left">
<ul>
<li> &#211;ptimo aprovechamiento de la capacidad de c&#243;mputo y procesamiento del hardware disponible.
<div class="p"><!----></div>
</li>

<li> Mayor posibilidad de lograr velocidades de procesamiento competitivas con productos comerciales similares.
<div class="p"><!----></div>
</li>

<li> Mayor posibilidad de obtener resultados aceptables en performance y rendimiento para puesta en producción.
<div class="p"><!----></div>
</li>
</ul>
</td><td align="left">
<ul>
<li> No se tiene la necesidad de modificar o desarrollar software en el lenguaje y en el entorno de programaci&#243;n de la tarjeta NetFPGA.
<div class="p"><!----></div>
</li>

<li> Evitar desarrollar software para la NetFPGA ahorra tiempo del proyecto que se puede invertir en otras l&#237;neas de trabajo igualmente importantes.
<div class="p"><!----></div>
</li>

<li> Programar el hardware NetFPGA con proyectos precompilados como el ReferenceNIC requiere &#250;nicamente de licencias de software que son accesibles sin costo ya sea mediante licencias gratuitas o de prueba.
<div class="p"><!----></div>
</li>
</ul>
</td></tr></table>


<div style="text-align:center">Table 1: OpenFlow NetFPGA vs ReferenceNIC + Open vSwitch - Ventajas</div>
</div></div>
<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb2">
</a> <div style="text-align:center"><div class="small">
<table border="1">
<tr><td colspan="2" align="center">Desventajas</td></tr><tr><td></td></tr>
<tr><td align="left">Extender proyecto OpenFlow NetFPGA </td><td align="left">ReferenceNIC + Open vSwitch</td></tr>
<tr><td align="left">
<ul>
<li> Extender el proyecto existente en s&#237; mismo constituye un empresa del porte de un proyecto de fin de carrera
<div class="p"><!----></div>
</li>

<li> El conocimiento técnico necesario se perfila m&#225;s al de un Ingeniero Eléctrico que al de un Ingeniero en Computación lo cual constituye un riesgo del proyecto.
<div class="p"><!----></div>
</li>

<li> Desarrollar software para el hardware NetFPGA y compilarlo requiere de licencias de software costosas.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
</td><td align="left">
<ul>
<li> No se aprovecha de forma óptima las capacidades de procesamiento del hardware disponible. En otras palabras se tiene hardware "caro" y potente en forma ociosa.
<div class="p"><!----></div>
</li>

<li> Los resultados obtenidos en relaci&#243;n al rendimiento del prototipo muy probablemente no sean los esperados para un equipo de producción.
<div class="p"><!----></div>
</li>
</ul>
</td></tr></table>


<div style="text-align:center">Table 2: OpenFlow NetFPGA vs ReferenceNIC + Open vSwitch - Desventajas</div>
</div></div>
<div class="p"><!----></div>
Teniendo presente el alcance del proyecto y el tiempo disponible para su ejecuci&#243;n, se opt&#243; por la  estrategia de programar el hardware con el proyecto ReferenceNIC e implementar en software funcionalidades de OpenFlow. 

<div class="p"><!----></div>
Con esta estrategia se logra obtener en forma rápida un prototipo de switch OpenFlow con el cual trabajar en la implementaci&#243;n del plano de control, desarrollando estrategias para construir servicios en una red h&#237;brida IP/MPLS, as&#237; como dise&#241;ar casos de uso y un laboratorio de pruebas que permitan validar tecnol&#243;gicamente la soluci&#243;n propuesta.<br />


 <a name="tth_chAp4"></a><h1>
Capítulo 4 <br />Dise&#241;o e Implementaci&#243;n del prototipo</h1>

<div class="p"><!----></div>

    


<div class="p"><!----></div>
Este cap&#237;tulo esta destinado a la comprensi&#243;n de los aspectos principales que hacen al dise&#241;o e implementaci&#243;n del prototipo para la RAU2. Cabe destacar que en el mismo se toman como punto de partida las decisiones asumidas en el cap&#237;tulo anterior.

<div class="p"><!----></div>
Por otro lado aspectos muy t&#233;cnicos o problemas encontrados en cada componente de la arquitectura son tratados en profundidad en respectivos ap&#233;ndices, citándose aquí las referencias en caso que corresponda para complementar su lectura.<br />

<div class="p"><!----></div>
 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Lineamientos generales</h2>

<div class="p"><!----></div>
Como se menciona en el cap&#237;tulo destinado al estado del arte, la arquitectura del prototipo a construir esta orientada a la implementaci&#243;n de servicios de VPN IP/MPLS Multipunto.

<div class="p"><!----></div>
Sin embargo varios aspectos de la arquitectura a&#250;n deben ser definidos. Es necesario definir la estrategia para calcular las rutas o caminos en la red del prototipo(algoritmo de ruteo), la forma en que se asignan y distribuyen etiquetas mpls para implementar el plano de reenvío(algoritmo de distribución de etiquetas), como se implementa la clasificaci&#243;n de tr&#225;fico y de que forma se implementan múltiples caminos para un mismo destino, ya sea para priorizar tr&#225;fico por tipo de aplicaci&#243;n por ejemplo o simplemente para hacer balanceo de carga. 

<div class="p"><!----></div>
A continuaci&#243;n se explica de que forma el prototipo implementa cada uno de los aspectos anteriores.

<div class="p"><!----></div>
     <a name="tth_sEc1.1"></a><h3>
1.1&nbsp;&nbsp;Algoritmo de Ruteo</h3>

<div class="p"><!----></div>
Una alternativa para el c&#225;lculo de rutas en el prototipo es utilizar protocolos de ruteo en base a IP, sacando provecho as&#237; de algoritmos existentes, robustos y ampliamente probados tanto en la academia como en la industria.<br />

<div class="p"><!----></div>
Hist&#243;ricamente se han utilizado algoritmos distribuidos para el computo de rutas, siendo algunos de los m&#225;s renombrados en la literatura OSPF[<a href="#moy1998rfc" name="CITEmoy1998rfc">661998Moy</a>], RIP[<a href="#malkin1994rip" name="CITEmalkin1994rip">621994Malkin</a>] y IS-IS[<a href="#routingprotocol" name="CITEroutingprotocol">771990Routing</a>]. Por otro lado también puede sacarse provecho de la visi&#243;n global del plano de control de SDN para implementar un algoritmo de ruteo centralizado, sencillo y transparente.

<div class="p"><!----></div>
En este trabajo se combinan ambos enfoques, utilizando el protocolo de ruteo OSPF en conjunto con un algoritmo centralizado en el controlador.<br />

<div class="p"><!----></div>
Se decide utilizar OSPF por la sencilla razón de que es una opción robusta y confiable, adem&#225;s de que existen buenas implementaciones en software libre (por ejemplo Quagga[<a href="#Quagga" name="CITEQuagga">412015Qua</a>] y BIRD[<a href="#BIRD" name="CITEBIRD">172015BIR</a>]).

<div class="p"><!----></div>
OSPF basa su algoritmo para el c&#225;lculo de rutas en el algoritmo Dijkstra(ampliamente conocido para el calculo de mejores caminos en un grafo) y utiliza como m&#233;trica el costo asociado a cada enlace. Este costo es ingresado manualmente por el administrador de la red y es un n&#250;mero &#250;nico, as&#237; que es necesario definir una relaci&#243;n matem&#225;tica entre las diferentes caracter&#237;sticas de un enlace a considerar (ancho de banda disponible, tecnolog&#237;a, latencia, etc.) y el valor en cuestión.

<div class="p"><!----></div>
Sin embargo, para la definición de políticas en ingeniería de tr&#225;fico que permitan hacer balanceo de carga y construcción de caminos alternativos, es necesario incorporar m&#225;s informaci&#243;n en el algoritmo de ruteo, como la saturaci&#243;n de enlaces por ejemplo entre otras restricciones.

<div class="p"><!----></div>
Esto es equivalente a extender OSPF para implementar un algoritmo de ruteo con restricciones, lo que en algunas implementaciones se denomina CSPF(Constrained Shortest Path First). Adem&#225;s interesa que estas restricciones puedan definirse dinamicamente en función de la alta y baja de servicios en el prototipo.<br />

<div class="p"><!----></div>
En el prototipo se ejecuta el protocolo OSPF en cada nodo y el propio controlador para construir y mantener una base de datos topologica de la red. Luego esta información es utilizada por el algoritmo CSPF en el controlador para el computo de las mejores rutas asociadas al tr&#225;fico de cada servicio de VPN.<br />

<div class="p"><!----></div>
La clasificaci&#243;n de tr&#225;fico, creación de m&#250;ltiples caminos y balanceo de carga son implementados en el controlador a través de una aplicaci&#243;n SDN, por lo que ser&#225;n explicados m&#225;s adelante en el capitulo 5.

<div class="p"><!----></div>
     <a name="tth_sEc1.2"></a><h3>
1.2&nbsp;&nbsp;Algoritmo de distribución de etiquetas</h3>

<div class="p"><!----></div>
Para la implementaci&#243;n de un algoritmo de distribución de etiquetas, se pueden tomar también dos enfoques posibles: enfoque centralizado y enfoque distribuido.<br />

<div class="p"><!----></div>
Siguiendo un enfoque distribuido una de las propuestas existentes es el protocolo LDP(Label distribution Protocol), del cual adem&#225;s se cuenta con la implementaci&#243;n ofrecida por Quagga-LDP. 

<div class="p"><!----></div>
Esta herramienta es una extension de Quagga, para brindar adem&#225;s soporte al protocolo LDP. Al igual que Quagga, el cual funciona interactuando con el kernel de Linux para la construcci&#243;n de la tabla de ruteo, Quagga-LDP interact&#250;a con el kernel de Linux para la construcci&#243;n de dos tablas. Estas dos tablas implementan los conceptos de FTN (Fec To NHLFE), ILM (Incoming Label Mapping) y NHLFE (Next Hop Label Forwarding Entry) en la definición de MPLS, los cuales determinan la forma en que un paquete es procesado y reenviado en un nodo.

<div class="p"><!----></div>
Quagga-LDP toma como entrada el resultado de la ejecuci&#243;n del protocolo OSPF, para luego ejecutar el protocolo LDP y así poblar las tablas mencionadas.

<div class="p"><!----></div>
Como Linux no ofrece nativamente soporte al protocolo MPLS, Quagga-LDP funciona en conjunto con MPLS-Linux; un kernel de Linux modificado para ofrecer una implementaci&#243;n de MPLS a nivel de kernel.<br />

<div class="p"><!----></div>
Utilizar Quagga-LDP y MPLS-Linux tiene dos ventajas principalmente: en primer lugar no se debe implementar un algoritmo propio con lo cual se ahorra tiempo de implementaci&#243;n y se tiene un algoritmo probado y confiable; en segundo lugar se disminuye la carga de computo en el controlador puesto que se ejecuta el algoritmo en cada nodo.

<div class="p"><!----></div>
Por ello en este trabajo se decide probar esta alternativa y experimentar con el algoritmo de distribución de etiquetas implementado por Quagga-LDP.<br />

<div class="p"><!----></div>
Se dedica un tiempo considerable a la instalaci&#243;n de las herramientas Quagga-LDP y MPLS-Linux para lo cual es necesario recompilar diferentes versiones de kernel de Linux intentando encontrar una versi&#243;n apropiada para la configuración de herramientas con la que se viene trabajando en la arquitectura del prototipo. Ademas existen dos versiones bien diferentes de la implementaci&#243;n MPLS-Linux: una versi&#243;n original del año 2005 de la cual existe poca documentación y una versi&#243;n m&#225;s nueva basada en la primera que data del año 2011, de la cual se cuenta con a&#250;n menos documentación. 

<div class="p"><!----></div>
Por un lado se prueba con ambas versiones, teniendo problemas para que el ambiente funcione correctamente (ver apéndice <a href="#apendiceB6">B.6</a> por mayores detalles). Mientras que por otro lado se evidenci&#243; dificultades para la integración entre la salida del protocolo LDP y el algoritmo de ruteo semidin&#225;mico definido en la sección anterior.

<div class="p"><!----></div>
Por ello se decide implementar desde cero un algoritmo de distribución de etiquetas centralizado en el controlador. Los detalles de implementaci&#243;n del mismo ser&#225;n explicados m&#225;s adelante en la sección <a href="#section5.5.4">5.5.4</a>.<br />

<div class="p"><!----></div>
A continuación se explica la arquitectura del prototipo. 

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Arquitectura del Prototipo</h2>

<div class="p"><!----></div>
La arquitectura del prototipo (ver figura &nbsp;<a href="#fig:OpenSourceRArch0">4.1</a>), sigue la arquitectura del enfoque OpenFlow/SDN como se mencion&#243; anteriormente. En concordancia con esta arquitectura en el prototipo se tienen dos componentes importantes: el controlador SDN y el switch compatible con OpenFlow.<br />

<div class="p"><!----></div>
En el controlador SDN se ejecuta RAUFlow, la aplicaci&#243;n encargada de proveer una implementaci&#243;n para servicios de redes privadas virtuales. A su vez esta aplicaci&#243;n implementa un algoritmo de ruteo CSPF centralizado, el algoritmo de distribución de etiquetas e implementa a su vez una interfaz gr&#225;fica de gesti&#243;n para la red del prototipo.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg1">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 1: Esquema general del prototipo</div>
<a name="fig:OpenSourceRArch0">
</a>
</div>
<div class="p"><!----></div>
Por otro lado se tiene el switch compatible con el protocolo OpenFlow. Este dispositivo, denominado de aqu&#237; en m&#225;s RAU-Switch, es implementado mediante una PC de escritorio con el hardware NetFPGA instalado y el software Open vSwitch entre otras componentes.<br />

<div class="p"><!----></div>
Para la implementaci&#243;n del plano de reenvío(encaminar un paquete de un nodo origen a otro destino), en cada nodo del prototipo se utilizan las tablas de flujos de OpenFlow. Mediante el esquema de flujos de OpenFlow se definen reglas de reenv&#237;o en base a la conmutaci&#243;n de etiquetas MPLS, as&#237; como pol&#237;ticas de clasificaci&#243;n de tr&#225;fico.<br />

<div class="p"><!----></div>
De esta forma el prototipo para la RAU2 se compone entonces de unos pocos nodos constru&#237;dos en base a RAU-Switch y un controlador SDN compatible con OpenFlow, en donde se ejecuta la aplicaci&#243;n RAUFlow.<br />

<div class="p"><!----></div>
A continuaci&#243;n se explica en detalle cada una de estas componentes y su interacci&#243;n.

<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;RAU-Switch</h2>
Cada nodo del prototipo es implementado por lo que en este trabajo se denomin&#243; RAU-Switch: un switch MPLS/OpenFlow híbrido. Por un lado es un switch OpenFlow puesto que esta dise&#241;ado en base a dicha arquitectura y por otro lado es un router MPLS puesto que conceptualmente el plano de reenvío es implementado a trav&#233;s de la conmutación de etiquetas.<br />

<div class="p"><!----></div>
Puesto que en la literatura OpenFlow es común la denominación de switch OpenFlow a cualquier dispositivo, no importando si en la practica implementa las funcionalidades de un switch de capa dos &#243; un router de capa tres, se decidió denominar a esta componente RAU-Switch.<br />

<div class="p"><!----></div>
La estructura de este dispositivo esta caracterizada por las componentes que se muestran en la figura&nbsp;<a href="#fig:OpenSourceRArch">4.2</a>.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg2">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 2: RAU-Switch - diagrama de componentes</div>
<a name="fig:OpenSourceRArch">
</a>
</div>
<div class="p"><!----></div>
Se utiliza una PC de escritorio como plataforma inicial a la cual se le incorpora una tarjeta NetFPGA-10G programada para que se comporte como una placa de red(proyecto ReferenceNIC) m&#225;s componentes adicionales de software como Open vSwitch, Quagga y un administrador SNMP. A continuaci&#243;n se explica en profundidad el rol de cada componente.

<div class="p"><!----></div>
     <a name="tth_sEc3.1"></a><h3>
3.1&nbsp;&nbsp;Plataforma de la PC</h3>
El switch esta constru&#237;do sobre la plataforma de una PC de escritorio convencional. En particular se trabaja con un procesador Intel Core i7 de 64 bits, una Mother ASUS ROG Maximus Formula VI, 16GB de memoria DDR3, y un disco HDD de 1TB de capacidad. En el anexo <a href="#annexI.1">I.2</a> pueden encontrarse estos detalles con mayor profundidad.

<div class="p"><!----></div>
     <a name="tth_sEc3.2"></a><h3>
3.2&nbsp;&nbsp;Sistema Operativo</h3>
En relación al sistema operativo, se decide trabajar con Ubuntu 12.04 sobre una arquitectura de 64 bits. Esta elecci&#243;n responde a dos criterios esencialmente: por un lado la premisa de trabajar con software libre y de c&#243;digo abierto preferentemente sugiere la b&#250;squeda de un sistema operativo entre alternativas basadas en GNU/Linux; por otro el hardware NetFPGA asi como los proyectos existentes para su programaci&#243;n fueron desarrollados trabajando sobre la plataforma Fedora14. 

<div class="p"><!----></div>
Teniendo presente esto, se intenta infructuosamente instalar y configurar el hardware sobre Fedora14. Se prueba con versiones m&#225;s recientes de la plataforma como Fedora17 y Fedora19 obteniendo los mismos resultados; detectándose entre las causas de error, incompatibilidades entre la placa madre de la PC y las versiones de sistema operativo mencionadas y falta de drivers apropiados para cable JTag utilizado para programar el hardware. En el apéndice <a href="#B.7">B.7</a> se explica con mayor detalle los problemas enfrentados en relaci&#243;n a esta componente.

<div class="p"><!----></div>
Finalmente tras probar con otras alternativas, se logra instalar y configurar exitosamente el hardware sobre la plataforma Ubuntu 12.04.

<div class="p"><!----></div>
     <a name="tth_sEc3.3"></a><h3>
3.3&nbsp;&nbsp;Hardware NetFPGA</h3>

<div class="p"><!----></div>
El hardware NetFPGA puede funcionar tanto conectado a una PC por un slot PCIe(modo servidor), como conectado &#250;nicamente a una fuente de energ&#237;a el&#233;ctrica(modo standalone). En el dise&#241;o planteado el hardware se encuentra conectado a la PC mediante un slot PCIe; es decir, en modo servidor.

<div class="p"><!----></div>

<div class="p"><!----></div>

<h4>Herramientas de Programaci&#243;n</h4>

<div class="p"><!----></div>
El hardware puede programarse mediante un cable programador JTAG y la herramienta Impact de la suite de desarrollo de Xilinx ISE SDK. Para ello es indispensable contar con una estaci&#243;n de trabajo con dichas herramientas instaladas, habilitando la programaci&#243;n  de la tarjeta NetFPGA que luego ser&#225; colocada en la PC de cada nodo. A su vez esta suite se compone por varias herramientas las cuales están licenciadas. Estas licencias adem&#225;s contemplan la arquitectura del chip Xilinx que en este caso tiene la tarjeta NetFPGA, por ello es indispensable contar con el paquete de licencias apropiado al modelo de chip con que se trabaja (en nuestro caso Virtex5) y a las herramientas utilizadas. 

<div class="p"><!----></div>
Esta estaci&#243;n de trabajo puede o bien ser la propia PC utilizada para el switch(alternativa utilizada en este proyecto) &#243; bien puede ser una PC independiente. Por ello no se incluye explicitamente esta componente en el diagrama a pesar de que forma parte de la arquitectura.

<div class="p"><!----></div>

<div class="p"><!----></div>

<h4>Programaci&#243;n simple</h4>
Programar el hardware con un proyecto puede hacerse al menos de dos formas diferentes; una de ellas es lo que aqu&#237; se denomina programaci&#243;n simple. Esta estrategia consiste en la utilizaci&#243;n de la herramienta Impact y el cable JTAG para grabar en dos chips de la tarjeta (chip FPGA y chip CPLD) dos archivos binarios, uno con la arquitectura del proyecto y otro con la implementaci&#243;n.

<div class="p"><!----></div>
Como principales ventajas de esta estrategia se destacan su simplicidad, no requiere de licencias pagas (puede descargarse una licencia gratuita para la herramienta Impact) y adem&#225;s es el procedimiento que se describe en la documentaci&#243;n de la plataforma NetFPGA.

<div class="p"><!----></div>
No obstante presenta una desventaja importante y es que al producirse un ciclo completo de corriente (apagado y encendido del equipo) el hardware se “desprograma”. Concretamente el contenido del chip FPGA es borrado y solo perdura el contenido del chip CPLD.<br />

<div class="p"><!----></div>
Tras constatarse este comportamiento, luego de revisar la documentaci&#243;n de la plataforma y recurrir al foro de la comunidad NetFPGA, se accede a una lista de correos mediante la cual se establece una comunicaci&#243;n con el equipo de desarrollo de NetFPGA(ver ap&#233;ndice &nbsp;<a href="#apendiceB2">B.2</a>). Este di&#225;logo adem&#225;s de ayudar en la comprensión del funcionamiento de la plataforma, desemboca en la segunda estrategia de programaci&#243;n.

<div class="p"><!----></div>

<h4>Programaci&#243;n persistente</h4>
El hardware NetFPGA cuenta en su arquitectura con dos unidades de memoria flash(Flash A y Flash B). En la programaci&#243;n persistente estas unidades se utilizan para almacenar la programaci&#243;n del hardware, permitiendo que en cada encendido el chip FPGA sea programado a partir del contenido de una de estas unidades. Por defecto el chip siempre se programa con el contenido de la memoria flash A, habilitando su reprogramaci&#243;n desde la memoria flash B v&#237;a la interfaz PCIe (por mayores detalles de este procedimiento ver &nbsp;[<a href="#PCIEProgProject" name="CITEPCIEProgProject">PCI, 2015</a>]).<br />

<div class="p"><!----></div>
Reprogramar el contenido del chip FPGA en tiempo de encendido, así como también mediante la interfaz PCIe requiere de módulos adicionales tanto en el contenido del chip FPGA, como en el del CPLD. En particular los proyectos ReferenceNIC[<a href="#ReferenceNICProject" name="CITEReferenceNICProject">Ref, 2015b</a>], ReferenceSwitch&nbsp;[<a href="#ReferenceSwitchProject" name="CITEReferenceSwitchProject">Ref, 2015a</a>] y ReferenceRouter&nbsp;[<a href="#ReferenceRouterProject" name="CITEReferenceRouterProject">Ref, 2015c</a>] incorporan estas características.<br />

<div class="p"><!----></div>
En el procedimiento empleado, inicialmente se programa el hardware con el proyecto ReferenceNIC utilizando la Programaci&#243;n Simple. Luego es necesario transformar la implementaci&#243;n del proyecto(archivo bitfile) en un archivo con el formato apropiado para ser almacenado en una de las memorias flash(archivo binario). Esto &#250;ltimo se realiza utilizando herramientas inclu&#237;das en la plataforma de NetFPGA. Finalmente utilizando la herramienta <b>pcieprog</b> también de la plataforma se transfiere el archivo generado a una de las memorias.<br />

<div class="p"><!----></div>
Cabe destacar que durante la ejecuci&#243;n de este procedimiento se detectan algunos errores y comportamientos inesperados en el hardware. Esto es reportado al equipo de desarrollo de NetFPGA a través de la lista oficial de correos de soporte, reportándose en total dos errores. Luego de este di&#225;logo se obtiene una soluci&#243;n a dichos errores los cuales cabe destacar que son contemplados y resueltos en la siguiente actualizaci&#243;n del repositorio de c&#243;digo fuente. Por m&#225;s detalles acerca de estos errores y la forma en que son resueltos provisoriamente referirse al ap&#233;ndice&nbsp;<a href="#apendiceA">A</a>.<br />

<div class="p"><!----></div>
Otro detalle a destacar de esta estrategia es la utilizaci&#243;n de herramientas de la suite de Xilinx que requieren de licencias especiales pagas. En el marco de este proyecto se solicita apoyo en relaci&#243;n a este tema a docentes del Instituto de Ingeniería Eléctrica de la Facultad de Ingenieria (IIE) y a través del programa de apoyo universitario de Xilinx, en busca de una soluci&#243;n a este problema.

<div class="p"><!----></div>
Si bien en el IIE no se trabaja con esta plataforma se obtiene de parte de este instituto asesoramiento sumamente &#250;til para la resoluci&#243;n de este obstáculo. Por otro lado a través del programa de apoyo a universidades de Xilinx se obtiene una interesante donaci&#243;n de licencias, posibilitando a una real explotaci&#243;n del hardware y de la plataforma. Por mayores detalles acerca de este problema puede consultarse el apéndice &nbsp;<a href="#apendiceB3">B.3</a>.

<div class="p"><!----></div>
     <a name="tth_sEc3.4"></a><h3>
3.4&nbsp;&nbsp;Open vSwitch</h3>
En la arquitectura del dispositivo, Open vSwitch es el encargado de implementar el plano de datos de OpenFlow. En otras palabras es la componente que convierte mediante una implementaci&#243;n en software a la PC construida en un switch OpenFlow.<br />

<div class="p"><!----></div>
Siguiendo la especificaci&#243;n de OpenFlow, Open vSwitch implementa el concepto de tabla de flujos. Estas tablas(Open vSwitch implementa 256 tablas) son utilizadas por la aplicaci&#243;n RAUFlow para construir caminos en la topolog&#237;a, instalando en forma de flujos OpenFlow las reglas de reenvío que sean necesarias en cada nodo involucrado en un camino.

<div class="p"><!----></div>
Para implementar el plano de reenvío en base a la conmutaci&#243;n de etiquetas MPLS, se utilizan primitivas del protocolo OpenFlow, que permiten colocar y extraer etiquetas MPLS de un paquete(PUSH y POP), así como clasificar un paquete acorde al valor de la etiqueta MPLS contenida en &#233;l.<br />

<div class="p"><!----></div>
En relaci&#243;n a este &#250;ltimo punto, vale la pena destacar que la compatibilidad con estas funcionalidades del protocolo OpenFlow, están acotadas naturalmente  por la implementaci&#243;n de Open vSwitch. En este trabajo inicialmente se utiliza la versi&#243;n oficial m&#225;s reciente de dicha herramienta (versi&#243;n 2.3.1), en la cual acorde a las notas de liberaci&#243;n y a la secci&#243;n de preguntas frecuentes se garantiza soporte para solo un subconjunto de funcionalidades de la versi&#243;n 1.3 del protocolo OpenFlow. Entre estas funcionalidades se destacan la capacidad para match, push y pop de una &#250;nica etiqueta MPLS, así como su posterior procesamiento en el pipe de OpenFlow. No obstante como se explica en el apéndice &nbsp;<a href="#apendiceB5">B.5</a> junto a otras características a destacar de Open vSwitch, este comportamiento no es el que realmente manifiesta esta versi&#243;n de la herramienta. En particular la operación Pop de MPLS no funciona correctamente, así como el posterior procesamiento del paquete tras manipular etiquetas acorde al pipe de OpenFlow.<br />

<div class="p"><!----></div>
Afortunadamente esta falla se encuentra reportada como error y es resuelta en la versi&#243;n de desarrollo&nbsp;[<a href="#OVSSourceCode" name="CITEOVSSourceCode">OVS, 2015</a>], la cual a su vez tras instalarse se comprueba que soporta match, push y pop de hasta tres etiquetas mpls, junto el posterior procesamiento del paquete. De esta forma la versi&#243;n de Open vSwitch utilizada es la de desarrollo.<br />

<div class="p"><!----></div>
Por otro lado, resulta interesante a tener en consideraci&#243;n que las operaciones de match, push y pop de MPLS son implementadas en modo usuario.

<div class="p"><!----></div>

<div class="p"><!----></div>
     <a name="tth_sEc3.5"></a><h3>
3.5&nbsp;&nbsp;Quagga</h3>
En cada nodo se ejecuta una instancia del software de enrutamiento Quagga, configurada para ejecutar el demonio ospf.<br />

<div class="p"><!----></div>
El demonio ospf implementa el protocolo de igual nombre y se utiliza para obtener información de cada nodo y sus adyacencias en la topolog&#237;a, guardando dicha información en una base de datos local(Link-State-Database o LSDB). Entre los datos que se incluyen en esta base de datos topol&#243;gica se destaca por ejemplo el costo asociado a cada adyacencia.<br />

<div class="p"><!----></div>
Como se menciona anteriormente, el algoritmo OSPF utiliza solamente para la construcci&#243;n de la base de datos topol&#243;gica. Si bien en un inicio se piensa en trabajar con la salida de dicho algoritmo, finalmente se decidió trabajar con el algoritmo CSPF implementado en el controlador. Por ello en la versi&#243;n final del prototipo no se utiliza la salida de este algoritmo a pesar de que la arquitectura permite su utilización para alimentar la entrada de procesos que se ejecuten en el controlador.<br />

<div class="p"><!----></div>
De esta forma cada switch del prototipo así como el propio controlador ejecuta una instancia de Quagga con el demonio ospf configurado como se muestra en el apéndice[poner referencia al apendice]. Cabe destacar de dicha configuración que la adyacencia entre un switch y el Controlador(enlace utilizado para el canal de comunicación OpenFlow) tiene asociado costo infinito para evitar que sea utilizado tanto por el algoritmo OSPF como por el algoritmo CSPF en la construcción de algún camino. En la implementaci&#243;n de Quagga el costo infinito es representado por el numero 65535(16 bits).

<div class="p"><!----></div>
     <a name="tth_sEc3.6"></a><h3>
3.6&nbsp;&nbsp;Agente SNMP</h3>
El protocolo de comunicación OpenFlow dentro de las estructuras de datos utilizadas para el intercambio de información entre un switch y el Controlador, no prevee una forma de comunicar el direccionamiento IP propio del equipo (direcciones IP de cada interfaz física). En particular la estructura <i>ofp_port</i> utilizada por el mensaje <i>OFPMP_PORT_DESCRIPTION</i> no provee de dicho campo (ver especificación del protocolo OpenFlow 1.3[<a href="#ofv133spec" name="CITEofv133spec">ONF, 2012</a>]). Por esta razón es necesaria una forma de comunicar al controlador información adicional sobre características de cada nodo como la dirección IP de una interfaz, que no puede ser enviada a través del protocolo OpenFlow. Para evitar modificar el protocolo extendiéndolo para comunicar dicha informaci&#243;n, se utiliza un agente SNMP.<br />

<div class="p"><!----></div>
SNMP definido a través de los RFC1065&nbsp;[<a href="#rose1990structure" name="CITErose1990structure">Rose and McCloghrie, 1990</a>] y RFC1157&nbsp;[<a href="#case1989simple" name="CITEcase1989simple">Case et&nbsp;al., 1989</a>] entre otros, es un protocolo que permite intercambio de información entre dispositivos de red y una entidad administradora. Para su despliegue en una red son necesarias tres componentes: 

<div class="p"><!----></div>

<ol type="1">
<li> Dispositivo administrado: Es el dispositivo de red que se quiere monitorear.
<div class="p"><!----></div>
</li>

<li> Agente: Es el software que se ejecuta en el dispositivo administrado y se encarga de comunicarse mediante el protocolo con el sistema administrador de red.
<div class="p"><!----></div>
</li>

<li> Sistema administrador de red: El dispositivo que se encargara de realizar las consultas sobre la información deseada a los dispositivos administrados.
<div class="p"><!----></div>
</li>
</ol>	

<div class="p"><!----></div>
En el prototipo se instala un agente SNMP en cada switch (dispositivo administrado), para enviar la correspondencia entre números de puerto OpenFlow y direcciones IP al Controlador(sistema administrador).<br />

<div class="p"><!----></div>
No es el objetivo de este trabajo entrar en detalle sobre el protocolo SNMP por lo que si el lector desea profundizar en estos conceptos se recomienda seguir las referencia mencionadas anteriormente.

<div class="p"><!----></div>
 <a name="tth_sEc4"></a><h2>
4&nbsp;&nbsp;Entidad Controlador</h2>
Como se muestra en <a href="#fig:OpenSourceRArch0">4.1</a> cada nodo del prototipo responde a una entidad denominada Controlador. Esta entidad en la pr&#225;ctica es una PC convencional con determinadas componentes de software(ver figura <a href="#fig:OpenSourceRArch3">4.3</a>) entre las cuales se destaca el controlador SDN Ryu utilizado para implementar y ejecutar la aplicaci&#243;n RAUFlow. A continuación se explica en detalle cada una de estas componentes y su funcionalidad.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg3">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 3: Diagrama de componentes del Controlador</div>
<a name="fig:OpenSourceRArch3">
</a>
</div>
<div class="p"><!----></div>
     <a name="tth_sEc4.1"></a><h3>
4.1&nbsp;&nbsp;Software de Control Ryu</h3>
Ryu es la alternativa de software elegido para implementar el controlador SDN compatible con el protocolo OpenFlow. Utilizando este controlador como ambiente de desarrollo y ejecuci&#243;n, se implementa la aplicaci&#243;n RAUFlow utilizada para construir el plano de control del prototipo. Esto quiere decir implementar un modelado de la realidad, proveer de funcionalidades para la creaci&#243;n de servicios de VPN, implementar estrategias para la clasificaci&#243;n de tr&#225;fico y brindar calidad de servicios(QoS), implementar un algoritmo de ruteo, entre otras responsabilidades.<br />

<div class="p"><!----></div>
Dada la complejidad de esta componente, se destina íntegramente el próximo cap&#237;tulo a la explicaci&#243;n de las diferentes características e implementaci&#243;n de esta aplicaci&#243;n. 

<div class="p"><!----></div>
     <a name="tth_sEc4.2"></a><h3>
4.2&nbsp;&nbsp;Quagga</h3>
Como se menciona anteriormente, el controlador ejecuta una instancia de Quagga, obteniendo de esta forma acceso local a la información de la base de datos topol&#243;gica construida por OSPF, una vez que el algoritmo converge.

<div class="p"><!----></div>
     <a name="tth_sEc4.3"></a><h3>
4.3&nbsp;&nbsp;LSDB Sync</h3>
LSDB Sync se encarga de tomar la información de la base de datos topol&#243;gica una vez que el algoritmo ospf converge, procesarla y enviarla a las aplicaciones que se ejecutan en el software de control(Ryu); en este caso la aplicación RAUFlow.<br />

<div class="p"><!----></div>
Esta componente consta de dos módulos. El primero se encarga de escuchar los mensajes del protocolo ospf que envia Quagga, generando un evento cuando ospf reconverge, luego de producirse un cambio en la topolog&#237;a; el cual luego es tomado por el segundo m&#243;dulo.

<div class="p"><!----></div>
Una vez capturado el evento anterior, el segundo m&#243;dulo toma la información topol&#243;gica en la base local del controlador y la procesa para ser enviada a la aplicación RAUFlow.

<div class="p"><!----></div>
Administrador SNMP
El administrador SNMP es utilizado para consultar al agente SNMP instalado en un nodo particular, por la correspondencia entre números de puerto OpenFlow y direcciones IP. Esta componente es utilizada por RAUFlow cada vez que es preciso obtener esta información de mapeo para un nodo en particular(por ejemplo en el proceso de actualización de la topolog&#237;a).<br /><br />&nbsp;


<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg4">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 4: Vista l&#243;gica ampliada del prototipo</div>
<a name="fig:OpenSourceRArch4">
</a>
</div>
<div class="p"><!----></div>
En resumen, el prototipo se compone de nodos construidos en base al hardware NetFPGA y una PC convencional m&#225;s agregados de software, implementando un switch MPLS/OpenFlow, y una PC controlador Controlador Ryu mas alg&#250;nas otras componentes de software que implementan el plano de control. Como se muestra en la figura<a href="#fig:OpenSourceRArch4">4.4</a>, luego la comunicación entre cada par de nodos es IP/MPLS, mientras que cada nodo es manipulado por el protocolo OpenFlow. Adicionalmente se tiene un canal IP entre cada nodo y el plano de control por donde se comunican cada agente snmp con el administrador.<br />

<div class="p"><!----></div>
 <a name="tth_chAp5"></a><h1>
Capítulo 5 <br />RAUFlow</h1>
<a name="Capítulo 5">
</a>

<div class="p"><!----></div>

    


<div class="p"><!----></div>
En cap&#237;tulos anteriores se han detallado las caracter&#237;sticas m&#225;s importantes del prototipo así como sus componentes. Resta detallar entonces el dise&#241;o e implementaci&#243;n de RAUFlow la aplicaci&#243;n encargada de implementar el plano de control en el prototipo. 

<div class="p"><!----></div>
En el presente cap&#237;tulo se propone un an&#225;lisis de RAUFlow siguiendo un proceso de dise&#241;o tradicional de Ingenier&#237;a de Software dividido en cuatro etapas. Una primera etapa de an&#225;lisis de requerimientos, una segunda etapa de relevamiento de casos de uso, una tercera etapa de dise&#241;o del modelo de datos y finalmente una cuarta etapa destinada al dise&#241;o general de la arquitectura. Ademas se presentan los aspectos m&#225;s importantes relacionados a la implementaci&#243;n de RAUFlow como por ejemplo las implementaciones del algoritmo de ruteo y del algoritmo de distribución de etiquetas, como se implementa clasificaci&#243;n de tr&#225;fico y como se puede implementar QoS entre otros detalles. 

<div class="p"><!----></div>
 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;An&#225;lisis de requerimientos</h2>

<div class="p"><!----></div>
Anteriormente en la sección &nbsp;<a href="#3.1">3.1</a> se definieron los requerimientos relevados para el prototipo de la RAU2. De ellos y de un trabajo de an&#225;lisis sobre la realidad modelada se desprende la siguiente tabla de requerimientos para RauFlow:

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb1">
</a> <div style="text-align:center">
<table border="1">
<tr><td align="left">Requerimientos Funcionales</td></tr><tr><td></td></tr>
<tr><td align="left">
<ul>
<li> El Sistema debe de proveer la facilidad para obtener la informaci&#243;n asociada a cada nodo de la red, permitiendo a su vez agregar informaci&#243;n que facilite la identificaci&#243;n del mismo para un usuario.
<div class="p"><!----></div>
</li>

<li> El Sistema debe proveer la facilidad para agregar, modificar y eliminar servicios de redes virtuales.
<div class="p"><!----></div>
</li>

<li> El Sistema debe proveer la facilidad para obtener toda la informaci&#243;n relevante a un servicio de red privada.
<div class="p"><!----></div>
</li>

<li> El Sistema debe permitir visualizar los caminos constru&#237;dos para encaminar el tr&#225;fico de una red virtual en particular, a trav&#233;s de la red del protot&#237;po.
<div class="p"><!----></div>
</li>

<li> El Sistema debe proveer la facilidad para visualizar el estado de las tablas de flujos asociadas a cualquier nodo de la red del protot&#237;po.
<div class="p"><!----></div>
</li>
</ul></td></tr></table>

</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb1">
</a> <div style="text-align:center">
<table border="1">
<tr><td align="left">Requerimientos no Funcionales</td></tr><tr><td></td></tr>
<tr><td align="left">
<ul>
<li> Se debe utilizar siempre que sea posible herramientas de software libre y c&#243;digo abierto.
<div class="p"><!----></div>
</li>
</ul></td></tr></table>

</div>
<div class="p"><!----></div>
Teniendo en cuenta la descripci&#243;n del problema y los requerimientos anteriores, se procede con el modelado de la realidad. Los resultados obtenidos se presentan en la siguiente secci&#243;n.

<div class="p"><!----></div>

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Modelado de la realidad</h2>

<div class="p"><!----></div>
Para la representaci&#243;n de la realidad se utiliza el paradigma de orientaci&#243;n a objetos. De esta forma el modelo de datos queda representado a través del siguiente diagrama de clases de diseño(ver ver figura &nbsp;<a href="#fig:ModeloDeDatos">5.1</a>).

<div class="p"><!----></div>
En el mismo se destacan en color amarillo las clases utilizadas para representar la topolog&#237;a de red y sus elementos(Nodos, Interfaces y Enlaces). Cabe destacar sobre estas tres clases que en la representaci&#243;n asumida se busca modelar la topolog&#237;a como un multigrafo dirigido. Esto se debe a que:

<div class="p"><!----></div>

<ol type="1">
<li> Tradicionalmente se utilizan grafos para el modelado de una topolog&#237;a de red, siendo una representaci&#243;n sencilla y clara.
<div class="p"><!----></div>
</li>

<li> En <b>mpls</b> un camino o mejor llamado LSP tiene sentido, permitiendo por ejemplo asegurar un valor de ancho de banda en un enlace para un sentido y un valor diferente para el otro. Adem&#225;s permite establecer caminos diferentes para el tr&#225;fico en un sentido y en otro, priorizando por ejemplo el tr&#225;fico  en uno de los sentidos al utilizar el mejor camino. Utilizando grafos dirigidos puede modelarse este comportamiento.
<div class="p"><!----></div>
</li>

<li> En la pr&#225;ctica nada impide que dos equipos o nodos est&#233;n conectados por m&#225;s de un enlace. De hecho este escenario es bastante beneficioso para asegurar conectividad ante la falla de enlaces. Utilizando multigrafos este escenario es modelado de forma clara.
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg1">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 1: Modelo de datos</div>
<a name="fig:ModeloDeDatos">
</a>
</div>
<div class="p"><!----></div>
En un mulrigrafo dirigido se tienen nodos y aristas con sentido. Cada nodo de la red puede ser representado por un nodo del grafo y cada enlace entre un par de nodos con dos aristas(una arista para cada sentido del enlace).

<div class="p"><!----></div>
En el modelo se tiene adem&#225;s el concepto de Interfaz de dispositivo. Este concepto re-define la noción de adyacencia entre dos nodos en el grafo de la siguiente forma: existe una arista para un par de nodos si cada uno de ellos esta asociado a una instancia de la clase Interfaz y existe una instancia de la clase Link asociada a ambas interfaces mediante las relaciones “Desde” y “Hacia”; indicando adem&#225;s estas el sentido del dicho Link.

<div class="p"><!----></div>
Cabe destacar que en el modelo la clase Nodo representa solamente dispositivos de capa 3. Esto quiere decir que dispositivos de capa 2 como switches no son contemplados. Sin embargo no se asume que el nodo este f&#237;sicamente implementado por RAU-Switch, con lo que se permite modelar nodos iplementados en base a otro tipo de dispositivos.<br />

<div class="p"><!----></div>
Por otro lado en rosado se destacan las clases utilizadas para representar los principales conceptos de <b>mpls</b> como las tablas FTN, ILM, NHLFE y el concepto de LSP. Si bien esta &#250;ltima clase no tiene atributos asociados en esta versi&#243;n del modelo de datos, eventualmente tiene sentido en un futuro que se puedan asociar atributos del camino como el m&#225;ximo ancho de banda disponible y algunas otras propiedades orientadas a funcionalidades de QoS por ejemplo.<br />

<div class="p"><!----></div>
Finalmente en azul se destaca el concepto Servicio con el cual se busca representar un servicio de red privada virtual. En particular cada instancia de Servicio representa una red privada punto a punto entre dos nodos en el prototipo; por ello esta clase tiene asociados los atributos nodo de ingreso y nodo de egreso. Adem&#225;s se asume que por cada nodo de borde se tiene a lo sumo una &#250;nica red privada conectada directamente a cada interfaz. Por ello un servicio queda determinado por los pares &lt;nodo, intefaz&#62; origen y &lt;nodo, interfaz&#62; destino. 

<div class="p"><!----></div>
Luego una red privada multipunto puede construirse por ejemplo definiendo servicios punto a punto para cada par de nodos involucrados.

<div class="p"><!----></div>
Esta clase también tiene asociados atributos con los cuales se implementa clasificaci&#243;n de tr&#225;fico; m&#225;s adelante en este cap&#237;tulo se detallan este y otros aspectos asociados a la implementaci&#243;n de esta clase.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
Teniendo en cuenta los requerimientos mencionados, y el modelado de la realidad presentado, se procede con el relevamiento de los casos de uso. Los resultados obtenidos se presentan en la siguiente secci&#243;n.

<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;Relevamiento de casos de uso</h2>
<a name="section5.3">
</a>

<div class="p"><!----></div>
La lista de casos de uso presentada a continuaci&#243;n se corresponde con un conjunto de funcionalidades b&#225;sicas, que permiten explorar el potencial del enfoque SDN aplicado a la implementaci&#243;n de un prototipo para la RAU2.<br />

<div class="p"><!----></div>

<ul>
<li> <b>CU1. Listar Servicios</b>: Devuelve un listado con los servicios existentes en el sistema, mostrando para cada uno la informaci&#243;n b&#225;sica asociada.
<div class="p"><!----></div>
</li>

<li> <b>CU2. Seleccionar Servicio</b>: Selecciona un Servicio de una lista de servicios (CU2. Listar servicios).
<div class="p"><!----></div>
</li>

<li> <b>CU3. Ver Servicio</b>: Devuelve la informaci&#243;n asociada al Servicio.
<div class="p"><!----></div>
</li>

<li> <b>CU4. Modificar Servicio</b>: Modifica una instancia de Servicio, actualizando los atributos seleccionados por el usuario con los valores ingresados.
<div class="p"><!----></div>
</li>

<li> <b>CU5. Eliminar Servicio</b>: Elimina un Servicio existente en el sistema.
<div class="p"><!----></div>
</li>

<li> <b>CU6. Agregar Servicio</b>: Crea un nuevo servicio de red privada en el sistema, con la inormaci&#243;n de nodos y sus respectivas interfaces a las que las subredes esta&#225;n directamente conectadas con la red del prototipo, ingresada. Adem&#225;s solicita para cada servicio los datos utilizados para la clasificaci&#243;n del tr&#225;fico asociado al servicio.
<div class="p"><!----></div>
</li>

<li> <b>CU7. Ver Topolog&#237;a</b>: Muestra la topolog&#237;a de red mediante una representaci&#243;n gr&#225;fica.
<div class="p"><!----></div>
</li>

<li> <b>CU.8 Filtrar Lsps</b>: Superpone en la representaci&#243;n gr&#225;fica de la red (CU7.Ver Topolog&#237;a), un conjunto de LSPs seleccionados.
<div class="p"><!----></div>
</li>

<li> <b>CU9. Seleccionar Nodo</b>: Selecciona un nodo de una lista de Nodos (CU7. Ver Topolog&#237;a).
<div class="p"><!----></div>
</li>

<li> <b>CU10. Ver informaci&#243;n b&#225;sica Nodo</b>: Despliega la informaci&#243;n asociada al nodo seleccionado de la topolog&#237;a (CU9. Seleccionar Nodo).
<div class="p"><!----></div>
</li>

<li> <b>CU11. Ver tabla de Flujos Nodo</b>: Muestra la inormaci&#243;n de la tabla de flujos OpenFlow asociada al nodo seleccionado (CU9. Seleccionar Nodo).
<div class="p"><!----></div>
</li>

<li> <b>CU12. Ver tablas MPLS</b>: Muestra la informaci&#243;n asociada a las tablas FTN, ILM y NHLFE asociadas al nodo seleccionado (CU9. Seleccionar Nodo).
<div class="p"><!----></div>
</li>

<li> <b>CU13. Editar Informaci&#243;n extra Nodo</b>: Permite ingresar informaci&#243;n adicional sobre un Nodo como Nombre, si es nodo de interno o de borde por ejemplo.
<div class="p"><!----></div>
</li>

<li> <b>CU14. Editar Informaci&#243;n extra Interfaz</b>: Ingresa informaci&#243;n adicional sobre la interfaz de un nodo seleccionado (CU9. Seleccionar Nodo). Por ejemplo si la interfaz es interna o externa.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
En la figura <a href="#fig:CasosDeUso">5.2</a> se representan las dependencias existentes entre los casos de uso mencionados anteriormente.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg2">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 2: Casos de Uso de RAUFlow</div>
<a name="fig:CasosDeUso">
</a>
</div>
<div class="p"><!----></div>
En la siguiente secci&#243;n se presenta la arquitectura de la aplicaci&#243;n RAUFlow, detallándose las componentes m&#225;s importantes.

<div class="p"><!----></div>


 <a name="tth_sEc4"></a><h2>
4&nbsp;&nbsp;Arquitectura de RauFlow</h2>
A modo de introducci&#243;n RAUFlow es una aplicaci&#243;n de gesti&#243;n de red, basada en el enfoque SDN e implementada utilizando entre otras tecnolog&#237;as el software de control Ryu.

<div class="p"><!----></div>
Es m&#225;s que una aplicaci&#243;n del plano de control OpenFlow, puesto que en su arquitectura se incluyen componentes que no son implementadas por un controlador OpenFlow.<br />

<div class="p"><!----></div>
La arquitectura de RAUFlow (ver figura <a href="#fig:VistaComponentes2">5.3</a>), se caracteriza por un diseño en capas l&#243;gicas. En particular esta compuesta de cuatro capas: (1) capa de presentación, (2) capa de aplicaciones Ryu, (3) capa de negocios, y (4) capa de dispositivos.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg3">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 3: Vista l&#243;gica</div>
<a name="fig:VistaComponentes2">
</a>
</div>
<div class="p"><!----></div>
A continuaci&#243;n se explican en detalle cada una de estas capas.

<div class="p"><!----></div>
     <a name="tth_sEc4.1"></a><h3>
4.1&nbsp;&nbsp;Capa de aplicaciones Ryu</h3>
En esta capa se encuentran las diferentes aplicaciones Ryu utilizadas para implementar el plano de control OpenFlow. Esta compuesta por una aplicaci&#243;n desarrollada en este proyecto (RAUFlowApp), y tres aplicaciones desarrolladas por terceros (incluidas en las aplicaciones que vienen con el controlador).<br />

<div class="p"><!----></div>
Dentro de las aplicaciones Ryu, RauFlowApp es realmente la encargada de implementar el plano de control  del prototipo. Esto quiere decir, guardar informaci&#243;n de la realidad (servicios de redes privadas), guardar el estado de la topolog&#237;a, implementar funcionalidades de QoS, ruteo din&#225;mico etc. 

<div class="p"><!----></div>
En consecuencia el alcance de esta aplicaci&#243;n puede ser casi tan grande como el propio alcance de RAUFlow. Sin embargo esto redundar&#237;a en una complejidad mayor en el dise&#241;o de la aplicaci&#243;n, poca mantenibilidad, calidad del c&#243;digo fuente, etc. Por estas razones se decide desacoplar la mayor cantidad de informaci&#243;n posible relacionada a la realidad modelada, de dicha aplicaci&#243;n; haciendo responsable a RauFlowApp solamente por el manejo de los eventos del protocolo OpenFlow.

<div class="p"><!----></div>
De esta forma se justifica la mencionada capa de Negocios. 

<div class="p"><!----></div>

<h4>Aplicaciones de terceros</h4>
Ryu incluye tres aplicaciones dise&#241;adas para implementar una interfaz gr&#225;fica, consistente en un p&#225;gina web implementada sobre html, javascript y web sockets, que permite visualizar de forma gr&#225;ica la red en su totalidad. Estos son los dispositivos del datapath con sus interfaces y los enlaces existentes.

<div class="p"><!----></div>
Por esta razón, orientado a proveer de una representaci&#243;n gr&#225;fica del estado de la red en el prototipo (el estado de la topolog&#237;a y sus dispositivos), se incluyen en el dise&#241;o estas aplicaciones.

<div class="p"><!----></div>
     <a name="tth_sEc4.2"></a><h3>
4.2&nbsp;&nbsp;Capa de Negocios</h3>
La capa de negocios presenta tres componentes bien definidas: una componente de reglas y l&#243;gica de negocios de la realidad, una API REST de servicios para el acceso y la manipulaci&#243;n de datos relacionados a la primera componente y una tercera componente denominada Agentes distribu&#237;dos.

<div class="p"><!----></div>

<h4>Lógica de Negocios</h4>
La componente lógica de negocios se subdivide a su vez en diferentes m&#243;dulos que agrupan funcionalidades acorde a su naturaleza y responsabilidades. Vale la pena destacar a su vez que cada uno de estos m&#243;dulos se corresponde en la implementaci&#243;n con un package en la filosof&#237;a de Python. Estas funcionalidades y responsabilidades se corresponden de la siguiente forma:

<div class="p"><!----></div>

<ul>
<li> <b>controller:</b> Este m&#243;dulo agrupa diferentes controladores de objetos. Actualmente contiene un &#250;nico controlador façade responsable de mantener el &#250;nico punto de acceso a la las componentes de la l&#243;gica de negocios. Contiene desde la implementaci&#243;n de funciones para dar de alta Servicios, crear LSPs, obtener el mejor camino entre dos nodos de la red, etc.
<div class="p"><!----></div>
</li>

<li> <b>topology:</b> Agrupa las definiciones de objetos utilizados para representar la topolog&#237;a como las clases Nodo, Interfaz y Link, así como otros conceptos de la realidad.
<div class="p"><!----></div>
</li>

<li> <b>mpls:</b> Contiene las definiciones de los conceptos FTN, ILM, NHLFE, as&#237; como los conceptos de conceptos de servicio de red y LSP(Label Switched Path).
<div class="p"><!----></div>
</li>

<li> <b>dataTypes:</b> Mantiene representaciones reducidas de los principales objetos definidos en todos los m&#243;dulos de la componente de negocios para el intercambio de datos por ejemplo con la capa de presentaci&#243;n.
<div class="p"><!----></div>
</li>

<li> <b>datapath:</b> Agrupa funcionalidades para el acceso al datapath de OpenFlow, como funciones para agregar y eliminar flujos en un switch, u obtener estad&#237;sticas de una tabla de flujos.
<div class="p"><!----></div>
</li>
</ul> 

<div class="p"><!----></div>

<h4>API REST de servicios</h4>
La componente de Servicios REST, se encuentra subdividida en varios m&#243;dulos respondiendo al criterio utilizado para el dise&#241;o modular de la componente de l&#243;gica de negocios. 

<div class="p"><!----></div>

<h4>Agentes distribuidos</h4>
Esta componente es la encargada de interactuar con diferentes agentes y procesos instalados en cada nodo del prototipo a través de un canal de comunicación IP. Este m&#243;dulo de comunicaci&#243;n, as&#237; como los diferentes agentes instalados en cada nodo, juegan un rol irreemplazable en la obtenci&#243;n de informaci&#243;n adicional sobre cada nodo; puesto que a partir del canal de comunicaci&#243;n OpenFlow solamente es accesible Open vSwitch y la informaci&#243;n contemplada por el protocolo OpenFlow.<br />

<div class="p"><!----></div>
Dentro de esta componente, en el diagrama se muestra un modulo denominado <b>managements_ agents</b>. Este m&#243;dulo opera como una interfaz de conexi&#243;n entre los diferentes m&#243;dulos de la l&#243;gica de negocios y los diferentes m&#243;dulos que implementan la comunicaci&#243;n con su respectivo agente distribu&#237;do.

<div class="p"><!----></div>
En la figura &nbsp;<a href="#fig:VistaComponentes2">5.3</a> se muestran el modulo <b>SNMP Management</b>, responsable de  la comunicaci&#243;n con el Agente SNMP instalado en cada nodo del prototipo, con el fin de obtener la correspondencia entre n&#250;meros de puerto OpenFlow y direcciones IP de cada interfaz.<br />

<div class="p"><!----></div>
     <a name="tth_sEc4.3"></a><h3>
4.3&nbsp;&nbsp;Capa de Presentación</h3>
Dentro de la capa de presentaci&#243;n se destacan: (1) una interfaz gr&#225;fica identificada en el esquema como RauFlowUI, la cual implementa cada uno de los casos de uso mencionados anteriormente en <a href="#section5.3">5.3</a>, (2) ControladorUI, componente que funciona como nexo entre las funcionalidades de RauFlowUI y la API Rest de Servicios y (3) <b>ryu.topology</b>. Esta &#250;ltima componente forma parte del conjunto de apliaciones Ryu adicionales que se incluyen en el dise&#241;o de RauFlow y tiene como principal funci&#243;n la de proveer una representaci&#243;n gr&#225;fica en tiempo real de la topolog&#237;a existente en el prototipo.<br />

<div class="p"><!----></div>
 <a name="tth_sEc5"></a><h2>
5&nbsp;&nbsp;Implementaci&#243;n</h2>

<div class="p"><!----></div>
En esta secci&#243;n se presentan los aspectos m&#225;s importantes relacionados a la implementaci&#243;n de RauFlow. Entre ellos se destacan la estrategia utilizada para implementar clasificaci&#243;n de tr&#225;fico,  la implementaci&#243;n de la clase servicio, la implementaci&#243;n de los algoritmos de ruteo y distribución de etiquetas distribusi&#243;n de etiquetas, entre otros.<br />

<div class="p"><!----></div>
     <a name="tth_sEc5.1"></a><h3>
5.1&nbsp;&nbsp;Clasificación de tr&#225;fico</h3>
En la literatura de MPLS, tradicionalmente se utiliza el concepto de FEC(fordwarding equivalence class) para distinguir a un conjunto de paquetes a ser tratados en forma similar(por ejemplo para aplicar t&#233;cnicas de QoS). Este concepto se define localmente a un dispositivo, determinando la forma en que un conjunto de paquetes son tratados solamente en el mismo.

<div class="p"><!----></div>
En RAUFlow se aprovecha la visi&#243;n global del plano de control SDN para definir una noci&#243;n de FEC global en una topolog&#237;a de red. En esta redefinici&#243;n se clasifica tr&#225;fico solamente en el nodo de ingreso a la red del prototipo, determinándose all&#237; el camino a seguir por un paquete hasta el nodo de egreso mediante el resultado obtenido en la ejecuci&#243;n del algoritmo de ruteo. 

<div class="p"><!----></div>

<div class="p"><!----></div>
Por otro lado se define un mapeo entre servicios de VPN y etiquetas mpls, permitiendo identificar el tr&#225;fico asociado a un servicio particular en cualquier nodo de la red. Luego a partir del valor de esta etiqueta se puede realizar un procesamiento diferencial en nodos intermedios de un LSP.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
En relaci&#243;n a como se implementa clasificaci&#243;n de tr&#225;fico, en el prototipo se utilizan las tablas de flujos de Open vSwitch en conjunto con los campos del cabezal OpenFlow (matching fields) para la definici&#243;n de flujos. Con los matching fields de OpenFlow se define la regla de un flujo(recordar figura <a href="#fig:OpenFlowArch2">2.4</a>) distinguiendo de este modo entre diferentes clases de tr&#225;fico para un procesamiento diferencial.

<div class="p"><!----></div>
Dentro del cabezal OpenFlow, existen campos para la definici&#243;n de las reglas de un flujo, campos para la definici&#243;n de la acci&#243;n de un flujo y campos que pueden ser utilizados para la definici&#243;n de ambas componentes.  

<div class="p"><!----></div>
Esta lista de campos varia con la versi&#243;n del protocolo OpenFlow y en el contexto del prototipo esta acotada a su vez por la implementaci&#243;n de Open vSwitch. En el ap&#233;ndice <a href="#appendix3">C</a> se muestra  la lista entera de atributos que contiene el cabezal OpenFlow para la versi&#243;n 1.3 y de estos la lista de atributos que son soportados por Open vSwitch y que pueden ser utilizados para la definici&#243;n de reglas y acciones. Ambas listas fueron constru&#237;das experimentalmente trabajando con las herramientas Open vSwitch y Ryu.<br />

<div class="p"><!----></div>
En el prototipo, el concepto de clasificaci&#243;n de tr&#225;fico esta estrictamente ligado al concepto de servicio de red privada. En la siguiente secci&#243;n se explica en detalle la implementaci&#243;n de este concepto.

<div class="p"><!----></div>
     <a name="tth_sEc5.2"></a><h3>
5.2&nbsp;&nbsp;Implementación de Servicio</h3>

<div class="p"><!----></div>
La clase Servicio implementa en el prototipo el concepto de servicios de red privada virtual. Como se menciona anteriormente un servicio define una clase de tr&#225;fico; por tanto adem&#225;s de la informaci&#243;n b&#225;sica que define a un servicio se tienen asociados tambi&#233;n todos los campos utilizados para implementar clasificaci&#243;n de tr&#225;fico(matching fields de OpenFlow).

<div class="p"><!----></div>
En el prototipo se asume que por cada nodo de borde  se tiene una &#250;nica red privada directamente conectada a cada interfaz externa. Esto permite definir un servicio a partir de la clase de tr&#225;fico definida y los pares nodo-interfaz origen y nodo-interfaz destino.

<div class="p"><!----></div>
Por otro lado se pdr&#237;a refinar la definici&#243;n de servicio agregando m&#225;s dimensiones; una dimensi&#243;n bastante &#250;til podr&#237;a ser por ejemplo tiempo. Utilizando esta dimensi&#243;n se podr&#237;a por ejemplo definir un servicio de red privada para un rango horario determinado.

<div class="p"><!----></div>
De igual forma se pueden incorporarse m&#225;s dimensiones orientado a brindar una mayor flexibilidad en la definici&#243;n de servicios, de cara a lo que podr&#237;an ser diferentes requerimientos de la RAU2. 

<div class="p"><!----></div>
De todos modos teniendo en cuenta el alcance del proyecto se decide dejar estas dimensiones extras como una posible l&#237;nea de trabajo a futuro.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>
De esta forma la clase servicio queda determinada de la siguiente forma:<br />

<div class="p"><!----></div>
class Service(object):

<div class="p"><!----></div>
		# Atributos generales
		ID 				    # str(uuid.uuid4()) ID unico  
		name 				# Nombre del servicio para 
							# identificacion de usuarios 
							# en RAUFlow

<div class="p"><!----></div>
		lsps				# Lista de LSPs para el servicio

<div class="p"><!----></div>
		ingress_node		# Nodo de ingreso del servicio

<div class="p"><!----></div>
		egress_node 		# Nodo de egreso del servicio

<div class="p"><!----></div>
		ingress_interface 	# Interfaz de ingreso en el nodo 
							# de ingreso

<div class="p"><!----></div>
		egress_interface 	# Interfaz de egreso en el nodo 
							# de ingreso 

<div class="p"><!----></div>
		# Campos del cabezal OFv1.3 
		in_port			# Switch input port. 
		metadata 		# Metadata passed between tables. 
		eth_dst 		# Ethernet destination address.
		eth_src 		# Ethernet source address. 
		eth_type 		# Ethernet frame type. 
		vlan_vID 		# VLAN id. 
		vlan_PCP		# VLAN priority. 
		IP_dscp 		# IP DSCP (6 bits in ToS field). 
		IP_ecn  		# IP ECN (2 bits in ToS field). 
		IP_proto		# IP protocol. 
		IPv4_src 		# IPv4 source address. 
		IPv4_dst 		# IPv4 destination address. 
		TCP_src 		# TCP source port. 
		TCP_dst 		# TCP destination port. 
		UDP_src 		# UDP source port. 
		UDP_dst 		# UDP destination port. 
		SCTP_src 		# SCTP source port. 
		SCTP_dst 		# SCTP destination port. 
		ICMPv4_type 	# ICMP type. 
		ICMPv4_code 	# ICMP code. 
		IPv6_src 		# IPv6 source address. 
		IPv6_dst 		# IPv6 destination address. 
		ICMPv6_type 	# ICMPv6 type. 
		ICMPv6_code 	# ICMPv6 code. 
		MPLS_label 		# MPLS label. 
		MPLS_tc 		# MPLS TC. 

<div class="p"><!----></div>
En RAUFlow, cuando se crea un nuevo servicio se ejecutan los algoritmos de ruteo y distribución de etiquetas par construir el menos un LSP. Luego este LSP es traducido a flujos OpenFlow que luego son instalados en cada uno de los nodos que forman parte. 

<div class="p"><!----></div>
En la siguiente secci&#243;n se detalla el algoritmo de ruteo utilizado en RAUFlow.

<div class="p"><!----></div>
     <a name="tth_sEc5.3"></a><h3>
5.3&nbsp;&nbsp;Algoritmo de ruteo</h3>
El algoritmo de ruteo implementado parte de un algoritmo Shortest Path First(SPF) centralizado para el calculo del mejor camino entre un par de nodos en la topolog&#237;a. Luego incorporando restricciones al mismo es posible llegar a un Constrained Shortest Path First(CSPF) con el cual se podr&#237;a implementar funcionalidades de QoS.<br />

<div class="p"><!----></div>
En el desarrollo de este proyecto, por razones de tiempo solo fu&#233; posible la implementaci&#243;n del algor&#237;itmo SPF centralizado. De esta forma la implementaci&#243;n de un algoritmo de ruteo CSPF se identifica como una posible l&#237;nea de trabajo a futuro.<br />

<div class="p"><!----></div>
A continuaci&#243;n se explica la implementaci&#243;n del algoritmo SPF. 

<div class="p"><!----></div>

<h4>Shortest Path First</h4>
El algoritmo SPF centralizado esta basado en el algoritmo Dijkstra. Este algoritmo permite calcular en forma eficiente el mejor camino entre un par de nodos en un grafo ponderado sin costos negativos.<br />

<div class="p"><!----></div>
Puesto que en este trabajo se representa a una topolog&#237;a de red mediante un multigrafo dirigido, es necesario o bien extender el algoritmo Dijkstra a esta representaci&#243;n o bien cambiar de representaci&#243;n. 

<div class="p"><!----></div>
Como los multigrafos dirigidos ofrecen un modelado de la realidad intuitivo y directo, se opta por la alternativa de extender el algoritmo Dijkstra.<br />

<div class="p"><!----></div>
En <a href="#fig:algoSPF1">5.4</a> se muestra el pseudo-codigo del algoritmo SPF centralizado para multigrafos dirigidos obtenido a partir de la extension del algoritmo Dijkstra. Este recibe como par&#225;metros la topolog&#237;a de red(G) y el par de nodos inicio y fin para el cual se quiere calcular el mejor camino posible. 

<div class="p"><!----></div>
Las operaciones <i>ObtenerNodosMenorCostoLink</i> y <i>ObtenerNodoAdyacenteMenorCosto</i> son funciones auxiliares que se explican a continuación:

<div class="p"><!----></div>

<ul>
<li> ObtenerNodosMenorCostoLink(w, v): Permite obtener el link &lt;w, v&#62; de menor costo asociado en la topolog&#237;a entre los nodos w y v. Notar que para un par de nodos w, v pueden existir m&#250;ltiples enlaces que los conecten, eventualmente con costos diferentes.
<div class="p"><!----></div>
</li>

<li> ObtenerNodoAdyacenteMenorCosto(G\S, D): Devuelve el nodo en G\S para el cual el costo de ir desde el nodo inicio a este es m&#237;nimo en la lista de costos D.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg4">
</a>  [H]
  
 
 


<div style="text-align:center">Fig. 4: Algoritmo de ruteo Centralizado sin restricciones (SPF)</div>
<a name="fig:algoSPF1">
</a>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg5">
</a>  [H]
  
 
 

<div class="p"><!----></div>
<div style="text-align:center">Fig. 5: SPF Centralizado continuaci&#243;n</div>

<div class="p"><!----></div>
     <a name="tth_sEc5.4"></a><h3>
5.4&nbsp;&nbsp;Algoritmo de distribución de etiquetas</h3>
<a name="section5.5.4">
</a>

<div class="p"><!----></div>
Tradicionalmente se utilizan en <b>mpls</b> etiquetas para: (a) distinguir el tr&#225;fico de una VPN particular y (b) encaminar tr&#225;fico en la red(reenvío en base a etiquetas). De esta forma se tienen dos niveles de etiquetas usualmente denominadas inner label(para marcar el tr&#225;fico) y outer label(para encaminar tr&#225;fico - LSP).<br />

<div class="p"><!----></div>
Para la distribución de etiquetas(inner y outer label) existen diferentes protocolos entre los cuales puede destacarse el protocolo LDP(Label Distribution Protocol)&nbsp;[<a href="#LDPRFC" name="CITELDPRFC">Andersson et&nbsp;al., 1996</a>]. Este protocolo representa un buen modelo a seguir para el algoritmo de distribución de etiquetas, pero esta definido en base a la visión topol&#243;gica local de un nodo, mientras que en RAUFlow es necesario definir un algor&#237;tmo basado en una visi&#243;n topol&#243;gica global. Por ello es necesario implementar un algoritmo de distribución de etiquetas diferente.<br />

<div class="p"><!----></div>
A continuaci&#243;n se explican los algoritmos utilizados para la distribución de etiquetas de ambos niveles.

<div class="p"><!----></div>

<h4>Etiquetas internas(Inner Labels)</h4>
Mapear etiquetas mpls a servicios de VPN para posteriormente poder identificar el tr&#225;fico asociado dentro del prototipo es una tarea bastante simple. Una soluci&#243;n posible es asignar secuencialmente etiquetas dentro de un espacio de etiquetas disponible.

<div class="p"><!----></div>
Esta estrategia es simple y permite crear en el sistema tantos servicios como etiquetas disponibles. Puesto que una etiqueta mpls se representa mediante 20bits  = 2<sup>20</sup> posibles valores, descontando los  valores reservados por el protocolo se pueden crear m&#225;s que suficientes servicios utilizando esta estrategia.<br />

<div class="p"><!----></div>
Si bien esta estrategia es suficiente para la asignación de la etiqueta interna, m&#225;s adelante se ver&#225; que es necesario modificarla ligeramente para soportar ciertos casos de uso en el prototipo.

<div class="p"><!----></div>

<h4>Etiquetas externas (Outer Label)</h4>
Asignar etiquetas a un camino mpls es un problema mas complejo. Aprovechando la visión global de la red puede redefinirse el algoritmo de distribución de etiquetas LDP de varias formas. Algunas de ellas pueden ser:

<div class="p"><!----></div>

<ol type="1">
<li> Asignar secuencialmente etiquetas a cada salto en un camino utilizando un espacio de etiquetas global para el LSP. De esta forma no existen dos LSPs en el sistema con etiquetas en com&#250;n. Esta solución tiene como ventajas su simplicidad y bajo costo en el computo del algoritmo. Por otro lado tiene como desventaja que consume el espacio de etiquetas disponibles m&#225;s r&#225;pido(hay que considerar la escala del prototipo).
<div class="p"><!----></div>
</li>

<li> Asignar secuencialmente etiquetas a cada salto en un camino utilizando un espacio de etiquetas local a cada nodo y descartando para cada interfaz etiquetas ya utilizadas por otros LSPs. Esta estrategia tiene como ventaja que es m&#225;s austera en el consumo de etiquetas y no existen dos LSPs con iguales etiquetas para una misma interfaz de entrada en un mismo nodo.
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
Notese en relación a la segunda alternativa, que debido a la forma en que se distribuyen etiquetas puede determinarse en un nodo a que servicio corresponde un paquete bas&#225;ndse solamente en la etiqueta externa y la interfaz por la que entra. Esto permite prescindir de la etiqueta interna para determinar lo que en un esquema tradicional de mpls se denomina FEC y aplicar diferentes pol&#237;ticas de ingenier&#237;a de tr&#225;fico. Esto a su vez repercute en un aumento del MTU del paquete.<br />

<div class="p"><!----></div>
En RauFlow se implementa la segunda alternativa mencionada. En <a href="#fig:AlgoritmoDE1">5.6</a> se muestra el pseudoc&#243;digo de una posible implementaci&#243;n de dicho algor&#237;tmo:<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg6">
</a> [H]

<div class="p"><!----></div>
     <div style="text-align:center">Fig. 6: Algoritmo de distribución de etiquetas</div>
<a name="fig:AlgoritmoDE1">
</a>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg7">
</a> [H]


 


<div style="text-align:center">Fig. 7: Algoritmo de distribución de etiquetas - continuación</div>

<div class="p"><!----></div>

<h4>Stack de etiquetas MPLS</h4>
Se le denomina stack de etiquetas mpls a la pila de etiquetas que resulta de superponer una etiqueta sobre otra en un paquete. Al inicio de esta secci&#243;n se menciona la utilizaci&#243;n de dos niveles de etiquetas: una etiqueta interna para identificar el servicio y una etiqueta externa para marcar el camino dentro de la red del prototipo. Luego cuando se describe la implementaci&#243;n del algoritmo de distribución de etiquetas de segundo nivel(etiquetas externas), se muestra que es posible prescindir del primer nivel de etiquetas(etiquetas internas).<br />

<div class="p"><!----></div>
Por un lado puede implementarse el prototipo trabajando solamente con un nivel de etiquetas, garantizando reenvío en base a etiquetas y clasificaci&#243;n de tr&#225;fico. Sin embargo para poder identificar el servicio asociado a un paquete en el nodo de egreso en un LSP, es necesario que el paquete presente la etiqueta correspondiente, permitiendo de esta forma reenviarlo por la correcta interfaz de salida del prototipo. 

<div class="p"><!----></div>
Esto no es posible si se implementa Penultimate Hop Popping(PHP) dado que la etiqueta es removida en el pen&#250;ltimo hop, llegando al nodo de egreso sin etiquetas.

<div class="p"><!----></div>
Pensando en permitir en un futuro la conexi&#243;n de nodos al prototipo implementados en base a diferentes tecolog&#237;as; es decir, nodos no implementados en base a RAU-Switch, es necesario mantener la implementaci&#243;n estándar del protocolo <b>mpls</b>. Esto quiere decir que se debe implementar PHP.

<div class="p"><!----></div>
Por ello en el prototipo se trabaja con dos niveles de etiquetas.<br />

<div class="p"><!----></div>
Por otro lado, anteriormente en esta secci&#243;n se mencion&#243; la necesidad de modificar el algoritmo de distribución de etiquetas de primer nivel con el objetivo de poder implementar ciertos casos de uso. A continuaci&#243;n se explica el escenario que origina este problema y su soluci&#243;n.<br />

<div class="p"><!----></div>
En una red MPLS cada vez que un paquete es manipulado para colocar un cabezal mpls, el ethertype del paquete original es sustituido por el del protocolo. A su vez en el nodo de egreso el ethertype original del paquete debe ser colocado nuevamente. 

<div class="p"><!----></div>
Este problema puede resolverse en una VPN de capa 3 obligando a indicar el ethertype de cada servicio definido. De esta forma se utiliza la informaci&#243;n del servicio para definir un flujo que coloque el ethertype apropiado en el nodo de egreso, para cada paquete. De esta forma no se deben realizar modificaciones sustanciales a la implementaci&#243;n del prototipo. No obstante para una VPN de capa 2 no se puede implementar una soluci&#243;n similar.

<div class="p"><!----></div>
De esta forma es necesario guardar en alg&#250;n lado el valor original de ethertype para cada paquete que ingresa al prototipo y colocarlo en el &#250;ltimo nodo del camino, si se quiere soportar la implementacion de VPNs de capa 2. Esto puede realizarse al menos de las siguientes dos formas:

<div class="p"><!----></div>

<ol type="1">
<li> Cada paquete que ingresa a la red del prototipo es enviado al Controlador en donde se extrae y guarda el valor original de ethertype. Luego se continua con el procesamiento del paquete acorde a las reglas ya definidas. Luego cuando el paquete arriba al ultimo nodo, tras procesarse acorde a las reglas ya definidas(extraer etiqueta interna por ejemplo) el paquete es enviado al Controlador donde se le coloca el valor original de ethertype. Finalmente se reenv&#237;a el paquete por la interfaz de salida de la red.

<div class="p"><!----></div>
Este soluci&#243;n se corresponde con un enfoque reactivo en una red SDN (recordar estado del arte en las redes definidas por software).
<div class="p"><!----></div>
</li>

<li> Se utilizan etiquetas para indicar el ethertype de un paquete. Se puede definir un tercer nivel de etiquetas para esto, definiendo un mapeo entre ethertype y etiqueta. Luego el nodo de egreso utilzando esta informaci&#243;n puede localmente mapear etiquetas a ethertypes colocando en el paquete el valor original antes de reenviarlo por la interfaz de salida. 

<div class="p"><!----></div>
Sin embargo utilizar un tercer nivel de etiqueta agregar&#237;a complejidad al procesamiento del paquete, pudiendo perjudicar la performance del prototipo, adem&#225;s de que reduce el MTU de un paquete.<br />

<div class="p"><!----></div>
Una alternativa, es utilizar el primer nivel de etiquetas para identificar el ethertype, definiendo para cada servicio de VPN en vez de una etiqueta un rango de etiquetas(una etiqueta para cada tipo de ethertype soportado). De esta forma se pueden definir flujos para cada servicio y cada ethertype en el nodo de egreso para colocar nuevamente el ethertype original dependiendo del valor en la etiqueta de primer nivel.

<div class="p"><!----></div>
En RAUFlow se implementa esta estrategia en la implementaci&#243;n de servicios de redes privadas de capa 2.
<div class="p"><!----></div>
</li>
</ol> 

<div class="p"><!----></div>
     <a name="tth_sEc5.5"></a><h3>
5.5&nbsp;&nbsp;Actualizaci&#243;n de la topologia</h3>
Una vez que el m&#243;dulo LSDB Sync envía a la aplicaci&#243;n la informaci&#243;n topol&#243;gica actualizada, se desencadena una secuencia acciones para la actualizaci&#243;n de la informaci&#243;n almacenada localmente (informaci&#243;n topolo&#243;gica y de servicios). Este proceso puede resumirse de la siguiente forma:

<div class="p"><!----></div>

<ol type="1">
<li> Se ejecuta el algoritmo <b>ActualizarTopologia</b> el cual mezcla la informaci&#243;n de la topolog&#237;a mantenida en memoria(informaci&#243;n desactualizada) con la informaci&#243;n recibida (informaci&#243;n actual). Se actualiza cada instancia de Nodo, Interfaz y Link considerando los casos en que se deben crear nuevos objetos, actualizar existentes y eventualmente eliminarlos. Cabe destacar que la operaci&#243;n eliminar tanto para Nodos, Interfaces como Links no efectúa un borrado f&#237;sico de estos objetos, simplemente los marca como no operativos(atributo “state” en cada clase).
<div class="p"><!----></div>
</li>

<li> Se ejecuta el algoritmo <b>ActualizarServiciosLSPs</b> el cual recalcula para cada servicio en el sistema los LSPs asociados. El proceso de actualización de un LSP implica:

<div class="p"><!----></div>

<ol type="a">
<li> Recalcular el mejor camino entre los nodos origen y destino del servicio
<div class="p"><!----></div>
</li>

<li> Mapear etiquetas al nuevo “mejor camino” conservando las mismas etiquetas que ten&#237;a el camino viejo para los links que se mantienen en el camino nuevo
<div class="p"><!----></div>
</li>

<li> Actualizar las tablas MPLS en cada nodo de la topolog&#237;a
<div class="p"><!----></div>
</li>

<li> Actualizar las tablas de flujos en cada nodo de la la topolog&#237;a
<div class="p"><!----></div>
</li>
</ol>
<div class="p"><!----></div>
</li>
</ol>    

<div class="p"><!----></div>
De ambos algoritmos se elige <b>ActualizarServiciosLSPs</b> para mostrar a continuaci&#243;n su funcionamiento a trav&#233;s de su especificaci&#243;n en pseudo-c&#243;digo.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg8">
</a> [H]
 Declare topologia <br />
 Declare servicios <br />
  
 
   


<div style="text-align:center">Fig. 8: Algoritmo de actualización de la topolog&#237;a</div>
<a name="fig:AlgoritmoDA1">
</a>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg9">
</a> [H]
  
 
 

<div class="p"><!----></div>
<div style="text-align:center">Fig. 9: Algoritmo de actualización de la topolog&#237;a - continuaci&#243;n</div>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg10">
</a> [H]
  
 
 

<div class="p"><!----></div>
<div style="text-align:center">Fig. 10: Algoritmo de actualización de la topolog&#237;a - continuaci&#243;n</div>

<div class="p"><!----></div>
     <a name="tth_sEc5.6"></a><h3>
5.6&nbsp;&nbsp;Ciclo de vida de un Nodo</h3>
En el prototipo se obtiene informaci&#243;n de varias fuentes de datos para la construcci&#243;n de la topolog&#237;a. Algunas de ellas son la base de datos topol&#243;gica de Quagga(LSDB), el datapath OpenFlow y a través del ingreso manual de informaci&#243;n mediante la interfaz gr&#225;fica de RAUFlow.

<div class="p"><!----></div>
Estas fuentes aportan datos en forma independiente, lo cual incide directamente en la construcci&#243;n de la topolog&#237;a y en decisiones que se toman dentro del prototipo como por ejemplo en el algoritmo de ruteo entre otros. Por ello a contnuaci&#243;n se ejemplifica el proceso de construcci&#243;n de la topolog&#237;a a partir de las fuentes de datos, tomando como ejemplo el ciclo de vida de un nodo en particular. Luego el ciclo de vida de interfaces y enlaces puede deducirse de forma análoga.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg11">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 11: Ciclo de vida de un Nodo</div>
<a name="fig:CicloVidaNodo">
</a>
</div>
<div class="p"><!----></div>
  Como se muestra en la figura<a href="#fig:CicloVidaNodo">5.11</a>, descartando el pseudo-estado inicial se identifican cinco estados diferentes para un Nodo: <i>Descubrimiento OpenFlow</i>, <i>Sin Conexión OpenFlow</i>, <i>Operativo</i>, <i>LSDB Eliminado OpenFlow Inactivo</i> y <i>LSDB Eliminado OpenFlow Activo</i>.<br />

<div class="p"><!----></div>
El significado de cada uno de estos as&#237; como las transiciones posibles se explican a continuación:

<div class="p"><!----></div>

<ul>
<li> <i>Descubrimiento OpenFlow:</i> Cuando un switch se reporta mediante el canal OpenFlow por primera vez con RAUFlow, se guarda diferente informaci&#243;n del mismo entre la cual se encuentra la instancia de la clase Datapath en la implementaci&#243;n de Ryu(esta instancia es utilizada posteriormente para interactuar con el dispositivo). Si el switch OpenFlow no se encuentra instanciado en el sistema como Nodo, la informaci&#243;n recibida se almacena en una lista auxilar de nodos. No se crea una instancia de Nodo como tal a&#250;n, instanciandose solamente cuando se advierte la precencia de &#233;ste en la Link State Database(LSDB). 

<div class="p"><!----></div>
Esto se corresponde con una decisión de diseño, en la cual se busca reconocer a un Nodo tanto para las funcionalidades de la interfaz gr&#225;fica de RAUFlow como para el algoritmo de ruteo, solamente cuando se advierte la existencia del mismo en la inforamci&#243;n de la LSDB. 

<div class="p"><!----></div>
De este estado existen dos transiciones posibles. En primer lugar el switch puede dejar de reportarse con el controlador en cuyo caso es eliminado de la lista temporal de nodos y en segundo lugar la componente LSDBSync puede enviar una actualizaci&#243;n de la topolog&#237;a, en donde se incluye el nodo en cuestión. En este caso se instancia un objeto Nodo con toda la informaci&#243;n provista por la LSDB, datapath de OpenFlow y la inormaci&#243;n adicional obtenida con el agente SNMP. Luego el Nodo cambia al estado <i>Operativo</i>.

<div class="p"><!----></div>
Vale la pena destacar que en caso de no poder obtener la informaci&#243;n adicional con el agente SNMP, el Nodo no es instanciado y se mantiene en el estado actual.
<div class="p"><!----></div>
</li>

<li> <i>Operacional:</i>

<div class="p"><!----></div>
Este estado es utilizado para representar a aquellos nodos que por un lado se encuentran en la LSDB y fueron rectificados o instanciados en la &#250;ltima actualizaci&#243;n de la topolog&#237;a, y por otro se encuentran report&#225;ndose mediante el protocolo OpenFlow.

<div class="p"><!----></div>
En este estado el nodo es mostrado por la interfaz gr&#225;fica de RAUFlow y se tiene acceso a todas las características del mismo, tanto del modelo de datos como las que son accesibles a trav&#233;s del datapath OpenFlow. Adem&#225;s es considerado para el calculo de las mejores rutas y puede ser utilizado como nodo de ingreso &#243; egreso en la definici&#243;n de servicios.

<div class="p"><!----></div>
En pocas palabras es un nodo completamente funcional a los efectos de la implementaci&#243;n de RAUFlow.

<div class="p"><!----></div>
Desde este estado se tienen tres transiciones posibles. Una primera posibilidad es que se produzca una actualización de la LSDB(evento LSDBSyncActualizarTopologia) y que en la topolog&#237;a recibida se encuentre el Nodo en cuesti&#243;n. En este caso solamente se actualiza la informaci&#243;n asociada al nodo(acción ActualizarNodo), entre la cual se incluye la actualización de las Interfaces y Links, quedando en el estado actual. Una segunda posibilidad es que el switch asociado deje de reportarse por el canal OpenFlow(evento OpenFlowConexiónPerdida) en cuyo caso se modifica el atributo <b>of_ready</b> del Nodo asignando el valor False y se cambia al estado <i>Sin Conexión OpenFlow</i>. Por &#250;ltimo una tercera posibilidad es que se produzca una actualizaci&#243;n de la LSDB(evento LSDBSyncActualizarTopologia) y que el Nodo no este en la topolog&#237;a recibida. En este caso se deshabilita el Nodo(NodoDOWN) asignando el valor DOWN al atributo <b>state</b> y se cambia al estado <i>LSDB Eliminado OpenFlow Activo</i>.
<div class="p"><!----></div>
</li>

<li> <i>LSDB Eliminado OpenFlow Activo:</i> Este estado contempla a un Nodo que no se encontraba en la LSDB recibida en la ultima actualizaci&#243;n topol&#243;gica pero que a&#250;n se reportan por el canal de comunicaci&#243;n OpenFlow.

<div class="p"><!----></div>
Desde este estado se tienen solamente dos transiciones posibles. Una primera posibilidad es que se produzca nuevamente una actualización de la topolog&#237;a 
(LSDBSyncActualizarTopologia) y que el nodo se encuentre dentro de la información recibida. En este caso se actualiza el atributo <b>state</b> del nodo valor UP y se cambia al estado <i>Operativo</i>. Una segunda posibilidad es que el nodo deje de reportarse por el canal OpenFlow(OpenFlowConexiónPerdida) en cuyo caso se actualiza el valor del atributo <b>of_ready</b> al valor False y se cambia al estado <i>LSDB Eliminado OpenFlow Inactivo</i>.
<div class="p"><!----></div>
</li>

<li> <i>Sin Conexión OpenFlow:</i> Este estado contempla a un Nodo que fue creado o rectificado en la &#250;ltima actualización de la LSDB pero que no se ha reportado por el canal OpenFlow, ya sea porque nunca lo hizo o porque dejo de hacerlo. En este estado el Nodo es mostrado a trav&#233;s de la interfaz gr&#225;fica de RAUFlow pero no es considerado ni para el el calculo de mejores caminos ni para la creaci&#243;n de nuevos servicios tanto como nodo de ingreso &#243; como nodo de egreso. Adem&#225;s la informaci&#243;n asociada al datapath de OpenFlow como las tabla de flujos y estad&#237;sticas tampoco son accesibles.

<div class="p"><!----></div>
De este estado existen tres transiciones posibles. Por un lado el nodo puede estar presente en la LSDB al producirse una actualizaci&#243;n de la topolog&#237;a<br />(LSDBSyncActualizarTopologia), actualizandose la informaci&#243;n del nodo <br />(ActualizarNodo) y manteniéndose en el mismo estado. Por otro lado tambi&#233;n al producirse una actualizaci&#243;n de la topolog&#237;a, el nodo puede no estar en la LSDB, en cuyo caso se actualiza el atributo <b>state</b> al valor DOWN y se cambia al estado <i>LSDB Eliminado OpenFlow Inactivo</i>. Finalmente el switch puede empezar a reportarse por el canal OpenFlow(OpenFlowReport) en cuyo caso se actualiza el valor del atributo <b>of_ready</b> a True y se cambia al estado <i>Operativo</i>.
<div class="p"><!----></div>
</li>

<li> <i>LSDB Eliminado OpenFlow Inactivo:</i> Este estado modela a un Nodo que nunca se reporto &#243; dej&#243; de reportarse por el canal OpenFlow, y que a su vez no se encuentra dentro de la LSDB en la &#250;ltima actualizaci&#243;n topol&#243;gica realizada. De este estado existen tres transiciones posibles.

<div class="p"><!----></div>
Una primera alternativa es que el switch en cuesti&#243;n se reporte con la aplicaci&#243;n <br />(OpenFlowReporte), en cuyo caso se actualiza el valor del atributo <b>of_ready</b> a True y se cambia el estado a <i>LSDB Eliminado OpenFlow Activo</i>. Una segunda alternativa es que se produzca una actualizaci&#243;n de la topolog&#237;a y el Nodo se encuentre dentro de la LSDB, en cuyo caso se cambia el valor del atributo <b>state</b> a UP y se cambia al estado <i>Sin conexión OpenFlow</i>. Finalmente una tercera alternativa es que el nodo  sea eliminado del sistema. Si bien esta funcionalidad no se encuentra actualmente implementada, la idea es que un Nodo para el cual no se ha recibido reportes por el canal OpenFlow luego de un tiempo determinado, y que tampoco aparece en la LSDB tras una actualicaci&#243;n, pueda ser eliminado del sistema, eventualmente tras la intervenci&#243;n de un usuario administrador.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
El ciclo de vida de una Interfaz &#243; un Link puede ser explicado de forma an&#225;loga con una m&#225;quina de estados similar, teniendo presente las relaciones existentes entre las tres clases, Nodos, Interfaces y Links.<br /><br />

<div class="p"><!----></div>
De esta forma queda explicada la implementaci&#243;n de la aplicaci&#243;n de gesti&#243;n RAUFlow, el diseño de  los nodos que formar&#225;n parte de la red prototipo (RAU-Switch) y el funcionamiento general del prototipo en su conjunto. 

<div class="p"><!----></div>
El siguiente cap&#237;tulo esta destinado al diseño y construcción de un laboratorio de pruebas, para la validaci&#243;n de las componentes desarrolladas y la implementaci&#243;n de algunos casos de uso representativos. 

<div class="p"><!----></div>
 <a name="tth_chAp6"></a><h1>
Capítulo 6 <br />Laboratorio de pruebas</h1>
<a name="chapter6">
</a>

<div class="p"><!----></div>

    


<div class="p"><!----></div>
Para validar funcionalmente el prototipo es preciso la construcción de un laboratorio de experimentación sobre el cual ejecutar un conjunto de pruebas e implementar algunos casos de uso representativos. Dentro de las pruebas funcionales se busca verificar aspectos importantes de la implementaci&#243;n como la clasificación de tr&#225;fico, el algoritmo de ruteo dinámico y el algoritmo de distribución de etiquetas.

<div class="p"><!----></div>
Por otro lado con la implementaci&#243;n de algunos casos de uso representativos se busca validar la utilización del enfoque OpenFlow/SDN en la construcci&#243;n a futuro de la RAU2.<br />

<div class="p"><!----></div>
El siguiente capitulo esta destinado a la presentación del laboratorio de pruebas construido y a la descripción de las diferentes pruebas realizadas.

<div class="p"><!----></div>

 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Definición del Laboratorio</h2>

<div class="p"><!----></div>
Como se menciona anteriormente uno de los objetivos de este laboratorio de experimentaci&#243;n es verificar el correcto funcionamiento de la implementaci&#243;n realizada, en particular en los aspectos críticos de la misma como la capacidad para clasificar trafico y los diferentes algoritmos de ruteo, distribución de etiquetas, actualización topologica, etc. 

<div class="p"><!----></div>
Para esto es necesario construir un prototipo con suficientes nodos y enlaces como para poder definir varios servicios de redes privadas. Tambi&#233;n interesa la existencia de caminos alternativos entre un par de nodos origen y destino para poder comprobar el funcionamiento del algoritmo de ruteo y evaluar el comportamiento del prototipo cuando la topolog&#237;a cambia, desconectando enlaces, apagando nodos, etc.<br />

<div class="p"><!----></div>
Teniendo en cuanta los requerimientos mencionados se construye un laboratorio de pruebas con la siguiente topolog&#237;a (ver figura <a href="#fig:LaboratorioDePruebasTopo">6.1</a>).

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg1">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 1: Laboratorio de pruebas - Topolog&#237;a</div>
<a name="fig:LaboratorioDePruebasTopo">
</a>
</div>
<div class="p"><!----></div>
El laboratorio se compone de cuatro nodos implementados en base al dispositivo RAU-Switch conectados entre si con enlaces de fibra &#243;ptica multimodo. Cada nodo esta conectado a los otros tres nodos de la topolog&#237;a implementando de esta forma una topolog&#237;a full mesh de cuatro nodos.

<div class="p"><!----></div>
A su vez cada nodo esta conectado al controlador SDN del prototipo mediante una interfaz de red de 10Mb (interfaces <b>eth0</b>).<br />

<div class="p"><!----></div>
Por la forma que tiene la topolog&#237;a, los cuatro nodos nombrados <i>Galois</i>, <i>Poisson</i>, <i>Oz</i> y <i>Alice</i> son nodos de borde. Esto quiere decir que RAUFlow los considera como nodos habilitados para ser nodo de ingreso &#243; nodo de egreso en la definici&#243;n de servicios de redes privadas.

<div class="p"><!----></div>
Por otro lado cada nodo cuenta con una interfaz de red de 100Mb (interfaz <b>eth1</b>) utilizada como interfaz externa para la conexi&#243;n con otras subredes. Estas interfaces son utilizadas por RAUFlow para la definici&#243;n de servicios de VPN como los puntos de entrada y salida de tr&#225;fico. Por tanto cada una de estas interfaces se encuentra directamente conectada a la subred de una VPN en particular.<br />

<div class="p"><!----></div>
Como se menciona anteriormente en lacp&#237;tulo 5, la representaci&#243;n utilizada para modelar una topolog&#237;a de red es la de un multigrafo dirigido ponderado. En la figura <a href="#fig:LaboratorioDePruebasCostos">6.2</a> se muestra esta representaci&#243;n para la topolog&#237;a del prototipo. Como puede apreciarse en la im&#225;gen cada enlace tiene su respectivo costo asociado.

<div class="p"><!----></div>
Por simplicidad en el diagrama se han obviado los enlaces existentes entre el Controlador y cada uno de los nodos. Como se menciona en el cap&#237;tulo 4 la instancia de Quagga ejecutada en el controlador tiene como &#250;nico objetivo contar con un acceso local a la LSDB. Por ello conceptualmente el costo asociado a cada uno de estos enlaces es infinito, lo cual en pr&#225;ctica se realiza asignando el valor 65535.  

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg2">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 2: Laboratorio de pruebas - Costos de la topolog&#237;a</div>
<a name="fig:LaboratorioDePruebasCostos">
</a>
</div>
<div class="p"><!----></div>
Sobre este laboratorio se implementan dos casos de uso representativos: (a) VPN de capa 3 y (b) VPN de capa 2. Sobre cada uno de estos casos de uso a su vez se ejecutan una serie de pruebas orientadas a verificar el correcto funcionamiento de las componentes m&#225;s importantes en la implementaci&#243;n de RAUFlow y RAU-Switch.

<div class="p"><!----></div>
A continuaci&#243;n se describen los resultados obtenidos en la implementaci&#243;n de estos casos de uso y la eecuci&#243;n de estas pruebas, comenzando por el caso de uso VPN de capa 3.

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;VPN de capa 3</h2>

<div class="p"><!----></div>
Las redes privadas virtuales de capa 3 son un tipo de servicio comunmente brindado por un operador de red y es en particular uno de los servicios que se quiere implementar en la RAU2. En particular con este tipo de red privada se puede implementar clasificaci&#243;n de tr&#225;fico en base a tipo de aplicaci&#243;n y numeraci&#243;n de capa 3.<br />

<div class="p"><!----></div>
En este trabajo se implementan dos escenarios diferentes para este tipo de red privada: (a) red privada multipunto con una &#250;nica organizaci&#243;n y tres sucursales f&#237;sicamente separadas, (b) red privada punto a punto con dos organizaciones, cada una de ellas con dos sucursales f&#237;sicamente separadas.

<div class="p"><!----></div>
     <a name="tth_sEc2.1"></a><h3>
2.1&nbsp;&nbsp;Escenario 1 - Red Privada Multipunto</h3>

<div class="p"><!----></div>
Este escenario representa una red privada multipunto de capa 3. Esta compuesto por una &#250;nica organizaci&#243;n o red privada dividida en 3 sucursales o subredes.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg3">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 3: VPN de capa 3 - Escenario 1</div>
<a name="fig:CUP1">
</a>
</div>
<div class="p"><!----></div>
Sobre este escenario se ejecutan una serie de pruebas orientadas a verificar los siguientes aspectos relacionados al prototipo:

<div class="p"><!----></div>

<ol type="1">
<li> Algoritmo de ruteo
<div class="p"><!----></div>
</li>

<li> Algoritmo de distribución de etiquetas
<div class="p"><!----></div>
</li>

<li> Clasificaci&#243;n de tr&#225;fico
<div class="p"><!----></div>
</li>

<li> Actualizaci&#243;n de rutas cuando cambia la topolog&#237;a
<div class="p"><!----></div>
</li>

<li> Capacidad para crear VPN multipunto
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
Para construir la red privada multipunto uniendo las tres subredes mencionadas y adem&#225;s verificar los puntos anteriores, se instancian los siguientes servicios(ver tabla <a href="#table:TablaFlujos">6.1</a>). Por cada par de subredes se crean dos servicios (un servicio para cada sentido del tr&#225;fico).<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb1">
</a> 
<table border="1">
<tr><td align="left">Nombre </td><td align="left">Ingreso </td><td align="left">Egreso </td><td width="158">Clasificación </td><td width="158">Descripción </td></tr>
<tr><td align="left"> S1 </td><td align="left">Galois - eth1 </td><td align="left">Oz - eth1 </td><td width="158">ip_src=20.20.20.128/26 ip_dst=20.20.20.0/26 </td><td width="158">Tr&#225;fico de Subred A a Subred C </td></tr>
<tr><td align="left"> S2 </td><td align="left">Oz - eth1 </td><td align="left">Galois - eth1 </td><td width="158">ip_src=20.20.20.0/26 ip_dst=20.20.20.128/26 </td><td width="158">Tr&#225;fico de Subred C a Subred A </td></tr>
<tr><td align="left"> S3 </td><td align="left">Galois - eth1 </td><td align="left">Poisson - eth1 </td><td width="158">ip_src=20.20.20.128/26 ip_dst=20.20.20.64/26 </td><td width="158">Tr&#225;fico de Subred A a Subred B </td></tr>
<tr><td align="left"> S4 </td><td align="left">Poisson - eth1 </td><td align="left">Galois - eth1 </td><td width="158">ip_src=20.20.20.64/26 ip_dst=20.20.20.128/26 </td><td width="158">Tr&#225;fico de Subred B a Subred A </td></tr>
<tr><td align="left"> S5 </td><td align="left">Poisson - eth1 </td><td align="left">Oz - eth1 </td><td width="158">ip_src=20.20.20.64/26 ip_dst=20.20.20.0/26 </td><td width="158">Tr&#225;fico de Subred B a Subred C </td></tr>
<tr><td align="left"> S6 </td><td align="left">Oz - eth1 </td><td align="left">Poisson - eth1 </td><td width="158">ip_src=20.20.20.0/26 ip_dst=20.20.20.64/26 </td><td width="158">Tr&#225;fico de Subred C a Subred B </td></tr></table>



<div style="text-align:center">Table 1: CU1 - Escenario 1, servicios creados</div>
<a name="table:TablaFlujos">
</a>

<div class="p"><!----></div>
Para cada uno de estos servicios adem&#225;s se indica el etherype 0x0800 correspondiente al tipo de tr&#225;fico IPv4.<br />

<div class="p"><!----></div>

<h4>Verificaci&#243;n de Algoritmo de ruteo</h4>
Para verificar el correcto funcionamiento del algoritmo de ruteo, se comparan los caminos calculados para cada servicio con los respectivos mejores caminos te&#243;ricos. 

<div class="p"><!----></div>
Para cada camino se verifican dos cosas: (1) que el camino calculado se corresponde con el camino te&#243;rico y (2) que el camino es correctamente instalado como reglas de reenvío (en base a conmutaci&#243;n de etiquetas) en la tabla de flujos OpenFlow de cada nodo del camino.<br />

<div class="p"><!----></div>
Los caminos te&#243;ricos son calculados manualmente observando los costos de la topolog&#237;a (figura <a href="#fig:LaboratorioDePruebasCostos">6.2</a>). Los resultados de este procedimiento se muestran en la figura
 <a href="#fig:CUP1Caminos">6.4</a>.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg4">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 4: Escenario 1 - Caminos para servicios</div>
<a name="fig:CUP1Caminos">
</a>
</div>
<div class="p"><!----></div>
Para comprobar que los caminos calculados por el algoritmo sean los correctos se analizan las tablas de flujos de Open vSwitch en cada nodo de la red del laboratorio. A partir del contenido de estas tablas fácilmente puede reconocerse el camino calculado.<br />

<div class="p"><!----></div>
En la figura <a href="#fig:CU1P1DumpFlows1">6.5</a> se muestran las tablas de flujo para cada nodo. Para conseguir esta informaci&#243;n se utiliza el comando dump-flows de la herrmaienta Open vSwitch. No obstante puede obtenerse tambi&#233;n esta informaci&#243;n de la propia interfaz gr&#225;fica de RAUFlow.<br />

<div class="p"><!----></div>
Tomando como ejemplo el caso del servicio S5, acorde al camino te&#243;rico todo tr&#225;fico con origen en la Subred B y destino Subred C es encaminado por el enlace que conecta directamente a ambos nodos por las respectivas interfaces <i>nf1</i>. 

<div class="p"><!----></div>
Asumiendose de aqu&#237; en m&#225;s la notaci&#243;n (n, i) para referirse a un enlace, donde <i>n</i> indica nodo origen e <i>i</i> interfaz de reenvío en <i>n</i> para el próximo salto,  &lt; (n<sub>1</sub>, i<sub>1</sub>), ..., (n<sub>k</sub>, i<sub>k</sub>) &gt;  para representar un camino, entonces el camino te&#243;rico para S5 es el siguiente:

<div class="p"><!----></div>
 
<br clear="all" /><table border="0" width="100%"><tr><td>
<table align="center" cellspacing="0"  cellpadding="2"><tr><td nowrap="nowrap" align="center">
 &lt; (Poisson, nf<sub>1</sub>) &gt; </td></tr></table>
</td></tr></table>



<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg5">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 5: Cambiar por imagen correcta</div>
<a name="fig:CU1P1DumpFlows1">
</a>
</div>
<div class="p"><!----></div>
[Aca describo los flujos en la tablas de Poisson que hacen que el trafico sea encapsulado y enviado por nf1, y luego los lujos en Oz que desencapsulan el trafico y lo reenvian por eth1 a la subred C implementando de esta forma el camino &lt;Poisson, nf1&#62;]<br />

<div class="p"><!----></div>
Por otro lado si se toma como ejemplo el servicio S6, seg&#250;n los costos de la topolog&#237;a el camino deber&#237;a de ser:

<div class="p"><!----></div>

<br clear="all" /><table border="0" width="100%"><tr><td>
<table align="center" cellspacing="0"  cellpadding="2"><tr><td nowrap="nowrap" align="center">
 &lt; (Oz, nf<sub>0</sub>), (Poisson, nf<sub>2</sub>) &gt; </td></tr></table>
</td></tr></table>

 

<div class="p"><!----></div>
Analizando las tablas de flujos de los nodos <i>Oz</i>, <i>Galois</i> y <i>Poisson</i> fácilmente se puede comprobar que el camino calculado se corresponde con el camino te&#243;rico.<br />

<div class="p"><!----></div>
[Explicar en base a los flujos como se hace el renvio de un lado para el otro]<br />

<div class="p"><!----></div>

<h4>Verificaci&#243;n de Algoritmo de distribución de etiquetas</h4>

<div class="p"><!----></div>
[De repente comentar en funcion al escenario anterior porque anda bien]

<div class="p"><!----></div>

<h4>Clasificaci&#243;n de tr&#225;fico</h4>
Se implementa clasificaci&#243;n de tr&#225;fico en el nodo de borde identificado como ingreso para el tr&#225;fico de un servicio en concreto. En particular para el escenario con el que se viene trabajando, se implementa clasificaci&#243;n de tr&#225;fico en los nodos de ingreso basándose en la numeraci&#243;n IP origen y destino de un paquete. De esta forma por ejemplo el nodo Galois determina que camino debe tomar un paquete con origen en la Subred A y destino la Subred B. 

<div class="p"><!----></div>
Sin embargo RAUFlow admite realizar clasificaci&#243;n de tr&#225;fico por un conjunto bastante m&#225;s amplio de atributos. A continuaci&#243;n se definen una nueva lista de servicios orientados a demostrar la correcta implementaci&#243;n de esta funcionalidad.

<div class="p"><!----></div>
Cabe destacar que para esta prueba los servicios anteriormente creados son eliminados.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb2">
</a> 
<table border="1">
<tr><td align="left">Nombre </td><td align="left">Ingreso </td><td align="left">Egreso </td><td width="158">Clasificación </td><td width="158">Descripción </td></tr>
<tr><td align="left"> S1 </td><td align="left">Galois - eth1 </td><td align="left">Oz - eth1 </td><td width="158">ip_src=20.20.20.128/26 ip_dst=20.20.20.0/26 </td><td width="158">Tr&#225;fico de Subred A a Subred C </td></tr>
<tr><td align="left"> S2 </td><td align="left">Oz - eth1 </td><td align="left">Galois - eth1 </td><td width="158">ip_src=20.20.20.0/26 ip_dst=20.20.20.128/26 </td><td width="158">Tr&#225;fico de Subred C a Subred A </td></tr>
<tr><td align="left"> S3 </td><td align="left">Galois - eth1 </td><td align="left">Poisson - eth1 </td><td width="158">ip_src=20.20.20.128/26 ip_dst=20.20.20.64/26 </td><td width="158">Tr&#225;fico de Subred A a Subred B </td></tr>
<tr><td align="left"> S4 </td><td align="left">Poisson - eth1 </td><td align="left">Galois - eth1 </td><td width="158">ip_src=20.20.20.64/26 ip_dst=20.20.20.128/26 </td><td width="158">Tr&#225;fico de Subred B a Subred A </td></tr>
<tr><td align="left"> S5 </td><td align="left">Poisson - eth1 </td><td align="left">Oz - eth1 </td><td width="158">ip_src=20.20.20.64/26 ip_dst=20.20.20.0/26 </td><td width="158">Tr&#225;fico de Subred B a Subred C </td></tr>
<tr><td align="left"> S6 </td><td align="left">Oz - eth1 </td><td align="left">Poisson - eth1 </td><td width="158">ip_src=20.20.20.0/26 ip_dst=20.20.20.64/26 </td><td width="158">Tr&#225;fico de Subred C a Subred B </td></tr></table>



<div style="text-align:center">Table 2: CU1 - Escenario 1, servicios extra</div>
<a name="table:TablaFlujos2">
</a>

<div class="p"><!----></div>

<h4>Actualizaci&#243;n de rutas</h4>
Para probar la actualizaci&#243;n de rutas cuando cambia la topolog&#237;a se trabaja nuevamente con los servicios definidos en <a href="#table:TablaFlujos">6.1</a>. 

<div class="p"><!----></div>
Una vez configurado el laboratorio de esta forma, se procede a derribar manualmente los enlaces  &lt; (Galois, nf<sub>2</sub>), (Poisson, nf<sub>2</sub>) &gt;  y  &lt; (Poisson, nf<sub>2</sub>), (Poisson, nf<sub>2</sub>) &gt; . De esta forma se produce una actualizaci&#243;n de la topolog&#237;a tras la cual se ejecutan los algoritmos de ruteo y distribución de etiquetas para actualizar cada LSP.<br />

<div class="p"><!----></div>
Acorde a los costos de la topolog&#237;a, en esta nueva topolog&#237;a los caminos asociados a cada servicio quedar&#237;an de la siguiente forma:

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg6">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 6: Escenario 1 - Caminos para servicios recalculados</div>
<a name="fig:CUP1Caminos2">
</a>
</div>
<div class="p"><!----></div>
 Notese que los &#250;nicos caminos que cambian son los asociados a los servicios S3 y S6. Cabe destacar adem&#225;s que en la nueva opolog&#237;a existe m&#225;s de un camino de m&#237;nimo costo para S3. Los caminos  &lt; (Galois, nf0), (Oz, nf1) &gt;  y  &lt; (Galois, nf0), (Oz, nf2), (Alice, nf0) &gt;  presentan ambos el costo 4.
Por tanto se tienen dos resultados v&#225;lidos posibles para la salida del algoritmo de ruteo para este servicio.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg7">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 7: Cambiar por imagen correcta</div>
<a name="fig:CU1P1DumpFlows2">
</a>
</div>
<div class="p"><!----></div>
     <a name="tth_sEc2.2"></a><h3>
2.2&nbsp;&nbsp;Escenario 2</h3>

<div class="p"><!----></div>
Este escenario representa una red privada punto a punto de capa 3. Esta compuesto por dos organizaciones diferentes, cada una de ellas con dos sucursales f&#237;sicamente separadas.

<div class="p"><!----></div>
Se quiere instancia entonces servicios en RAUFlow con el objetivo de brindar conectividad entre cada par de sucursales.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg8">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 8: VPN de capa 3 - Escenario 2</div>
<a name="fig:CUP2">
</a>
</div>
<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;Pruebas</h2>

<div class="p"><!----></div>
     <a name="tth_sEc3.1"></a><h3>
3.1&nbsp;&nbsp;Asignación de etiquetas</h3>

<div class="p"><!----></div>

<h4>Objetivos</h4>

<ol type="1">
<li> Probar que el algoritmo de distribución de etiquetas funciona correctamente en relación a: (a) asignación de etiquetas identificadoras a nuevos servicios(segundo nivel de etiquetas) y (b) asignación de etiquetas a caminos LSP.
<div class="p"><!----></div>
</li>

<li> Analizar el comportamiento de este algoritmo cuando se prueba con un rango amplio de etiquetas dentro del espacio de etiquetas disponible.
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>

<h4>Descripción de la prueba:</h4>
[Crear muchisimos servicios para utilizar gran parte del arango de etiquetas disponibles. Tomar un muestreo de servicios que utilicen etiquetas que creamos convenientes y analizar el comportamiento del prototipo para el trafico generado en estos servicios en:]

<div class="p"><!----></div>

<ul>
<li> Trafico asociado a una VPN es correctamente etiquetado a la entrada del prototipo
<div class="p"><!----></div>
</li>

<li> Trafico asociado a una VPN sale de la red del laboratorio por el nodo de salida y la interfaz de salida prevista, sin etiquetas.
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>

<h4>Resultados obtenidos:</h4>

<div class="p"><!----></div>
     <a name="tth_sEc3.2"></a><h3>
3.2&nbsp;&nbsp;Clasificación de tr&#225;fico</h3>

<div class="p"><!----></div>

<h4>Objetivos</h4>

<ol type="1">
<li> Verificar la clasificación de trafico en base a los matching fields de OpenFlow en los nodos de ingreso
<div class="p"><!----></div>
</li>

<li> Verificar la clasificación de tr&#225;fico en función a los campos puerto de entrada y etiqueta en los nodos internos(LSRs)
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>

<h4>Descripción de la prueba</h4>
[Hacemos capturas de pantalla en nodos de entrada y en nodos del medio para ver que el trafico se clasifica correctamente. Para esto debemos crear varios servicios]

<div class="p"><!----></div>

<h4>Resultados obtenidos</h4>

<div class="p"><!----></div>
     <a name="tth_sEc3.3"></a><h3>
3.3&nbsp;&nbsp;Algoritmo de ruteo dinámico</h3>

<div class="p"><!----></div>

<h4>Objetivos:</h4>

<div class="p"><!----></div>

<h4>Descripción de la prueba:</h4>

<div class="p"><!----></div>

<h4>Resultados obtenidos:</h4>

<div class="p"><!----></div>
     <a name="tth_sEc3.4"></a><h3>
3.4&nbsp;&nbsp;Numeraciones superpuestas</h3>

<div class="p"><!----></div>
<b>Objetivos</b>

<div class="p"><!----></div>
<b>Descripción de la prueba</b>

<div class="p"><!----></div>
<b>Resultados obtenidos</b>

 <a name="tth_chAp7"></a><h1>
Capítulo 7 <br />Ejecución del proyecto</h1>

<div class="p"><!----></div>

    


<div class="p"><!----></div>
 <a name="tth_chAp8"></a><h1>
Capítulo 8 <br />Conclusiones</h1>

<div class="p"><!----></div>

    


<div class="p"><!----></div>
En este capitulo se resumen los principales resultados, logros y conclusiones de este trabajo. Luego se enumeran las principales l&#237;neas de trabajo a futuro identificadas, entre las cuales se incluyen mejoras y extensiones al prototipo, como posibles l&#237;neas de investigaci&#243;n.

<div class="p"><!----></div>
 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Conclusiones</h2>
Se realiz&#243; una ivestigaci&#243;n en profundidad del estado del arte de las redes definidas por software(SDN) y la plataforma de hardware NetPGA, presentando un res&#250;men de los resultados obtenidos en el cap&#237;tulo 2 de este trabajo.

<div class="p"><!----></div>
Por otro lado se logr&#243; desarrollar un prototipo funcional utilizando el hardware NetFPGA y el enfoque de SDN, dotado de funcionalidades para la creaci&#243;n y gesti&#243;n de servicios de redes privadas virtuales. 

<div class="p"><!----></div>
El prototipo alcanzado se compone de un dispositivo de red denominado RAU-Switch y una aplicaci&#243;n de gesti&#243;n denominada RAUFlow. Orientado a una futura reproducci&#243;n de este trabajo, se gener&#243; un manual de construcci&#243;n de RAU-Switch donde se detallan todas las componentes de hardware y software utilizadas, junto con el procedimiento de instalaci&#243;n y configuraci&#243;n de las mismas, para obtener un dispositivo id&#233;ntico al desarrollado en este trabajo. Adem&#225;s se cre&#243; un sitio del proyecto en la plataforma Github en donde se comparte el conocimiento y experiencia generados en relaci&#243;n a las herramientas utilizadas. 

<div class="p"><!----></div>
Se logr&#243; integrar el equipo de desarrollo con la comunidad de NetFPGA a trav&#233;s de la participaci&#243;n en la lista oficial de correos; contribuyendo desde la experiencia generada en la contestaci&#243;n de dudas y reportando dos errores importantes en el proyecto ReferenceNIC. A su vez se gener&#243; un grupo de trabajo local integrando profesionales del SeCIU, Centro de Capacitaci&#243;n y Desarrollo de ANTEL y del Centro Universitario de la Regi&#243;n Este(CURE), en el cual se organizaron reuniones quincenales durante un per&#237;odo de casi ocho meses para la puesta en com&#250;n y generaci&#243;n de experiencia y conocimiento en el &#225;rea de SDN.

<div class="p"><!----></div>
Por otro lado se diseñ&#243; e implement&#243; un laboratorio de experimentaci&#243;n sobre el cual se ejecutaron una serie de pruebas orientadas a la validaci&#243;n funcional del prototipo constru&#237;do. Se implementaron dos casos de uso representativos utilizando la plataforma constru&#237;da. Ellos fueron la implementaci&#243;n de una VPN de capa 3 multipunto y una VPN de capa 2 punto a punto, sobre los cuales a su vez se ejecutaron una serie de pruebas para verificar la correcta implementaci&#243;n. De esta forma se logr&#243; validar la aplicabilidad del enfoque SDN para la construcci&#243;n de la RAU2.

<div class="p"><!----></div>
Finalmente si bien el dispositivo RAU-Switch diseñado esta fuertemente limitado en su rendimiento y performance por su implementaci&#243;n mayoritariamente en software y a pesar de que no se pudieron realizar pruebas comparativas con productos comerciales similares en funcionalidades, se logr&#243; validar la utlilzaci&#243;n de esta plataforma en la construcci&#243;n de un dispositivo compatible con OpenFlow. Adem&#225;s se identific&#243; el camino a seguir para la construcci&#243;n de un prototipo con mejores prestaciones explotando al m&#225;ximo las capacidades del hardware disponible.  

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Trabajo a futuro</h2>
En este trabajo no se compara el rendimiento del prototipo con productos comerciales similares en funcionalidades. Esto se debe a que la implementaci&#243;n del plano de datos OpenFlow se realiza por software. Interesa fuertemente extender el proyecto OpenFlow de la plataforma NetFPGA para que implemente el protocolo en la versi&#243;n 1.3 y as&#237; poder realizar una evaluaci&#243;n experimental que permita validar desde el punto de vista del rendimiento la aplicaci&#243;n de las tecnolog&#237;as utilizadas en la construcci&#243;n de la RAU2.

<div class="p"><!----></div>
El algoritmo de ruteo implementado basa su m&#233;trica solamente en el costo asociado a un enlace de red. Extender la definici&#243;n de esta m&#233;trica, contemplando otros atributos de un enlace como el ancho de banda disponible o tecnolog&#237;a (fibra &#243;ptica, enlace de cobre, etc) y a la vez contemplar restricciones como garantizar un ancho de banda m&#237;nimo, cantidad de enlaces atravesados, incluir &#243; excluir nodos por los que pasar&#225; el tr&#225;fico y demoras de extremo a extremo(en otras palabras implementar un CSPF) redituaría en la capacidad para incorporar funcionalidades de calidad de servicios.

<div class="p"><!----></div>
De la mano de la extensi&#243;n del algoritmo de ruteo a un algoritmo CSPF, se puede trabajar en el desarrollo de funcionalidades avanzadas que permitan implementar t&#233;cnicas de Ingenier&#237;a de Tr&#225;fico, desarrollando as&#237; un prototipo m&#225;s flexible en la asignaci&#243;n de los recursos disponibles y con mejor calidad en los servicios brindados.

<div class="p"><!----></div>
Un servicio queda definido en el sistema por los nodos e interfaces de entrada y salida a la red y las caracter&#237;sticas del tr&#225;fico asociado. La incorporaci&#243;n de nuevas dimensiones a la definici&#243;n de un servicio, como dimensiones de calidad de servicios(QoS) o simplemente la dimensi&#243;n tiempo representar&#237;a una gran mejora funcional y un salto de calidad en el aprovechamiento de las capacidades de la infraestructura del prototipo. Por ejemplo incorporando la dimensi&#243;n tiempo se podr&#237;an definir servicios para rangos horarios, mejorando la precisi&#243;n con la que se distribuye el ancho de banda disponible.

<div class="p"><!----></div>
En el prototipo se almacena en memoria informaci&#243;n que es ingresada por un usuario de la aplicaci&#243;n RAUFlow. En particular se almacenan datos extra de cada nodo e interfaz y se almacena la definici&#243;n de los servicios. Esta informaci&#243;n se pierde en caso de que la aplicaci&#243;n sea interrumpida puesto que no se persiste de forma no volátil. Resulta interesante entonces incorporar a la arquitectura de RAUFlow una capa de persistencia que permita guardar de forma no vol&#225;til esta informaci&#243;n, adem&#225;s de la definici&#243;n de estrategias para la reconstrucci&#243;n de servicios cuando se carga esta informaci&#243;n eventualmente en una topolog'&#237;a de red diferente a la inicial.

<div class="p"><!----></div>
Almacenar en memoria y de forma centralizada la informaci&#243;n topol&#243;gica de red, informaci&#243;n asociada a servicios y eventualmente funcionalidades de QoS e Ingenier&#237;a de tr&#225;fico, as&#237; como ejecutar algoritmos que utilicen intensivamente estos datos como un CSPF centralizado puede acarrear serias barreras de escalabilidad en el prototipo. Una l&#237;nea de investigaci&#243;n interesante ser&#237;a el desarrollo de una jeraqru&#237;a de Controladores, cada uno responsable del plano de control de una porci&#243;n de la topolog&#237;a global. De esta forma se generar&#237;an islas SDN en donde el tr&#225;fico interno es resuelto por el controlador local y se consulta al Controlador global para resolver la forma en que se enruta tr&#225;fico entre diferentes islas. A su vez se puede crecer en la cantidad de niveles dentro de la jerarqu&#237;a tanto como se quiera.

<div class="p"><!----></div>
Finalmente y no menos importante se puede trabajar en el desarrollo de nuevas funcionalidades en RAUFlow, así como en la mejora de las ya existentes. Algunas de las funcionalidades que se pueden incorporar son la capacidad para definir manualmente caminos, indicando los nodos por los que se quiere  
pasar, soportar m&#250;ltiples caminos para un mismo par de nodos origen y destino y as&#237; poder implementar balanceo de carga, creaci&#243;n de una VPN multipunto autom&#225;ticamente a partir de una lista de nodos e interfaces involucradas. 

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>
0.9

<div class="p"><!----></div>

<div class="p"><!----></div>


<div class="p"><!----></div>
<h2>References</h2>

<dl>
 <dt><a href="#CITEControllersBeacon" name="ControllersBeacon">[12014aCon]</a></dt><dd>
 (2014a).
 Beacon Controller.
 url: <a href="https://openflow.stanford.edu/display/Beacon/Home"><tt>https://openflow.stanford.edu/display/Beacon/Home</tt></a>, último
  acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersFloodlight" name="ControllersFloodlight">[22014bCon]</a></dt><dd>
 (2014b).
 Floodlight Controller.
 url: <a href="http://www.projectfloodlight.org/floodlight/"><tt>http://www.projectfloodlight.org/floodlight/</tt></a>, último
  acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersFlowvisor" name="ControllersFlowvisor">[32014cCon]</a></dt><dd>
 (2014c).
 Flowvisor Controller.
 url: <a href="https://github.com/OPENNETWORKINGLAB/flowvisor/wiki"><tt>https://github.com/OPENNETWORKINGLAB/flowvisor/wiki</tt></a>,
  último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersHelios" name="ControllersHelios">[42014dCon]</a></dt><dd>
 (2014d).
 Helios Controller.
 url: <a href="http://www.nec.com/"><tt>http://www.nec.com/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersJaxon" name="ControllersJaxon">[52014eCon]</a></dt><dd>
 (2014e).
 Jaxon Controller.
 url: <a href="http://jaxon.onuos.org/wp/?page_id=12"><tt>http://jaxon.onuos.org/wp/?page_id=12</tt></a>, último acceso
  Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersMaestro" name="ControllersMaestro">[62014fCon]</a></dt><dd>
 (2014f).
 Maestro Controller.
 url: <a href="https://code.google.com/p/maestro-platform/"><tt>https://code.google.com/p/maestro-platform/</tt></a>, último
  acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersNodeFlow" name="ControllersNodeFlow">[72014gCon]</a></dt><dd>
 (2014g).
 NodeFlow Controller.
 url: <a href="https://github.com/dreamerslab/node.flow"><tt>https://github.com/dreamerslab/node.flow</tt></a>, último acceso
  Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersNOX" name="ControllersNOX">[82014hCon]</a></dt><dd>
 (2014h).
 NOX Controller.
 url: <a href="http://www.noxrepo.org/nox/about-nox/"><tt>http://www.noxrepo.org/nox/about-nox/</tt></a>, último acceso
  Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersOpendaylight" name="ControllersOpendaylight">[92014iCon]</a></dt><dd>
 (2014i).
 OpenDaylight Controller.
 url: <a href="http://www.opendaylight.org/"><tt>http://www.opendaylight.org/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersOpenMUL" name="ControllersOpenMUL">[102014jCon]</a></dt><dd>
 (2014j).
 OpenMUL Controller.
 url: <a href="http://kulcloud.wordpress.com/"><tt>http://kulcloud.wordpress.com/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersPOX" name="ControllersPOX">[112014kCon]</a></dt><dd>
 (2014k).
 POX Controller.
 url: <a href="http://www.noxrepo.org/pox/about-pox/"><tt>http://www.noxrepo.org/pox/about-pox/</tt></a>, último acceso
  Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersRouteflow" name="ControllersRouteflow">[122014lCon]</a></dt><dd>
 (2014l).
 Routeflow Controller.
 url: <a href="http://cpqd.github.io/RouteFlow/"><tt>http://cpqd.github.io/RouteFlow/</tt></a>, último acceso Junio
  2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersSNAC" name="ControllersSNAC">[132014mCon]</a></dt><dd>
 (2014m).
 SNAC Controller.
 url: <a href="http://www.openflowhub.org/display/Snac/SNAC+Home"><tt>http://www.openflowhub.org/display/Snac/SNAC+Home</tt></a>, último
  acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersRyu" name="ControllersRyu">[142014nCon]</a></dt><dd>
 (2014n).
 SNAC Controller.
 url: <a href="http://osrg.github.io/ryu/"><tt>http://osrg.github.io/ryu/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEControllersTrema" name="ControllersTrema">[152014oCon]</a></dt><dd>
 (2014o).
 Trema Controller.
 url: <a href="http://trema.github.io/trema/"><tt>http://trema.github.io/trema/</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEAPAN" name="APAN">[162015APA]</a></dt><dd>
 (2015).
 Asia Pacific Advanced Network(APAN).
 url: <a href="https://www.apan.net/"><tt>https://www.apan.net/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEBIRD" name="BIRD">[172015BIR]</a></dt><dd>
 (2015).
 BIRD internet routing daemon.
 url: <a href="http://www.nongnu.org/quagga/"><tt>http://www.nongnu.org/quagga/</tt></a>, último acceso Junio 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITECanarie" name="Canarie">[182015Can]</a></dt><dd>
 (2015).
 CANARIE sitio oficial, CA*net4.
 url: <a href="http://www.canarie.ca/"><tt>http://www.canarie.ca/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITECentec" name="Centec">[192015Cen]</a></dt><dd>
 (2015).
 Centec Networks whitebox solutions for sdn.
 url: <a href="https://www.opennetworking.org/listings/centec-networks"><tt>https://www.opennetworking.org/listings/centec-networks</tt></a>,
  ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEGEANT" name="GEANT">[202015GEA]</a></dt><dd>
 (2015).
 GÉANT Project sitio oficial.
 url: <a href="http://www.geant.net/Pages/default.aspx"><tt>http://www.geant.net/Pages/default.aspx</tt></a>, último acceso
  Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEHP" name="HP">[212015HP]</a></dt><dd>
 (2015).
 HP productos compatibles con openflow.
 url:
  <a href="http://h17007.www1.hp.com/uy/es/solutions/technology/openflow/index.aspx"><tt>http://h17007.www1.hp.com/uy/es/solutions/technology/openflow/index.aspx</tt></a>,
  ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEInternet2" name="Internet2">[222015Int]</a></dt><dd>
 (2015).
 Internet 2 sitio oficial.
 url: <a href="http://www.internet2.edu/"><tt>http://www.internet2.edu/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITENetFPGABetaMailing" name="NetFPGABetaMailing">[232015aNet]</a></dt><dd>
 (2015a).
 Lista de correos netfpga-nf10g-beta.
 url: <a href="cl-netfpga-nf10g-beta@lists.cam.ac.uk"><tt>cl-netfpga-nf10g-beta@lists.cam.ac.uk</tt></a>, ultimo acceso Mayo
  2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITESDNProductlist" name="SDNProductlist">[242015SDN]</a></dt><dd>
 (2015).
 Lista de productos compatibles con sdn.
 url: <a href="https://www.opennetworking.org/products-listing"><tt>https://www.opennetworking.org/products-listing</tt></a>, ultimo
  acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITENetFPGA8" name="NetFPGA8">[252015bNet]</a></dt><dd>
 (2015b).
 Manual de ejecucion test rldram netfpga-10g.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/RLDRAM%20Test%20Manual"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/RLDRAM%20Test%20Manual</tt></a>,
  ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEMininet1" name="Mininet1">[262015Min]</a></dt><dd>
 (2015).
 Mininet - Entorno de emulacion virtual.
 url: <a href="http://mininet.org/"><tt>http://mininet.org/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEPCIEProgProject" name="PCIEProgProject">[272015PCI]</a></dt><dd>
 (2015).
 NetFPGA - PCIE Programming.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/PCIE-Programming"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/PCIE-Programming</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEProdTestManual" name="ProdTestManual">[282015Pro]</a></dt><dd>
 (2015).
 NetFPGA - Production Test Manual.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/Production%20Test%20Manual"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/Production%20Test%20Manual</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITENetFPGA4" name="NetFPGA4">[292015cNet]</a></dt><dd>
 (2015c).
 NetFPGA - Publicaciones academicas.
 url: <a href="http://netfpga.org/site/#/publications/"><tt>http://netfpga.org/site/#/publications/</tt></a>, último acceso
  Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITENetFPGA2" name="NetFPGA2">[302015dNet]</a></dt><dd>
 (2015d).
 NetFPGA-1G - Lista de proyectos para la plataforma.
 url:
  <a href="http://netfpga.org/site/#/systems/4netfpga-1g/applications/"><tt>http://netfpga.org/site/#/systems/4netfpga-1g/applications/</tt></a>, último
  acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITENetFPGA7" name="NetFPGA7">[312015eNet]</a></dt><dd>
 (2015e).
 NetFPGA Descripción de RLDRAM Test NetFPGA-10G.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G%20RLDRAM%20Test"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G%20RLDRAM%20Test</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITENetFPGA5" name="NetFPGA5">[322015fNet]</a></dt><dd>
 (2015f).
 NetFPGA Descripción de Test de Producción NetFPGA-10.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Production-Tes"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Production-Tes</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEReferenceSwitchProject" name="ReferenceSwitchProject">[332015aRef]</a></dt><dd>
 (2015a).
 NetFPGA Learning CAM Switch.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Learning-CAM-Switch"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Learning-CAM-Switch</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITENetFPGA6" name="NetFPGA6">[342015gNet]</a></dt><dd>
 (2015g).
 NetFPGA Manual de Test de Produción NetFPGA-10.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/Production%20Test%20Manual"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/Production%20Test%20Manual</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITENetFPGA1" name="NetFPGA1">[352015hNet]</a></dt><dd>
 (2015h).
 NetFPGA Plataform.
 url: <a href="http://netfpga.org/index.htm"><tt>http://netfpga.org/index.htm</tt></a>, último acceso Junio 2014.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEReferenceNICProject" name="ReferenceNICProject">[362015bRef]</a></dt><dd>
 (2015b).
 NetFPGA Reference NIC.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Reference-NIC"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Reference-NIC</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEReferenceRouterProject" name="ReferenceRouterProject">[372015cRef]</a></dt><dd>
 (2015c).
 NetFPGA Reference Router.
 url:
  <a href="https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Reference-Router"><tt>https://github.com/NetFPGA/NetFPGA-public/wiki/NetFPGA-10G-Reference-Router</tt></a>,
  último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEOfelia" name="Ofelia">[382015Ofe]</a></dt><dd>
 (2015).
 Ofelia openflow in europe: Linking infrastructure and applications.
 url: <a href="http://www.fp7-ofelia.eu/"><tt>http://www.fp7-ofelia.eu/</tt></a>, ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEOVSSourceCode" name="OVSSourceCode">[392015OVS]</a></dt><dd>
 (2015).
 Open vSwitch Repositorio de código fuente.
 url: <a href="https://github.com/openvswitch/ovs"><tt>https://github.com/openvswitch/ovs</tt></a>, último acceso Mayo
  2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEPica8" name="Pica8">[402015Pic]</a></dt><dd>
 (2015).
 Pica8 whitebox sdn.
 url: <a href="http://www.pica8.com/"><tt>http://www.pica8.com/</tt></a>, ultimo acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEQuagga" name="Quagga">[412015Qua]</a></dt><dd>
 (2015).
 Quagga routing suite.
 url: <a href="http://bird.network.cz/"><tt>http://bird.network.cz/</tt></a>, último acceso Junio 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITELDPRFC" name="LDPRFC">[421996Andersson et&nbsp;al.]</a></dt><dd>
Andersson, L., AB, A., and Minei, I. (1996).
 LDP Specification rfc5036.
 <em>Network Working Group</em>.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITESDNBook1" name="SDNBook1">[432013Azodolmolky]</a></dt><dd>
Azodolmolky, S. (October 2013).
 <em>Software Defined Networking with OpenFlow, Get hands on withe
  the platforms and development tools used to build OpenFlow network
  applications.</em>
 Packt Publishing, 35 Livery Street, published by packt publishing
  ltd. edition.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEcaesar2005design" name="caesar2005design">[442005Caesar et&nbsp;al.]</a></dt><dd>
Caesar, M., Caldwell, D., Feamster, N., Rexford, J., Shaikh, A., and van&nbsp;der
  Merwe, J. (2005).
 Design and implementation of a routing control platform.
 In <em>Proceedings of the 2nd conference on Symposium on Networked
  Systems Design &amp; Implementation-Volume 2</em>, pages 15-28. USENIX Association.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEcampbell1999open" name="campbell1999open">[451999Campbell et&nbsp;al.]</a></dt><dd>
Campbell, A.&nbsp;T., Katzela, I., Miki, K., and Vicente, J. (1999).
 Open signaling for atm, internet and mobile networks (opensig'98).
 <em>ACM SIGCOMM Computer Communication Review</em>, 29(1):97-108.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEcasado2007ethane" name="casado2007ethane">[462007Casado et&nbsp;al.]</a></dt><dd>
Casado, M., Freedman, M.&nbsp;J., Pettit, J., Luo, J., McKeown, N., and Shenker, S.
  (2007).
 Ethane: taking control of the enterprise.
 In <em>ACM SIGCOMM Computer Communication Review</em>, volume&nbsp;37, pages
  1-12. ACM.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEcase1989simple" name="case1989simple">[471989Case et&nbsp;al.]</a></dt><dd>
Case, J., Fedor, M., Schoffstall, M., and Davin, C. (1989).
 A simple network management protocol (snmp).

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEdoria2002general" name="doria2002general">[482002Doria et&nbsp;al.]</a></dt><dd>
Doria, A., Hellstrand, F., Sundell, K., and Worster, T. (2002).
 General switch management protocol (gsmp) v3.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEdoria2010forwarding" name="doria2010forwarding">[492010Doria et&nbsp;al.]</a></dt><dd>
Doria, A., Salim, J.&nbsp;H., Haas, R., Khosravi, H., Wang, W., Dong, L., Gopal, R.,
  and Halpern, J. (2010).
 Forwarding and control element separation (forces) protocol
  specification.
 <em>Internet Requests for Comments, RFC Editor, RFC</em>, 5810.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITErosen2006bgp" name="rosen2006bgp">[502006E.&nbsp;Rosen]</a></dt><dd>
E.&nbsp;Rosen, Y.&nbsp;R. (2006).
 Bgp/mpls ip virtual private networks (vpns).

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEfeamster2004case" name="feamster2004case">[512004Feamster et&nbsp;al.]</a></dt><dd>
Feamster, N., Balakrishnan, H., Rexford, J., Shaikh, A., and Van Der&nbsp;Merwe, J.
  (2004).
 The case for separating routing from routers.
 In <em>Proceedings of the ACM SIGCOMM workshop on Future directions
  in network architecture</em>, pages 5-12. ACM.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEStateOfArt2" name="StateOfArt2">[522013Feamster et&nbsp;al.]</a></dt><dd>
Feamster, N., Rexford, J., and Zegura, E. (2013).
 The road to sdn.
 <em>Queue</em>, 11(12):20.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEgreenberg2005clean" name="greenberg2005clean">[532005Greenberg et&nbsp;al.]</a></dt><dd>
Greenberg, A., Hjalmtysson, G., Maltz, D.&nbsp;A., Myers, A., Rexford, J., Xie, G.,
  Yan, H., Zhan, J., and Zhang, H. (2005).
 A clean slate 4d approach to network control and management.
 <em>ACM SIGCOMM Computer Communication Review</em>, 35(5):41-54.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEgude2008nox" name="gude2008nox">[542008Gude et&nbsp;al.]</a></dt><dd>
Gude, N., Koponen, T., Pettit, J., Pfaff, B., Casado, M., McKeown, N., and
  Shenker, S. (2008).
 Nox: towards an operating system for networks.
 <em>ACM SIGCOMM Computer Communication Review</em>, 38(3):105-110.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEhassas2012kandoo" name="hassas2012kandoo">[552012Hassas&nbsp;Yeganeh and Ganjali]</a></dt><dd>
Hassas&nbsp;Yeganeh, S. and Ganjali, Y. (2012).
 Kandoo: a framework for efficient and scalable offloading of control
  applications.
 In <em>Proceedings of the first workshop on Hot topics in software
  defined networks</em>, pages 19-24. ACM.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEheller2012controller" name="heller2012controller">[562012Heller et&nbsp;al.]</a></dt><dd>
Heller, B., Sherwood, R., and McKeown, N. (2012).
 The controller placement problem.
 In <em>Proceedings of the first workshop on Hot topics in software
  defined networks</em>, pages 7-12. ACM.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEjain2013b4" name="jain2013b4">[572013Jain et&nbsp;al.]</a></dt><dd>
Jain, S., Kumar, A., Mandal, S., Ong, J., Poutievski, L., Singh, A., Venkata,
  S., Wanderer, J., Zhou, J., Zhu, M., et&nbsp;al. (2013).
 B4: Experience with a globally-deployed software defined wan.
 In <em>ACM SIGCOMM Computer Communication Review</em>, volume&nbsp;43, pages
  3-14. ACM.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEkompella2007virtual" name="kompella2007virtual">[582007Kompella and Rekhter]</a></dt><dd>
Kompella, K. and Rekhter, Y. (2007).
 Virtual private lan service (vpls) using bgp for auto-discovery and
  signaling.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEkoponen2014distributed" name="koponen2014distributed">[592014Koponen et&nbsp;al.]</a></dt><dd>
Koponen, T., Casado, M., Gude, N., and Stribling, J. (2014).
 Distributed control platform for large-scale production networks.
 US Patent 8,830,823.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITElasserre2007virtual" name="lasserre2007virtual">[602007Lasserre and Kompella]</a></dt><dd>
Lasserre, M. and Kompella, V. (2007).
 Virtual private lan service (vpls) using label distribution protocol
  (ldp) signaling.
 Technical report, RFC 4762, January.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITElevin2012logically" name="levin2012logically">[612012Levin et&nbsp;al.]</a></dt><dd>
Levin, D., Wundsam, A., Heller, B., Handigol, N., and Feldmann, A. (2012).
 Logically centralized?: state distribution trade-offs in software
  defined networks.
 In <em>Proceedings of the first workshop on Hot topics in software
  defined networks</em>, pages 1-6. ACM.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEmalkin1994rip" name="malkin1994rip">[621994Malkin]</a></dt><dd>
Malkin, G. (1994).
 Rip version 2-carrying additional information.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEmckeown2008openflow" name="mckeown2008openflow">[632008McKeown et&nbsp;al.]</a></dt><dd>
McKeown, N., Anderson, T., Balakrishnan, H., Parulkar, G., Peterson, L.,
  Rexford, J., Shenker, S., and Turner, J. (2008).
 Openflow: enabling innovation in campus networks.
 <em>ACM SIGCOMM Computer Communication Review</em>, 38(2):69-74.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEONFSuccessCase" name="ONFSuccessCase">[642014Migration Working Group]</a></dt><dd>
Migration Working Group (2014).
 <em>ONF Migration Use Cases and Methods</em>.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEmoore2001towards" name="moore2001towards">[652001Moore and Nettles]</a></dt><dd>
Moore, J.&nbsp;T. and Nettles, S.&nbsp;M. (2001).
 Towards practical programmable packets.
 In <em>Proceedings of the 20th Conference on Computer Communications
  (INFOCOM). Citeseer</em>. Citeseer.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEmoy1998rfc" name="moy1998rfc">[661998Moy]</a></dt><dd>
Moy, J. (1998).
 rfc 2328: Ospf version 2.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEnadeau2004multiprotocol" name="nadeau2004multiprotocol">[672004Nadeau et&nbsp;al.]</a></dt><dd>
Nadeau, T., Srinivasan, C., Bloomberg, L., and Viswanathan, A. (2004).
 Multiprotocol label switching mpls forwarding equivalence class to
  next hop label forwarding entry fec-to-nhlfe management information base mib.
 Technical report, RFC 3814.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITESDNReadingList" name="SDNReadingList">[682015aNetFPGA]</a></dt><dd>
NetFPGA (2015a).
 Compendio de referencia de SDN.
 url: <a href="https://sites.google.com/site/sdnreadinglist/"><tt>https://sites.google.com/site/sdnreadinglist/</tt></a>, último
  acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITENetFPGA" name="NetFPGA">[692015bNetFPGA]</a></dt><dd>
NetFPGA (2015b).
 NetFPGA sitio del proyecto.
 url: <a href="http://www.netfpga.org/"><tt>http://www.netfpga.org/</tt></a>, último acceso Mayo 2015.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEStateOfArt1" name="StateOfArt1">[702014Nunes et&nbsp;al.]</a></dt><dd>
Nunes, B., Mendonca, M., Nguyen, X., Obraczka, K., and Turletti, T. (2014).
 A survey of software-defined networking: Past, present, and future of
  programmable networks.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEofv133spec" name="ofv133spec">[712012ONF]</a></dt><dd>
ONF (Agosto 2012).
 Specification openflow switch, pages 70-71.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITErexford2004network" name="rexford2004network">[722004Rexford et&nbsp;al.]</a></dt><dd>
Rexford, J., Greenberg, A., Hjalmtysson, G., Maltz, D.&nbsp;A., Myers, A., Xie, G.,
  Zhan, J., and Zhang, H. (2004).
 Network-wide decision making: Toward a wafer-thin control plane.
 In <em>Proc. HotNets</em>, pages 59-64.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITErose1990structure" name="rose1990structure">[731990Rose and McCloghrie]</a></dt><dd>
Rose, M.&nbsp;T. and McCloghrie, K. (1990).
 Structure and identification of management information for
  tcp/ip-based internets.
 <em>Structure</em>.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITErosen2001mpls" name="rosen2001mpls">[742001aRosen et&nbsp;al.]</a></dt><dd>
Rosen, E., Rekhter, Y., Tappan, D., Farinacci, D., Fedorkow, G., Li, T., and
  Conta, A. (2001a).
 Mpls label stack encoding.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITErosen2001multiprotocol" name="rosen2001multiprotocol">[752001bRosen et&nbsp;al.]</a></dt><dd>
Rosen, E., Viswanathan, A., Callon, R., et&nbsp;al. (2001b).
 Multiprotocol label switching architecture.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITErosen1999bgp" name="rosen1999bgp">[761999Rosen and Rekhter]</a></dt><dd>
Rosen, E.&nbsp;C. and Rekhter, Y. (1999).
 Bgp/mpls vpns.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEroutingprotocol" name="routingprotocol">[771990Routing]</a></dt><dd>
Routing, O. I.-I. I.-d. (1990).
 Protocol, 1990. internet engineering task force.
 Technical report, RFC 1142.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEsherwood2010carving" name="sherwood2010carving">[782010Sherwood et&nbsp;al.]</a></dt><dd>
Sherwood, R., Chan, M., Covington, A., Gibb, G., Flajslik, M., Handigol, N.,
  Huang, T.-Y., Kazemian, P., Kobayashi, M., Naous, J., et&nbsp;al. (2010).
 Carving research slices out of your production networks with
  openflow.
 <em>ACM SIGCOMM Computer Communication Review</em>, 40(1):129-130.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEtennenhouse1997survey" name="tennenhouse1997survey">[791997Tennenhouse et&nbsp;al.]</a></dt><dd>
Tennenhouse, D.&nbsp;L., Smith, J.&nbsp;M., Sincoskie, W.&nbsp;D., Wetherall, D.&nbsp;J., and
  Minden, G.&nbsp;J. (1997).
 A survey of active network research.
 <em>Communications Magazine, IEEE</em>, 35(1):80-86.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEtennenhouse2002towards" name="tennenhouse2002towards">[802002Tennenhouse and Wetherall]</a></dt><dd>
Tennenhouse, D.&nbsp;L. and Wetherall, D.&nbsp;J. (2002).
 Towards an active network architecture.
 In <em>DARPA Active NEtworks Conference and Exposition, 2002.
  Proceedings</em>, pages 2-15. IEEE.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEtootoonchian2010hyperflow" name="tootoonchian2010hyperflow">[812010Tootoonchian and Ganjali]</a></dt><dd>
Tootoonchian, A. and Ganjali, Y. (2010).
 Hyperflow: A distributed control plane for openflow.
 In <em>Proceedings of the 2010 internet network management
  conference on Research on enterprise networking</em>, pages 3-3. USENIX
  Association.

<div class="p"><!----></div>
</dd>
 <dt><a href="#CITEyu2011scalable" name="yu2011scalable">[822011Yu et&nbsp;al.]</a></dt><dd>
Yu, M., Rexford, J., Freedman, M.&nbsp;J., and Wang, J. (2011).
 Scalable flow-based networking with difane.
 <em>ACM SIGCOMM Computer Communication Review</em>, 41(4):351-362.</dd>
</dl>
 
<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>
 
<div class="p"><!----></div>

 <a name="tth_chAp9"></a><h1>
Capítulo 9 <br />Soluci&#243;n a errores encontrados en la plataforma NetFPGA</h1> 
<a name="apendiceA">
</a>

<div class="p"><!----></div>
Durante el tiempo en que se trabajo con el hardware NetFPGA, utilizando la versi&#243;n 5.0.5 del c&#243;digo fuente de la plataforma, se detectaron errores en el mismo. En particular se encontraron errores de severidad alta en relaci&#243;n al impacto que tienen en el diseño del el prototipo.<br />

<div class="p"><!----></div>
En total se detectaron dos errores, los cuales fueron reportados a la comunidad y equipo de desarrollo de NetFPGA, a través de una lista de correos[<a href="#NetFPGABetaMailing" name="CITENetFPGABetaMailing">Net, 2015a</a>]. El equipo de NetFPGA confirmo ambos errores y sugirió soluciones pasajeras a ambos problemas.

<div class="p"><!----></div>
A su vez estos errores fueron solucionados en la siguiente versi&#243;n del c&#243;digo fuente de la plataforma.

<div class="p"><!----></div>
 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Problema con la herramienta pcieprog y ReferenceNIC</h2>
El primer error detectado se encuentra en la herramienta <b>pcieprog</b> dentro de la plataforma de NetFPGA. La misma se utiliza para transferir el archivo binario generado a partir del bitfile del proyecto (en este caso ReferenceNIC) a una de las memorias flash del hardware.

<div class="p"><!----></div>
Al ejecutarse esta herramienta con el binario generado para el proyecto ReferenceNIC, la misma se ejecutaba durante horas con un 100% de utilizaci&#243;n de la CPU, sin indicios de terminar su ejecución en algún momento. Tras probar con varias ejecuciones, se realizo la misma prueba con dos proyectos adicionales(reference_ switch y reference_ router), compatibles también con la programaci&#243;n persistente. 

<div class="p"><!----></div>
Para estos proyectos la ejecucion del programa terminaba tras algunos minutos, por lo que se presumi&#243; la presencia de algún error en el c&#243;digo del proyecto ReferenceNIC. Por otro lado se realizo una inspecci&#243;n al c&#243;digo de la herramienta <b>pcieprog</b>, para determinar en que momento el programa se estancaba. Se detecto que el programa quedaba trancado en un bucle infinito dado que no se cumplía nunca la condici&#243;n de salida.<br />

<div class="p"><!----></div>
Tras reportarse este comportamiento a la lista en correos[<a href="#NetFPGABetaMailing">Net, 2015a</a>], se confirmo el error y se proporciono la siguiente solución transitoria:

<div class="p"><!----></div>

<ol type="1">
<li> Dentro del directorio donde se encuentra el c&#243;digo fuente de la plataforma, posicionarse en 
	  projects/reference_nic/hw/. Abrir el archivo system.mhs.<br />
	  Cambiar:

<div style="text-align:center">
	"PORT axi_ emc_ 0_ Mem_ DQ_ pin = axi_ emc_ 0_ Mem_ DQ, DIR = IO, VEC = [*7*:0]"
</div>
por

<div style="text-align:center">
"PORT axi_ emc_0 _Mem_ DQ_ pin = axi_ emc_ 0_ Mem_ DQ, DIR = IO, VEC = [*31*:0]"
</div>
<div class="p"><!----></div>
</li>

<li> Posicionarse ahora en projects/reference_nic/hw/. Abrir el archivo xflow.opt.<br />
	  Cambiar:

<div style="text-align:center">
"-t *1*"
</div>
por

<div style="text-align:center">
"-t *5*"
</div>
<div class="p"><!----></div>
</li>

<li> Recompilar el proyecto ReferenceNIC ejecutando el comando make en el directorio projects/reference_ nic/
<div class="p"><!----></div>
</li>

<li> Utilizar el nuevo bitfile generado para programar el hardware, generar un nuevo archivo binario y transerir a la memoria flash.
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
Una vez realizados estos cambios la herramienta pcieprog funciono correctamente, transfiriendo el contenido del archivo binario a la memoria flash.

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Error en el driver para el proyecto ReferenceNIC</h2>
El segundo error detectado trabajando con este proyecto en la modalidad de programaci&#243;n persistente  se encuentra relacionado al driver del proyecto, utilizado en la interacción entre el hardware y el sistema operativo anfitrión.<br />

<div class="p"><!----></div>
Este error fue detectado experimentalmente al realizar pruebas de funcionamiento entre varios nodos utilizando el hardware programado con el proyecto ReferenceNIC, y se detalla a continuaci&#243;n:

<div class="p"><!----></div>

<ul>
<li> <b>Descripci&#243;n:</b><br />
Al programarse el hardware con el proyecto ReferenceNIC desde la memoria flash utilizando la programaci&#243;n pcie (programación persistente), si el equipo es encendido con los enlaces de fibra óptica conectados en cada uno de los puertos físicos, el hardware deja de funcionar correctamente. Por otro lado si el equipo es encendido sin enlaces conectados a algún puerto, y luego son conectados manualmente una vez que el encendido ha terminado, el hardware funciona correctamente.
<div class="p"><!----></div>
</li>

<li> <b>Escenario:</b>

<ul>
<li> Dos PCs cada una con una tarjeta NetFPGA instalada
<div class="p"><!----></div>
</li>

<li> Las tarjetas estan conectadas por cuatro enlaces de fibra &#243;ptica (nf0-nf0, nf1-nf1, nf2-nf2 y nf3-nf3)
<div class="p"><!----></div>
</li>

<li> Ambas tarjetas estan programadas con el proyecto ReferenceNIC via programaci&#243;n pcie.
<div class="p"><!----></div>
</li>

<li> Se utiliza la versi&#243;n 5.0.5 de c&#243;digo fuente, con el parche mencionado en la sección anterior.
<div class="p"><!----></div>
</li>
</ul>
<div class="p"><!----></div>
</li>

<li> <b>Efecto:</b><br />
Incapacidad parar ejecutar exitosamente el comando ping entre cualquier par de interfaces conectadas
<div class="p"><!----></div>
</li>

<li> <b>Reproducci&#243;n:</b><br />
Partiendo de ambas PCs apagadas

<ol type="1">
<li> Encender una de las PCs, teniendo conectados los enlaces &#243;pticos entre ambas equipos
<div class="p"><!----></div>
</li>

<li> Cargar el driver y configurar manualmente direcciones IP para cada interfaz (nf0 ... nf3)
<div class="p"><!----></div>
</li>

<li> Luego realizar el mismo procedimiento para la otra PC
<div class="p"><!----></div>
</li>

<li> LEDs asociados a cada puerto en una de las tarjetas NetFPGA son encendidos por completo, mientras que en la otra tarjeta solamente la mitad de los LEDs se encuentran encendidos.
<div class="p"><!----></div>
</li>

<li> Si se apaga y enciendo la PC con la tarjeta NetFPGA cuyos LEDs se encuentran todos encendidos, luego la otra tarjeta enciende todos sus LEDs, mientras que la primera enciende solo la mitad.
<div class="p"><!----></div>
</li>

<li> El &#250;ltimo paso puede repetirse y el comportamiento observado se mantiene. En particular la tarjeta que es iniciada al final funciona correctamente mientras que la otra no.
<div class="p"><!----></div>
</li>
</ol>
<div class="p"><!----></div>
</li>

<li> <b>Soluci&#243;n:</b>

<ol type="1">
<li> Apagar ambas PCs
<div class="p"><!----></div>
</li>

<li> Desconectar todos los enlaces &#243;pticos
<div class="p"><!----></div>
</li>

<li> Encender ambas PCs(cargar el driver y configurar cada interfaz con una direcci&#243;n IP apropiada)
<div class="p"><!----></div>
</li>

<li> Luego de que cada PC esta encendida y configurada conectar los enlaces &#243;pticos uno a uno.
<div class="p"><!----></div>
</li>
</ol>
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Este reporte fue enviado a la comunidad NetFPGA, en donde se constata nuevamente la presencia de un error, esta vez en la configuraci&#243;n de los chips PHY de la tarjeta.<br />

<div class="p"><!----></div>
La soluci&#243;n propuesta por el equipo de NetFPGA es la siguiente:

<div class="p"><!----></div>

<ol type="1">
<li> Dentro del directorio donde se encuentra el c&#243;digo fuente de la plataforma, posicionarse en 
	  projects/reference_ nic/sw/host/driver/. Abrir el archivo nf10_ phy_ conf.c .
<div class="p"><!----></div>
</li>

<li> Comentar las lineas 217,219 y 240.
<div class="p"><!----></div>
</li>

<li> Recompilar el driver nf10
<div class="p"><!----></div>
</li>

<li> Reiniciar la PC y cargar el nuevo driver
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
Tras esta correcci&#243;n el proyecto ReferenceNIC funciona correctamente, comportandose el hardware como una tarjeta de red convencional. A su vez cada vez que la PC es encendida el hardware se reprograma desde la memoria flash apropiadamente.

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>


<div class="p"><!----></div>
 <a name="tth_chAp10"></a><h1>
Capítulo 10 <br />Desaf&#237;os t&#233;cnicos encontrados</h1>

<div class="p"><!----></div>

    


<div class="p"><!----></div>
Durante toda la ejecuci&#243;n del proyecto, nos enfrentamos a desaf&#237;os y en particular de carácter t&#233;cnico. Dentro de estos desaf&#237;os podemos encontrar algunos que se destacan por su complejidad y dificultad para resolverlos; y otros que se destacan por su severidad como riesgo tecnol&#243;gico dentro del proyecto. Por ello consideramos apropiado incluir un ap&#233;ndice para hacer menci&#243;n a los mismos y a sus respectivas resoluciones.

<div class="p"><!----></div>
 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Problemas con SFPs y Patchcoords</h2>

<div class="p"><!----></div>
Los tranceptores SFP+ o transceivers SFP+, son una componente de hardware utilizados en cada puerto f&#237;sico de la tarjeta NetFPGA instalada en RAU-Switch. Básicamente se encargan de implementar el mecanismo de acceso a la capa f&#237;sica, en este caso enlace de fibra &#243;ptica.<br />

<div class="p"><!----></div>
Or&#237;ginalmente se trabaja con dos tranceptores SFP+ compatibles con fibra monomodo y multimodo, y patchords de f&#237;bra &#243;ptica monomodo, con los cuales se aprecia un correcto funcionamiento del hardware. 

<div class="p"><!----></div>
Posteriormente se incorporaron m&#225;s unidades de SFP+ con el objetivo de constru&#237;r el laboratorio de pruebas descrito en el cap&#237;tulo <a href="#chapter6">6</a>. En este caso se detectan problemas para establecer conexiones exitosas para cualquier par de nodos en que se utilizara al menos uno de los SFP nuevos.<br />

<div class="p"><!----></div>
La primera reacci&#243;n es de preocupación, ante la posibilidad de que la partida de SFP comprada estuviera fallada. Por ello se menciona el inconveniente en el grupo de trabajo generado GT-SDNUy, donde amablemente los integrantes del Centro de Desarrollo y Capacitacion de ANTEL comprueban el funcionamiento del hardware utilizando sus propios equipos.

<div class="p"><!----></div>
Tras comprobar que el hardware no se encuentra dañado, advirtiendo la posibilidad de que los SFP no sean compatibles con patchcoords de fibra monomodo, el equipo de ANTEL nos facilita un par de patchcoords multimodo, con los cuales posteriormente se constata el correcto funcionamiento de los SFP.<br />

<div class="p"><!----></div>
De esta forma, el laboratorio de pruebas queda implementado con SFPs compatibles solamente con fibra multimodo, y patchcoords de fibra multimodo.

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Desprogramaci&#243;n del hardware NetFPGA</h2>
<a name="apendiceB2">
</a>

<div class="p"><!----></div>
Para la programación del hardware NetFPGA, se utiliza inicialmente el procedimiento indicado en el manual de ejecuci&#243;n del Test de Producci&#243;n &nbsp;[<a href="#ProdTestManual" name="CITEProdTestManual">Pro, 2015</a>]. Tras experimentarse un tiempo con este procedimiento y el hardware se constata que esta técnica de programación no es persistente. Esto significa que tras apagar y encender el equipo(ciclo completo de encendido) el mismo pierde la programación. 

<div class="p"><!----></div>
Utilizando la herramienta Impact, observando el comportamiento de LEDs en la placa y corroborando con el el manual de la misma, se constata que el contenido del chip CPLD no es borrado, no así el contenido del chip FPGA que si es borrado.<br />

<div class="p"><!----></div>
Con esta información se consulta la documentación de la plataforma y se buscan experiencias similares  de otros usuarios. No se encuentran problemas o experiencias similares pero se encuentra dentro de los proyectos de la plataforma un desarrollo de nombre Reference Flash, cuya funcionalidad es habilitar la programación persistente del hardware.

<div class="p"><!----></div>
Este proyecto utiliza las dos unidades de memoria flash disponibles, para almacenar la programaci&#243;n del equipo. Mientras que en la Flash A se almacena un bitstream de configuración para el equipo (bootstrap bitstream), la  Flash B queda disponible para almacenar un bitstream a elección del programador. 

<div class="p"><!----></div>
La configuración del equipo es siempre manejada por el chip CPLD, el cual en el tiempo de encendido programa al equipo con el contenido de la Flash A. Luego el equipo puede ser reprogramado desde la Flash B a través de la interfaz PCIe.

<div class="p"><!----></div>
Basándose en este proyecto, se prueba como solución al problema de la desprogramaci&#243;n del equipo dicha alternativa, utilizando como bitstream para la Flash B, el proyecto ReferenceNIC.<br />

<div class="p"><!----></div>
Esta estrategia funciona  exitosamente en relación a la desprogramaci&#243;n del hardware, puesto que tal como su documentación lo indica se programa a partir del contenido de la Flash A; y luego utilizando la interfaz PCIe es existosamente reprogramada desde la Flash B. No obstante el comportamiento final del hardware no es el correspondiente al esperado por el proyecto ReerenceNIC. En particular tras reprogramar el equipo desde la Flash B no se puede insertar el driver para el ReferenceNIC en el kernel del sistema operativo(Linux).<br />

<div class="p"><!----></div>
Tras utilizar el foro oficial de la plataforma, sin obtener respuestas se accede finalmente a una lista de correos en la que tambi&#233;n participan usuarios y desarrolladores de la plataforma. Aquí se explica el problema de la desprogrmaci&#243;n del hardware, y las alternativas exploradas. 

<div class="p"><!----></div>
Afortunadamente se obtiene respuesta del equipo de desarrolladores de NetFPGA, el cual nos indica que este comportamiento se debe a una incorrecta programaci&#243;n del hardware. Si bien se entiende correctamente la funcionalidad del proyecto ReferenceFlash, el proyecto ReferenceNIC precisa de módulos adicionales en el diseño de la CPLD que el ReferenceFlash no tiene. 

<div class="p"><!----></div>
Sin embargo, el proyecto ReferenceNIC ya tiene incorporado el modulo de persistencia que se presenta en ReferenceFlash. Por lo tanto, para lograr una programaci&#243;n persistente ademas de programar el equipo con el ReferenceNIC, se debe utilizar la herramienta <b>pcieprog</b>(dentro del mismo proyecto) para cargar en la memoria Flash A, un archivo bitstream con el contenido del mismo proyecto. De esta forma al producirse el encendido del equipo, este se programa con el contenido de la memoria Flash A.<br />

<div class="p"><!----></div>
Esta es estrategia funciona correctamente, y es lo que en este trabajo se di&#243; a conocer como programación persistente.

<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;Falta de licencias para suite de Xilinx ISE SDK</h2>
<a name="apendiceB3">
</a>

<div class="p"><!----></div>
La suite de Xilinx ISE SDK, se componen por un conjunto extenso de herramientas y entornos de desarrollo. Dentro de la suite se encuentran herramientas habilitadas para su uso bajo licencias gratuitas, otras bajo licencias de prueba (tipicamente 30 dias), y otras solamente bajo licencias pagas.<br />

<div class="p"><!----></div>
En particular en este proyecto se comienza trabajando con un paquete de licencias gratuitas. Seguir explicando ...<br />

<div class="p"><!----></div>
Por otro lado, esta estrat&#233;gia de programaci&#243;n utiliza herramientas de la suite de Xilinx que requieren de licencias pagas. Este detalle se constato experimentalmente en la ejecuci&#243;n del proyecto. Cabe destacar que la informaci&#243;n de error proporcionada por la herramienta al intentar ejecutarla sin las licencias correspondientes, asi como la documentaci&#243;n disponible no suguieren de forma intuitiva la naturaleza del problema. Por ello, teniendo la intuici&#243;n de que se trataba efectivamente de un problema de licenciamiento se solicitaron licencias adicionales a trav&#233;s del programa de apoyo universitario de Xilinx.

<div class="p"><!----></div>
Tras pasar varias semanas a la espera de una respuesta, se establecio un contacto con docentes del IIE de Facultad de Ingenieria. Desde el IIE se contesto que no trabajaban con esta plataforma pero que lo hab&#237;an hecho en un pasado; facilit&#225;ndonos un conjunto de licencias con el que se logro resolver el problema de licenciamiento. 

<div class="p"><!----></div>
Mucho tiempo despu&#233;s el programa universitario de Xilinix don&#243; un paquete de licencias entre las cuales se encontraban las necesarias.

<div class="p"><!----></div>
 <a name="tth_sEc4"></a><h2>
4&nbsp;&nbsp;Falta de driver para cable JTAG Xilinx</h2>
Si mal no recuerdo tuvimos algun problema en relacion a esto. Tengo la vaga idea de que yo queria usar algo de linux que te permite correr drivers no recuerdo si viejos.... Fa capaz me estoy mareando con lo de la wireless de asus.

<div class="p"><!----></div>
 <a name="tth_sEc5"></a><h2>
5&nbsp;&nbsp;Desaf&#237;os con Open vSwitch</h2>
<a name="apendiceB5">
</a>

<div class="p"><!----></div>
Open vSwitch es uno de los pilares fundamentales de RAU-Switch puesto que es el encargado de implementar el plano de datos de OpenFlow. Durante el desarrollo de &#233;ste proyecto, en el tiempo empleado a la experimentaci&#243;n con esta herramienta, se genera conocimiento en relaci&#243;n aspectos importantes sobre la herramienta; los cuales no son triviales y vale la pena mencionar en este apartado. A continuaci&#243;n se enumeran los siguientes:<br />

<div class="p"><!----></div>

<ol type="1">
<li> Parte de las funcionalidades de Open vSwitch, est&#225;n implementadas en modo kernel y otra parte en modo usuario. Estas &#250;ltimas presentan un nivel de performance muy pobre en comparaci&#243;n a las primera; y en particular las funcionalidades de MPLS se encuentran solamente disponibles en modo usuario.
<div class="p"><!----></div>
</li>

<li> De acuerdo a la p&#225;gina de preguntas frecuentes, la &#250;ltima versi&#243;n de Open vSwitch al momento de realizarse este proyecto, garantiza soporte a las operaciones de Match, Push y Pop de hasta tres etiquetas mpls apiladas, as&#237; como su posterior procesamiento acorde al pipe de OpenFlow. Todas estas funcionalidades adem&#225;s se implementan en modo usuario.<br />

<div class="p"><!----></div>
Por otro lado, de acuerdo  a las notas de libreaci&#243;n de la versi&#243;n 2.3.1, solamente se garantiza el soporte a las funciones de Match, Push y Pop de mpls para un &#250;nico nivel de etiquetas; esto es una sola etiqueta mpls. Luego se garantiza adem&#225;s el posterior procesamiento acorde al pipe OpenFlow.<br />

<div class="p"><!----></div>
Esta informaci&#243;n no solo es contradictoria, si no que adem&#225;s no se condice con la realidad. Experimentalmente se prueba que si bien la versi&#243;n 2.3.1 implementa correctamente las operaciones de Match y Push de una &#250;nica etiqueta mpls, la operaci&#243;n Pop no se encuentra implementada para esta versi&#243;n, rompiendo adem&#225;s el pipe de procesamiento OpenFlow. 

<div class="p"><!----></div>
Afortunadamente &#233;ste comportamiento hab&#237;a sido detectado y reportado por otros usuarios de Open vSwitch como un BUG, resolviéndose posteriormente en la versi&#243;n de desarrollo&nbsp;[<a href="#OVSSourceCode">OVS, 2015</a>].
<div class="p"><!----></div>
</li>

<li> Trabajando con la versi&#243;n m&#225;s del HEAD, en el repositorio de desarrollo de Open vSwitch&nbsp;[<a href="#OVSSourceCode">OVS, 2015</a>], se comprueba experimentalmente el soporte para las operaciones de Match, Push y Pop de hasta 3 etiquetas, as&#237; como el posterior procesamiento del paquete seg&#250;n el pipe de  OpenFlow.
<div class="p"><!----></div>
</li>

<li> Puertos OpenFlow con direcciones IP:

<div class="p"><!----></div>
Para lograr que cada interfaz f&#237;sica de la tarjeta NetFPGA se comporte tanto como un puerto OpenFlow, como una interfaz IP, es necesario:

<div class="p"><!----></div>

<ol type="a">
<li> Crear un bridge con Open vSwitch y agregar cada interaz f&#237;sica como un puerto OpenFlow al mismo.
<div class="p"><!----></div>
</li>

<li> Asignar una dirección IP a la propia intefaz f&#237;sica (por ejemplo utilizando el comando ifconfig).
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>

<div class="p"><!----></div>
El comportamiento deseado por una interfaz h&#237;brida IP/OpenFlow en el prototipo ser&#237;a el que se muestra en la figura &nbsp;<a href="#fig:OVSInterfaces">B.1</a>(mitad izquierda).

<div class="p"><!----></div>
El paquete ingresa al router atrav&#233;s de la interfaz f&#237;sica <b>nf0</b> y de ah&#237; en m&#225;s el procesamiento del mismo es delegado al m&#243;dulo de Open vSwitch en el kernel de Linux. Aqu&#237; el paquete es procesado y tratado en consecuencia a lo que indica la tabla de flujos en Open vSwitch. En el prototipo, para este paquete existen dos alternativas: (1) se procesa el contenido del mismo y se reenv&#237;a por otra interfaz acorde a la regla correspondiente, (2) es contemplado por una regla con la acci&#243;n <i>NORMAL</i> y por tanto es procesado como lo har&#237;a un switch legado(en este caso como lo procesar&#237;a el kernel de linux).<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg1">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 1: Descripci&#243;n esquematica de interfaces propuesta</div>
<a name="fig:OVSInterfaces">
</a>
</div>
<div class="p"><!----></div>
No obstante, el procesamiento del paquete al ingresar por la interfaz f&#237;sica <b>nf0</b> difiere del comportamiento deseado (ver mitad derecha de la imagen). El paquete es procesado por Open vSwitch como se describi&#243; anteriormente, pero tambi&#233;n es enviado para su procesamiento en el kernel de Linux.

<div class="p"><!----></div>
Esto implica que el switch se comporte como una mezcla de un switch OpenFlow  con una PC Linux normal, ocasionando un tratamiento de paquetes en el prototipo diferente al especificado por las reglas OpenFlow calculadas.<br />

<div class="p"><!----></div>
Para solucionar este problema se utilizan interfaces virtuales. Ficha por favor como seguimos aca......

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg2">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 2: Descripci&#243;n esquematica de interfaces propuesta</div>
<a name="fig:OVSInterfaces2">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
 <a name="tth_sEc6"></a><h2>
6&nbsp;&nbsp;MPLS Linux y Quagga LDP</h2>
<a name="apendiceB6">
</a>

<div class="p"><!----></div>
Bueno creo que es aca donde sacarnos las ganas de hablar de Quagga LDP y MPLS linux

<div class="p"><!----></div>
Que es MPLS LINUX, las dos versiones que hay, que es quagga LDP las dos versiones que hay.

<div class="p"><!----></div>
Con que se empezo, que problemas tuvimos. Poca documentacion o practicamente nada.

<div class="p"><!----></div>
Se recompila kernel, cambia configuracion, se logra configuracion intermedia.

<div class="p"><!----></div>
La version original de MPLS Linux, no andaba porque no soportaba openvswitch, no reconocia la placa etc.

<div class="p"><!----></div>
Ademas no compilaba la version que estaba en repositorio. Tenia errores de compilacion, faltaban herramientas para instalar que no existian. Muchos pero muchos problemas tecnicos.

<div class="p"><!----></div>
Se probaron kernels desde la 3.09.algo hasra la 3.11.26 creo generic, a unas versiones mas viejas. En la confuguracion del kernel no estaba disponible MPLS, Openvswitch, un asco todo.

<div class="p"><!----></div>
Se logor hacer andar el MPLS linux nuevo con el LDP nuevo.

<div class="p"><!----></div>
Se podian insertar cosas en las tablas de MPLS y el quagga por si solo andaba. No obstante no funcionaba que se insertaran desde el LDP cosas a las tablas MPLS. En particular se moria cuando intentaba insertar la primer entrada de MPLS asociada a la tabla FTN para una fec en particualr. Debugueamos el codigo, inspeccionamos y acorralamos el bug pero no pudimos solucionarlo. Se co nsidero una perdida de tiempo asi que se siguio.

<div class="p"><!----></div>
[make menuconfig]

<div class="p"><!----></div>
 <a name="tth_sEc7"></a><h2>
7&nbsp;&nbsp;Instalaci&#243;n de Sistema Operativo</h2>
<a name="B.7">
</a>

<div class="p"><!----></div>
Instalamos Fedora 14 y no booteaba. Desactivamos el uefi de la bios y tampoco. Probamos con varias versiones de Fedora incluso con un dvd de la uri y descargada de internet. Probamos con dvd y usb.

<div class="p"><!----></div>
Fedeora 20 era compatible con la mother pero no encontrabamos driver para el cable programador.

<div class="p"><!----></div>
Logramos instalar Xilinx en window XP SP3 con los drivers del cable. Aqui se podian programar las placas para las pruebas de aceptacion.

<div class="p"><!----></div>
Probamos con Ubuntu 14.04, y las placas se reconocian el driver existia pero las pruebas de aceptacion no encaraban (fallaban todas). Problemas con el DMA.

<div class="p"><!----></div>
Se probo con Ubuntu 12.04 y se logro instalar Xilinx, conseguir driver, reconocer placa y las pruebas de aceptacion ok!. 

<div class="p"><!----></div>
 <a name="tth_chAp11"></a><h1>
Capítulo 11 <br />Clasificaci&#243;n de tr&#225;fico utilizando cabezales OpenFlow</h1>
<a name="appendix3">
</a>

<div class="p"><!----></div>
En este ap&#233;ndice se detallan de los atributos presentes en el cabezal OpenFlow versi&#243;n 1.3, cuales pueden ser utilizados para la definici&#243;n de reglas y cuales pueden ser utilizados en la definici&#243;n de acciones en un flujo. Esta informaci&#243;n es de carácter experimental y esta acotada a la compatibilidad de Open vSwitch; es decir, aquellos atributos que esta herramienta no soporta o simplemente no se consiguió documentaci&#243;n para entender su utilizaci&#243;n, no son considerados.

<div class="p"><!----></div>
 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Cabezal OpenFlow versi&#243;n 1.3</h2> 

<div class="p"><!----></div>
of_v13_match_fields:

<div class="p"><!----></div>
	in_port			# Switch input port.
	in_phy_port 	# Switch physical input port. 
	metadata 		# Metadata passed between tables. 
	eth_dst 		# Ethernet destination address.
	eth_src 		# Ethernet source address. 
	eth_type 		# Ethernet frame type. 
	vlan_vID 		# VLAN id. 
	vlan_PCP		# VLAN priority. 
	IP_dscp 		# IP DSCP (6 bits in ToS field). 
	IP_ecn  		# IP ECN (2 bits in ToS field). 
	IP_proto		# IP protocol. 
	IPv4_src 		# IPv4 source address. 
	IPv4_dst 		# IPv4 destination address. 
	TCP_src 		# TCP source port. 
	TCP_dst 		# TCP destination port. 
	UDP_src 		# UDP source port. 
	UDP_dst 		# UDP destination port. 
	SCTP_src 		# SCTP source port. 
	SCTP_dst 		# SCTP destination port. 
	ICMPv4_type 	# ICMP type. 
	ICMPv4_code 	# ICMP code. 
	ARP_op			# ARP opcode. 
	ARP_spa 		# ARP source IPv4 address. 
	ARP_tpa 		# ARP target IPv4 address. 
	ARP_sha 		# ARP source hardware address. 
	ARP_tha 		# ARP target hardware address. 
	IPv6_src 		# IPv6 source address. 
	IPv6_dst 		# IPv6 destination address. 
	IPv6_flabel 	# IPv6 Flow Label 
	ICMPv6_type 	# ICMPv6 type. 
	ICMPv6_code 	# ICMPv6 code. 
	IPv6_nd_target 	# Target address for ND. 
	IPv6_nd_ssl 	# Source link-layer for ND. 
	IPv6_nd_tll  	# Target link-layer for ND. 
	MPLS_label 		# MPLS label. 
	MPLS_tc 		# MPLS TC. 
	MPLS_bos		# MPLS BoS bit. 
	PBB_is_id 		# PBB I-SID. */
	tunnel_id 		# Logical Port Metadata. 
	IPv6_txhdr 		# IPv6 Extension Header pseudo-field 

<div class="p"><!----></div>
Estos atributos fueron estudiados uno por uno y se llega a las dos listas presentadas en las siguientes secciones.

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;OpenFlow v1.3 atributos soportados para definir reglas</h2>

<div class="p"><!----></div>
A continuaci&#243;n se detallan los atributos soportados para la definici&#243;n de reglas en un flujo lo cual en otras palabras permite la construcci&#243;n de una FEC y así clasificar tr&#225;fico.

<div class="p"><!----></div>

<ul>
<li> <b>in_port</b>: Es utilizado para discriminar por el puerto de entrada en un switch
<div class="p"><!----></div>
</li>

<li> <b>metadata</b>: Es utilizado entre otras cosas para compartir informaci&#243;n entre varios flujos en diferentes tablas OpenFlow
<div class="p"><!----></div>
</li>

<li> <b>eth_dst</b>: Indica la direcci&#243;n destino de capa 2 en un paquete, direcci&#243;n MAC
<div class="p"><!----></div>
</li>

<li> <b>eth_src</b>: Indica la direcci&#243;n origen de capa 2 en un paquete, direcci&#243;n MAC
<div class="p"><!----></div>
</li>

<li> <b>dl_type</b>: Indica el ethertype de un paquete. Se utiliza el c&#243;digo hexadecimal de cada protocolo
<div class="p"><!----></div>
</li>

<li> <b>vlan_vID</b>: Utilizado para filtrar por ID de una VLAN, se utiliza el campo dl_vlan de Open vSwitch para filtrar por este valor
<div class="p"><!----></div>
</li>

<li> <b>vlan_PCP</b>: Utilizado para filtrar paquetes con el valor de prioridad de VLAN indicado, se utiliza el campo dl_vlan_pcp de Open vSwitch para filtrar por este valor
<div class="p"><!----></div>
</li>

<li> <b>IP_dscp</b>: Aparea tr&#225;fico utilizando los bits TOS en un paquete IP, se utliza el campo nw_tos en Open vSwitch y es necesario adem&#225;s indicar antes en la regla ethertype de IPv4 o IPv6, esto es dl_type 0x0800 &#243; 0x86DD
<div class="p"><!----></div>
</li>

<li> <b>IP_ecn</b>: Aparea tr&#225;fico utilizando los bits ECN en un paquete IP, se utiliza el campo nw_ecn de Open vSwitch y es necesario adem&#225;s indicar antes en la regla ethertype de IPv4(0x0800) &#243; IPv6(0x86DD)
<div class="p"><!----></div>
</li>

<li> <b>IP_proto</b>: Se utiliza campo nw_proto en Open vSwitch, indica protocolo IP utilizado. Es necesario indicar ethertype de IPv4 o IPv6
<div class="p"><!----></div>
</li>

<li> <b>IPv4_src</b>: Se utiliza campo nw_src en Open vSwitch, indica direcci&#243;n origen de protocolo IP. Es necesario indicar antes en la regla ethertype de IPv4(0x0800)
<div class="p"><!----></div>
</li>

<li> <b>IPv4_dst</b>: Se utiliza campo nw_src en Open vSwitch, indica direcci&#243;n destino de protocolo IP. Es necesario indicar antes en la regla ethertype de IPv4(0x0800)
<div class="p"><!----></div>
</li>

<li> <b>TCP_src</b>: Indica numero de puerto de aplicaci&#243;n TCP origen, se utiliza campo tp_src en Open vSwitch y es necesario indicar antes en la regla ethertype 0x0800 y nw_proto 6
<div class="p"><!----></div>
</li>

<li> <b>TCP_dst</b>: Indica numero de puerto de aplicaci&#243;n TCP destino, se utiliza campo tp_src en Open vSwitch y es necesario indicar antes en la regla ethertype 0x0800 y nw_proto 6
<div class="p"><!----></div>
</li>

<li> <b>UDP_src</b>: Indica numero de puerto de aplicaci&#243;n UDP origen, se utiliza campo tp_src en Open vSwitch y es necesario indicar antes en la regla ethertype 0x0800 y nw_proto 17
<div class="p"><!----></div>
</li>

<li> <b>UDP_dst</b>: Indica numero de puerto de aplicaci&#243;n UDP destino, se utiliza campo tp_src en Open vSwitch y es necesario indicar antes en la regla ethertype 0x0800 y nw_proto 17
<div class="p"><!----></div>
</li>

<li> <b>SCTP_src</b>: Indica numero de puerto de aplicaci&#243;n SCTP origen, se utiliza campo tp_src en Open vSwitch y es necesario indicar antes en la regla ethertype 0x0800 y nw_proto 132
<div class="p"><!----></div>
</li>

<li> <b>SCTP_dst</b>: Indica numero de puerto de aplicaci&#243;n SCTP destino, se utiliza campo tp_src en Open vSwitch y es necesario indicar antes en la regla ethertype 0x0800 y nw_proto 132
<div class="p"><!----></div>
</li>

<li> <b>ICMPv4_type</b>: Indica campo tipo en protocolo ICMP, se utiliza campo icmp_type en Open vSwitch y es necesario indicar antes en la regla ethertype 0x0800 y nw_proto 1
<div class="p"><!----></div>
</li>

<li> <b>ICMPv4_code</b>: Indica campo código en protocolo ICMP, se utiliza campo icmp_code en Open vSwitch y es necesario indicar antes en la regla ethertype 0x0800 y nw_proto 1
<div class="p"><!----></div>
</li>

<li> <b>IPv6_src</b>: Se utiliza campo ipv6_src en Open vSwitch, indica direcci&#243;n origen de protocolo IPv6. Es necesario indicar antes en la regla ethertype de IPv6(0x86DD)
<div class="p"><!----></div>
</li>

<li> <b>IPv6_dst</b>: Se utiliza campo ipv6_dst en Open vSwitch, indica direcci&#243;n destino de protocolo IPv6. Es necesario indicar antes en la regla ethertype de IPv6(0x86DD)
<div class="p"><!----></div>
</li>

<li> <b>ICMPv6_type</b>: Indica campo tipo en protocolo ICMPv6, se utiliza campo icmp_type en Open vSwitch y es necesario indicar antes en la regla ethertype 0x086DD y nw_proto 58
<div class="p"><!----></div>
</li>

<li> <b>ICMPv6_code</b>: Indica campo código en protocolo ICMPv6, se utiliza campo icmp_code en Open vSwitch y es necesario indicar antes en la regla ethertype 0x086DD y nw_proto 58
<div class="p"><!----></div>
</li>

<li> <b>MPLS_label</b>: Indica etiqueta mpls en un paquete, es necesario indicar antes en la regla el ethertype de MPLS(0x8847)
<div class="p"><!----></div>
</li>

<li> <b>MPLS_tc</b>: Indica bits experimentales tc en el cabezal mpls, es necesario indicar antes en la regla el ethertype de MPLS(0x8847)
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;OpenFlow v1.3 atributos soportados para definir acciones</h2>
A continuaci&#243;n se detallan los atributos soportados para la definici&#243;n de acciones en un flujo lo cual en otras palabras permite la manipulaci&#243;n de tr&#225;fico.

<div class="p"><!----></div>

<ul>
<li> <b>eth_src</b>: Modifica direcci&#243;n origen de capa 2, mod_dl_src:&lt;mac&#62;
<div class="p"><!----></div>
</li>

<li> <b>eth_dst</b>: Modifica direcci&#243;n destino de capa 2, mod_dl_dst:&lt;mac&#62;
<div class="p"><!----></div>
</li>

<li> <b>vlan_vID</b>: Modifica identificador de VLAN, push_vlan:&lt;ethertype&#62;,set_field:&lt;value&#62; &#243;
                         mod_vlan_vid:&lt;vlan_vid&#62;
<div class="p"><!----></div>
</li>

<li> <b>vlan_PCP</b>: Modifica valor de prioridad de VLAN, mod_vlan_pcp:&lt;vlan_pcp&#62;
<div class="p"><!----></div>
</li>

<li> <b>IP_dscp</b>: Modifica bits TOS de paquete IP, mod_nw_tos:&lt;tos&#62;
<div class="p"><!----></div>
</li>

<li> <b>IPv4_src</b>: Modifica direcci&#243;n origen IP, mod_nw_src:&lt;ip&#62;
<div class="p"><!----></div>
</li>

<li> <b>IPv4_dst</b>: Modifica direcci&#243;n destino IP, mod_nw_dst:&lt;ip&#62;
<div class="p"><!----></div>
</li>

<li> <b>TCP_src</b>: Modifica puerto TCP origen de aplicaci&#243;n, mod_tp_src:&lt;port&#62; dl
                         _type=0x0800 + nw_proto=6
<div class="p"><!----></div>
</li>

<li> <b>TCP_dst</b>: Modifica puerto TCP destino de aplicaci&#243;n , mod_tp_dst:&lt;port&#62; dl 
                         _type=0x0800 + nw_proto=6
<div class="p"><!----></div>
</li>

<li> <b>UDP_src</b>: Modifica puerto UDP origen de aplicaci&#243;n, mod_tp_src:&lt;port&#62; dl	                    
                         _type=0x0800 + nw_proto=17
<div class="p"><!----></div>
</li>

<li> <b>UDP_src</b>: Modifica puerto UDP destino de aplicaci&#243;n, mod_tp_dst:&lt;port&#62; dl	                    
                         _type=0x0800 + nw_proto=17
<div class="p"><!----></div>
</li>

<li> <b>SCTP_src</b>: Modifica puerto SCTP origen de aplicaci&#243;n, mod_tp_src:&lt;port&#62; dl	                    
                         _type=0x0800 + nw_proto=132
<div class="p"><!----></div>
</li>

<li> <b>SCTP_src</b>: Modifica puerto SCTP destino de aplicaci&#243;n, mod_tp_dst:&lt;port&#62; dl	                    
                         _type=0x0800 + nw_proto=132
<div class="p"><!----></div>
</li>

<li> <b>MPLS_label</b>: Modifica etiqueta mpls en paquete, set_field:mpls_labe + dl_type=0x8847 
							en conjunto con push_mpls, pop_mpls
<div class="p"><!----></div>
</li>

<li> <b>MPLS_tc</b>: Modifica el valor de bits experimentales mpls, dl_type=0x8847 + set_mpls_tc:&lt;value&#62;
<div class="p"><!----></div>
</li>
</ul>


<div class="p"><!----></div>
 <a name="tth_chAp12"></a><h1>
Capítulo 12 <br />Archivos de configuración</h1>

<div class="p"><!----></div>
En el presente anexo se incluyen los scripts y archivos de configuraci&#243;n de las principales herramientas utilizadas en la construcción del prototipo.

<div class="p"><!----></div>
Los siguientes archivos son tomados del PC identificado en el laboratorio de pruebas como <b>Galois</b>; los archivos de conifiguraci&#243;n de los restantes tres nodos son similares.

<div class="p"><!----></div>
 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Archivos de configuraci&#243;n Quagga</h2>
Para la configuración de Quagga, se utilizan tres archivos: el archivo <b>daemons</b> en donde se indica la configuraci&#243;n de demonios (en el caso del proyecto se activa el demonio zebra y ospfd), el archivo <b>zebra.conf</b> y el archivo <b>ospfd.conf</b>.

<div class="p"><!----></div>
<br /><br />Archivo <b>daemons</b>:

#
# Entries are in the format: &lt;daemon&#62;=(yes - no - priority)
#   0, "no"  = disabled
#   1, &#255;es" = highest priority
#   2 .. 10  = lower priorities
# Read /usr/share/doc/quagga/README.Debian for details.
#
# Sample configurations for these daemons can be found in
# /usr/share/doc/quagga/examples/.
#

<div class="p"><!----></div>
vtysh=yes
zebra=yes
bgpd=no
ospfd=yes
ospf6d=no
ripd=no
ripngd=no
isisd=no

<div class="p"><!----></div>
Archivo <b>zebra.conf</b>:

!
! Zebra configuration saved from vty
!   2015/02/05 21:26:43
!
hostname Router
password zebra
enable password zebra
log file /var/log/zebra.log
!
debug zebra kernel
!
!
interface eth0
 ipv6 nd suppress-ra
!
interface veth1
 ipv6 nd suppress-ra
!
interface lo
!
interface mpls0
 !ipv6 nd suppress-ra
!
interface vnf0
 ipv6 nd suppress-ra
!
interface vnf1
 ipv6 nd suppress-ra
!
interface vnf2
 ipv6 nd suppress-ra
!
interface vnf3
 ipv6 nd suppress-ra
!
ip forwarding
!
line vty
!


<div class="p"><!----></div>
Archivo <b>ospfd.conf</b>:

! -*- ospf -*-
!
! OSPFd sample configuration file
!
!
hostname ospfd
password zebra
!enable password please-set-at-here
!
interface l0
!
interface eth0
  ip ospf cost 65535
!
interface veth1
 ip ospf cost 1
!
interface vnf0
  ip ospf cost 1
!
interface vnf1
  ip ospf cost 3
!
interface vnf2
  ip ospf cost 1
!
interface vnf3
  ip ospf cost 1
!

<div class="p"><!----></div>
router ospf
 ospf router-id 192.168.1.11
 network 10.10.1.0/24 area 0.0.0.0
 network 10.10.4.0/24 area 0.0.0.0
 network 10.10.5.0/24 area 0.0.0.0
 network 192.168.1.0/24 area 0.0.0.0
 network 192.168.2.0/24 area 0.0.0.0
!
log stdout
!


<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Archivos de configuraci&#243;n de Open vSwitch</h2>

<div class="p"><!----></div>
Script de configuraci&#243;n inicial de ovs:

<div class="p"><!----></div>
###################################################
# Configuracion de Open vSwitch para nodo Galois  #
###################################################
#
# Declara bridge, interfaces
ovs-vsctl add-br bral
ovs-vsctl set bridge bral datapath_type=netdev
ovs-vsctl set bridge bral protocols=OpenFlow13
ovs-vsctl set bridge bral other-config:datapath-id=000000000000000B
ovs-vsctl add-port bral eth1
ovs-vsctl add-port bral nf0
ovs-vsctl add-port bral nf1
ovs-vsctl add-port bral nf2
#
# Setea por defecto el modo de falla como seguro
ovs-vsctl set-fail-mode bral secure
#
# Interfaces virtuales
ovs-vsctl add-port bral veth1 
- set interface veth1 type=internal
ovs-vsctl add-port bral vnf0 
- set interface vnf0 type=internal
ovs-vsctl add-port bral vnf1 
- set interface vnf1 type=internal
ovs-vsctl add-port bral vnf2 
- set interface vnf2 type=internal
#
# Conexion con el controlador
ovs-vsctl set-controller bral tcp:192.168.1.10:6633
#


<div class="p"><!----></div>
Script para iniciar OVS:

ovsdb-server /usr/local/etc/openvswitch/conf.db &nbsp;-remote=punix:/usr/local/var/run/openvswitch/db.sock &nbsp;-remote=db:Open_vSwitch,Open_vSwitch,manager_options &nbsp;-private-key=db:Open_vSwitch,SSL,private_key &nbsp;-certificate=db:Open_vSwitch,SSL,certificate &nbsp;-bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert &nbsp;-pidfile -detach 
-log-file=/var/log/openvswitch/openvswitch.log

<div class="p"><!----></div>
ovs-vsctl -no-wait init
ovs-vswitchd -pidfile -detach 
-log-file=/var/log/openvswitch/vswitch.log
ovs-vsctl show

<div class="p"><!----></div>
<br /><br />Script de instalaci&#243;n de flujos para bridging entre interfaces f&#237;sicas y virtuales:

ovs-ofctl -O openflow13 add-flow bral table=0,priority=0,
hard_timeout=0,idle_timeout=0, in_port=1,actions=output:5
ovs-ofctl -O openflow13 add-flow bral table=0,priority=0,
hard_timeout=0,idle_timeout=0,in_port=5,actions=output:1

<div class="p"><!----></div>
ovs-ofctl -O openflow13 add-flow bral table=0,priority=0,
hard_timeout=0,idle_timeout=0,in_port=2,actions=output:6
ovs-ofctl -O openflow13 add-flow bral table=0,priority=0,
hard_timeout=0,idle_timeout=0,in_port=6,actions=output:2

<div class="p"><!----></div>
ovs-ofctl -O openflow13 add-flow bral table=0,priority=0,
hard_timeout=0,idle_timeout=0,in_port=3,actions=output:7
ovs-ofctl -O openflow13 add-flow bral table=0,priority=0,
hard_timeout=0,idle_timeout=0,in_port=7,actions=output:3

<div class="p"><!----></div>
ovs-ofctl -O openflow13 add-flow bral table=0,priority=0,
hard_timeout=0,idle_timeout=0,in_port=4,actions=output:8
ovs-ofctl -O openflow13 add-flow bral table=0,priority=0,
hard_timeout=0,idle_timeout=0,in_port=8,actions=output:4


<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;Otros scripts de configuraci&#243;n</h2>

<div class="p"><!----></div>
Configuraci&#243;n de interfaces f&#237;sicas:


<div class="p"><!----></div>
#######################################################
# Configuracion de interfaces de Red para Host Galois #
#######################################################
#
# Interfaz para la LAN de gestion
ifconfig eth0 192.168.1.11 netmask 255.255.255.0 up
#
# las interfaces fisicas no tienen direccion IP
ifconfig eth1 0.0.0.0 up
ifconfig nf0 0.0.0.0 up
ifconfig nf1 0.0.0.0 up
ifconfig nf2 0.0.0.0 up

<div class="p"><!----></div>
<br /><br />Configuraci&#243;n de interfaces virtuales:

# Interfaces virtuales Galois 
ifconfig veth1 192.168.2.11 netmask 255.255.255.0 up
ifconfig vnf0 10.10.1.1 netmask 255.255.255.0 up
ifconfig vnf1 10.10.5.1 netmask 255.255.255.0 up
ifconfig vnf2 10.10.4.1 netmask 255.255.255.0 up



<div class="p"><!----></div>

<div class="p"><!----></div>


<div class="p"><!----></div>
 <a name="tth_chAp1"></a><h1>
Capítulo  <br />Manual de construcción de RAU-Switch</h1>

<div class="p"><!----></div>

    


<div class="p"><!----></div>

<div class="p"><!----></div>
 <a name="tth_sEc1"></a><h2>
1&nbsp;&nbsp;Resumen</h2>

<div class="p"><!----></div>
En este documento se ofrece una guía paso a paso para construir partiendo de una PC de escritorio, una tarjeta NetFPGA-10G y diferentes herramientas de software un switch OpenFlow 1.3 IP/MPLS híbrido; lo que denominamos RAUSwitch.<br />

<div class="p"><!----></div>
En la misma no se asume ningún conocimiento previo acerca de la plataforma NetFPGA, Xilinx ISE y sobre el protocolo OpenFlow.<br />

<div class="p"><!----></div>
Cabe destacar que existen diferentes tutoriales y guías para algunas de las diferentes etapas de la construcción del switch. No obstante se pensó este documento como un resumen práctico de todos ellos, de forma de agilizar el proceso de construcción de tal dispositivo para una persona que recien se encuentra familiarizándose con los conceptos aquí tratados.<br />

<div class="p"><!----></div>
En caso de querer profundizar en alguna de las etapas, se puede recurrir al material (en caso de existir) utilizado para la construcci&#243;n de esta gu&#237;a, siguiendo las referencias indicadas a lo largo de este documento.

<div class="p"><!----></div>

<div class="p"><!----></div>
 <a name="tth_sEc2"></a><h2>
2&nbsp;&nbsp;Plataforma utilizada</h2>
<a name="annexI.1">
</a>
A continuación se especifican las caracter&#237;sticas del hardware utilizado en la construcci&#243;n de RAU-Switch, as&#237; como la especificaci&#243;n de las herramientas de software con sus correspondientes versiones de producto.

<div class="p"><!----></div>
Se sugiere respetar en la medida que sea posible los mismos conforme a evitar comportamientos inesperados o no contenidos en el alcance de esta guía.<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb1">
</a> <div style="text-align:center">
<table border="1">
<tr><td colspan="1" align="center">Hardware</td></tr>
<tr><td align="left">
<ul>
<li> Procesador: Intel Core i7 4770K 3.50GHz
<div class="p"><!----></div>
</li>

<li> Motherboard Asus ROG Maximus VI Formula
<div class="p"><!----></div>
</li>

<li> Fuente Thermaltake TR2 600W ATX 12V 2.3
<div class="p"><!----></div>
</li>

<li> Memoria RAM Kingstom 8GB DIMM DDR3 1333 MHz (x2)
<div class="p"><!----></div>
</li>

<li> Disco duro Seagate 1TB
<div class="p"><!----></div>
</li>
</ul></td></tr>
<tr><td align="left">
<ul>
<li> NetFPGA-10G: 10-Gigabit SFP+ (x4), x8 gen1 PCIe, Xilinx’s Virtex-5 TX240TFPGA
<div class="p"><!----></div>
</li>

<li> Transiever Finisar FTLX8571D3BCL 850nm 13-50
	  Class 1 21CFR1040.10 LN#50 7 01
<div class="p"><!----></div>
</li>

<li> Patchcord multimodo OM3 50/125 duplex lc lc 10 metros pvc 2mm
	  il  &#8804;  0.25dB RL  &#8805;  28dB
<div class="p"><!----></div>
</li>

<li> TP-Link Gigabit PCI Express TG-3468
<div class="p"><!----></div>
</li>
</ul></td></tr></table>


<div style="text-align:center">Table 1: RAU-Switch, especificaciones de hardware</div>
<a name="table:RAUHSpecs">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_tAb2">
</a> <div style="text-align:center">
<table border="1">
<tr><td colspan="1" align="center">Software</td></tr>
<tr><td align="left">
<ul>
<li> Sistema Operativo: Ubuntu 12.04 LTS release 12.04 precise 3.11.0-15 generic x86_64
<div class="p"><!----></div>
</li>

<li> Open vSwitch v2.3.9
<div class="p"><!----></div>
</li>

<li> Repositorio NetFPGA v5.0.5
<div class="p"><!----></div>
</li>

<li> Xilinx ISE_DS_LIn 13.04_087xd.3.0
<div class="p"><!----></div>
</li>
</ul></td></tr></table>


<div style="text-align:center">Table 2: RAU-Switch, especificaciones de software</div>
<a name="table:RAUSSpecs">
</a>
</div>
<div class="p"><!----></div>
 <a name="tth_sEc3"></a><h2>
3&nbsp;&nbsp;Instalación y configuración</h2>
En esta sección se explica el proceso de instalación y configuración del software necesario para la construcción de RAUSwitch.<br />

<div class="p"><!----></div>
En cuanto al orden de procedencia esta guía está organizada de la siguiente forma:

<div class="p"><!----></div>

<ol type="1">
<li> Instalación del sistema operativo
<div class="p"><!----></div>
</li>

<li> Instalación de librerías y dependencias
<div class="p"><!----></div>
</li>

<li> Instalación de suite de desarrollo Xilinx ISE SDK
<div class="p"><!----></div>
</li>

<li> Configuración del entorno de desarrollo NetFPGA
<div class="p"><!----></div>
</li>

<li> Pruebas de aceptación del hardware NetFPGA
<div class="p"><!----></div>
</li>

<li> Programación del hardware
<div class="p"><!----></div>
</li>

<li> Instalación de Open vSwitch
<div class="p"><!----></div>
</li>

<li> Instalaci&#243;n de Quagga
<div class="p"><!----></div>
</li>

<li> Instalaci&#243;n de agente SNMP
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
     <a name="tth_sEc3.1"></a><h3>
3.1&nbsp;&nbsp;Instalación de un sistema operativo</h3>
Para la instalaci&#243;n del sistema operativo, utilizamos un dvd de instalaci&#243;n de Ubuntu 12.04 LTS descargado desde la pagina oficial. Para descargar una imagen con el instalador de este sistema se puede recurrir a:

<div class="p"><!----></div>

<div style="text-align:center">http://releases.ubuntu.com/12.04/
</div>  

<div class="p"><!----></div>
En caso de no estar familiarizado con el proceso de instalacion de un sistema operativo recomendamos el siguiente tutorial:

<div class="p"><!----></div>

<div style="text-align:center">http://www.ubuntu.com/download/desktop/install-ubuntu-desktop
</div>

<div class="p"><!----></div>
     <a name="tth_sEc3.2"></a><h3>
3.2&nbsp;&nbsp;Instalación de librerías y dependencias</h3>
Las siguientes librerías son necesarias por las diferentes herramientas de software utilizadas en el switch: gitk, git-gui, libusb, build-essential, libc6-dev, fxload, autotools-dev, autoconf, uml-utilities, libtool.<br />

<div class="p"><!----></div>
Para instalarlas abrir una consola y ejecutar los siguientes comandos:<br />

<div class="p"><!----></div>
# sudo apt-get update
# sudo apt-get install gitk git-gui libusb-dev build-essential 
# sudo apt-get install libc6-dev-i386 fxload autotools-dev
# sudo apt-get install autoconf uml-utilities libtool


<div class="p"><!----></div>
En caso de tener en algún momento errores con el comando gmake(por ejemplo “gmake comand not found”), crear un link simbólico entre <i>gmake</i> y <i>make</i>. Para ello ejecutar en una consola:<br />

<div class="p"><!----></div>
# sudo ln -s /usr/bin/make /usr/bin/gmake


<div class="p"><!----></div>
     <a name="tth_sEc3.3"></a><h3>
3.3&nbsp;&nbsp;Instalación de suite de desarrollo Xilinx ISE SDK</h3>

<div class="p"><!----></div>
La instalación de la suite Xilinx ISE SDK la realizaremos desde el código fuente por lo que primero que nada debemos descargar dichos archivos desde la página oficial de Xilinx. En este trabajo se descargo el siguiente archivo:

<div class="p"><!----></div>

<div style="text-align:center">Xilinx_ISE_DS_Lin_13.4_087xd.3.0.tar
</div>

<div class="p"><!----></div>
Luego debemos ir al directorio en donde se tienen descargado dichos archivos y ejecutamos los siguientes comandos:

<div class="p"><!----></div>
# tar -xvf Xilinx_ISE_DS_Lin_13.4_087xd.3.0.tar
# sudo chmod +x xsetup
# sudo ./xsetup


<div class="p"><!----></div>
De esta forma se ejecuta el asistente de instalación de la suite de Xillinx.<br />

<div class="p"><!----></div>
Dejamos todas las opciones que marca por defecto con la excepción de la instalación de los drivers para el cable de programación. En este caso destildar el checkbox correspondiente para evitar que la instalación se finalice con errores.<br />

<div class="p"><!----></div>
Una vez finalizada la instalación de la suite de Xilinx, debemos instalar los drivers para el cable de programación JTag. En particular trabajaremos con un driver proporcionado por terceras partes disponible en un repositorio github.<br />

<div class="p"><!----></div>
Por defecto la instalación de Xilinx se realizó en el directorio /opt/Xilinx, en caso contrario trabajar en lo que prosigue en el directorio correspondiente.<br />

<div class="p"><!----></div>
Abrimos una consola y ejecutamos los siguiente:<a href="#tthFtNtAAB" name="tthFrefAAB"><sup>1</sup></a><br />

<div class="p"><!----></div>
# cd /opt/Xilinx/
# git clone git://git.zerfleddert.de/usb-driver
# cd usb-driver/
# make
# chmod +x setup_pcusb
# ./setup_pcusb /opt/Xilinx/13.4/ISE_DS/ISE


<div class="p"><!----></div>
Finalmente reiniciamos la PC.

<div class="p"><!----></div>

<h4>Configuración de variables de entorno</h4>
Para utilizar la suite de Xilinx es necesario agregar ciertas variables de entorno al sistema. Para ello una alternativa es editar el archivo <b>.bashrc</b> en la consola del usuario root, agregando las siguientes líneas:<br />

<div class="p"><!----></div>
# sudo su
# nano /root/.bashrc

<div class="p"><!----></div>
PATH=$PATH:/opt/Xilinx/13.4/ISE_DS/EDK/bin/lin64
:/opt/Xilinx/13.4/ISE_DS/common/lib/lin64
:/opt/Xilinx/13.4/ISE_DS/ISE/bin/lin64/
:/opt/Xilinx/13.4/ISE_DS/common/bin/lin64
:/opt/Xilinx/13.4/ISE_DS/EDK/gnu/microblaze/lin64/bin

<div class="p"><!----></div>
XILINX=/opt/Xilinx/13.4/ISE_DS/ISE/
XILINX_EDK=/opt/Xilinx/13.4/ISE_DS/EDK
XILINXD_LICENSE_FILE=/opt/Xilinx
LD_LIBRARY_PATH=/opt/Xilinx/13.4/ISE_DS/ISE/lib/lin64/
:/opt/Xilinx/13.4/ISE_DS/EDK/lib/lin64

<div class="p"><!----></div>
export PATH
export XILINX
export XILINX_EDK
export XILINXD_LICENSE_FILE
export LD_LIBRARY_PATH


<div class="p"><!----></div>
De esta forma queda instalada la suite de Xilinx y los drivers del cable JTag para la programación de las tarjetas.

<div class="p"><!----></div>

<h4>Ejecución de herramientas de la suite de Xilinx</h4>
Para la ejecución de cualquier herramienta de la suite Xilinx es necesario ejecutar un script de configuración, presente en el directorio de instalación de la herramienta. Para ello abrir una consola y ejecutar lo siguiente:<br />

<div class="p"><!----></div>
# /opt/Xilinx/13.4/ISE_DS/settings64.sh


<div class="p"><!----></div>
En caso de que el archivo <b>setting64.sh</b> no cuente con permisos de ejecución asignárselos.<br />

<div class="p"><!----></div>
Luego se puede ejecutar cualquier herramienta de la suite Xilinx, como por ejemplo la herramienta Impact para programar normalmente las tarjetas de la siguiente forma:<br />

<div class="p"><!----></div>
# impact


<div class="p"><!----></div>

<h4>Instalación de licencias de usuario</h4>
Para trabajar con las herramientas de Xilinx es necesario contar con licencias correspondientes para cada una de ellas y en particular dependiendo del tipo de hardware con el que se trabaje dependerá el tipo de licencia necesaria. En nuestro caso se necesita una licencia completa(full license), y vale la pena destacar que con una licencia de prueba no se cuentan con las licencias apropiadas para compilar los proyectos de NetFPGA con los que se trabajar&#225;.<br />

<div class="p"><!----></div>
Para obtener una licencia full se debe ir al sitio de Xilinx de gestión de licencias, registrarse y comprar una licencia full.<br />

<div class="p"><!----></div>
Asumiendo que ya se tiene descargada una licencia full, para cargar la misma abrir una consola y ejecutar lo siguiente:<br />

<div class="p"><!----></div>
# /opt/Xilinx/13.4/ISE_DS/common/bin/lin64/xlcm


<div class="p"><!----></div>
Una vez que se ejecuta el comando anterior, se abrirá el programa  Xilinx Lincense Configuration Manager, que es el asistente de licencias de la suite de Xilinx. Una vez abierto el mismo ir a la pestaña de Manejo de Licencias (Manage Xilinx Licenses) e importar los archivos de licencias <em>.lic</em> obtenidos(botón copy license).

<div class="p"><!----></div>
Una vez importada la licencia se podrán ver abajo las diferentes características que proveen las licencias cargadas. En caso de que no se actualice la pestañea apretar el botón <b>Refresh</b>.

<div class="p"><!----></div>
     <a name="tth_sEc3.4"></a><h3>
3.4&nbsp;&nbsp;Configuración del entorno de desarrollo NetFPGA</h3>
A continuación se explica como acceder al código fuente de NetFPGA y como empezar a programar el hardware con el mismo.<br />

<div class="p"><!----></div>
Naturalmente lo primero que debemos hacer es obtener el código fuente con las implementaciones de proyectos y herramientas, para lo cual es necesario crearse un usuario github y luego registrarse como desarrollador NetFPGA en el sitio oficial. Este último proceso puede llevar algunas horas dado que el proceso de aceptación se realiza manualmente.<br />

<div class="p"><!----></div>
Para registrarse como desarrollador NetFPGA completar el siguiente formulario:

<div class="p"><!----></div>

<div style="text-align:center">http://netfpga.org/2014/#/10G_going_beta/
</div>

<div class="p"><!----></div>
Una vez aceptada la solicitud de unirse al equipo de NetFPGA, se tendrá acceso a los repositorios de código fuente en github. Descargamos la versión base de código fuente en el sitio de NetFPGA en github.

<div class="p"><!----></div>
Para descargar el código fuente basta con acceder al siguiente link estando logueado en Github:

<div class="p"><!----></div>

<div style="text-align:center">https://github.com/NetFPGA/NetFPGA-10G-live/tags
</div>

<div class="p"><!----></div>
En particular trabajaremos con la release 5.0.5 la cual se encuentra accesible mediante el siguiente enlace:

<div class="p"><!----></div>

<div style="text-align:center">https://github.com/NetFPGA/NetFPGA-10G-live/releases/tag/release_5.0.5
</div>

<div class="p"><!----></div>
Una vez descargado el código fuente descomprimimos el archivo .zip en un directorio a elección(en nuestro caso utilizaremos el directorio /home/mina). Luego abrimos una consola en el directorio y ejecutamos lo siguiente:

<div class="p"><!----></div>
# cd /home/mina/NetFPGA-10G-live-release_5.0.5/
# make cores


<div class="p"><!----></div>
Este proceso puede llevar de 5 a 10 minutos dependiendo de las prestaciones del hardware. Una vez finalizado el proceso anterior estamos en condiciones de programar las tarjetas NetFPGA con el proyecto que creamos conveniente.

<div class="p"><!----></div>
     <a name="tth_sEc3.5"></a><h3>
3.5&nbsp;&nbsp;Pruebas de aceptación del hardware NetFPGA</h3>
Antes de empezar a trabajar con el hardware NetFPGA y en especial a programarlo con cualquier proyecto, es sumamente importante ejecutar un conjunto de pruebas desarrolladas específicamente para la tarjeta NetFPGA-10G, con el objetivo de chequear el correcto funcionamiento de sus componentes. En particular existen dos pruebas:

<div class="p"><!----></div>

<ul>
<li> RLDRAM Test[<a href="#NetFPGA5" name="CITENetFPGA5">Net, 2015f</a>]
<div class="p"><!----></div>
</li>

<li> Production Test[<a href="#NetFPGA7" name="CITENetFPGA7">Net, 2015e</a>]
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Estos tests son proyectos de referencia; es decir fueron desarrollados por NetFPGA y la universidad de Stanford. Con los mismos se programan las tarjetas, de igual forma que se programaría cualquier otro proyecto, y luego se ejecutan una serie de scripts que realizan pruebas y verificaciones en el hardware. En pocas palabras, además de verificar el correcto funcionamiento de las tarjetas, al ejecutar los test se tiene una primera experiencia programando el hardware.<br />

<div class="p"><!----></div>
Es importante tener en cuenta que las tarjetas NetFPGA pueden funcionar tanto conectadas a una PC (escenario elegido) denominado server mode como conectadas solamente a una fuente de poder denominado standalone mode.<br />

<div class="p"><!----></div>
En particular, ambos proyectos de pruebas pueden programarse para ejecutarse en ambas modalidades de funcionamiento. En esta guía se explica como programar y ejecutar dichos proyectos bajo el modo de funcionamiento servidor. En caso de requerirse de una ejecuci&#243;n de pruebas en modo standalone puede encontrarse en[<a href="#NetFPGA6" name="CITENetFPGA6">Net, 2015g</a>][<a href="#NetFPGA8" name="CITENetFPGA8">Net, 2015b</a>] una guía detallada.

<div class="p"><!----></div>

<h4>Programación y ejecución del Production Test</h4>
A continuación se detalla el procedimiento para programar el hardware con el test de producción y luego ejecutar el conjunto de pruebas asociadas. Esta guía esta basada a su vez en una guía mas completa accesible en[<a href="#NetFPGA6">Net, 2015g</a>]<br />

<div class="p"><!----></div>
EL primer paso para programar el test de producción es configurar la tarjeta NetFPGA-10G correctamente, así como instalarla en una PC. Es sumamente IMPORTANTE seguir cuidadosamente cada una de las indicaciones que aquí se mencionan para lograr los resultados correctos y no dañar el hardware en el proceso de manipulación.<br />

<div class="p"><!----></div>
La tarjeta necesita un suministro de corriente mediante el conector ATX para poder funcionar. A continuación se describe como instalar el hardware:

<div class="p"><!----></div>

<ul>
<li> Asegurarse que el jumper J16 no esta activado. Este Jumper se encuentra en la esquina superior izquierda de la tarjeta, en la cercanía del conector RS232 DB-9(figura&nbsp;<a href="#fig:Img1">I.1</a>.

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg1">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 1: Jumper J16, Imagen extra&#237;da de [<a href="#NetFPGA6">Net, 2015g</a>]</div>
<a name="fig:Img1">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>

<li> Setear el switch SW9 en la posición PCIe. Asegurarse bien de que el switch esta completamente puesto en dicha posición(figura &nbsp;<a href="#fig:Img2">I.2</a>).

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg2">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 2: SW9 en posicion PCIe, Imagen extra&#237;da de [<a href="#NetFPGA6">Net, 2015g</a>]</div>
<a name="fig:Img2">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>

<li> Asegurarse que los switches DIP SW1, SW2, SW6 y SW10 están correctamente seteados acorde al siguiente esquema(figura &nbsp;<a href="#fig:Img3">I.3</a>):<br />

<div class="p"><!----></div>
<b>SW1:</b> SEL0, SEL1, M2, M1, M0, N2, N1, N0 son seteados en off, off, off, on, on, on, off, off<br />
<b>SW6:</b> SEL0, SEL1, M2, M1, M0, N2, N1, N0 son seteados en off, off, off, on, off on, off, on<br />
<b>SW3:</b> M0, M1, M2 son seteados en on, on, on<br />
<b>SW10:</b> SEL0, SEL1, SEL2 son seteados en on, off, off<br />

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg3">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 3: Posiciones DIP SW1 SW2 SW6 SW10, Imagen extra&#237;da de [<a href="#NetFPGA6">Net, 2015g</a>]</div>
<a name="fig:Img3">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>

<li> Con la PC apagada y desconectada de la electricidad instalar la tarjeta NetFPGA-10G en un slot PCIe disponible.
<div class="p"><!----></div>
</li>

<li> Conectar alguno de los conectores ATX disponibles de la PC al conector ATX de la tarjeta NetFPGA-10G(figura &nbsp;<a href="#fig:Img4">I.4</a>).

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg4">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 4: Conector ATX, , Imagen extra&#237;da de [<a href="#NetFPGA6">Net, 2015g</a>]</div>
<a name="fig:Img4">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>

<li> Conectar el cable Samtec Twinax a la interfaz de expansión de la tarjeta formando un “loop”( figura &nbsp;<a href="#fig:Img5">I.5</a>)

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg5">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 5: Cable samtec twinax, , Imagen extra&#237;da de [<a href="#NetFPGA6">Net, 2015g</a>]</div>
<a name="fig:Img5">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>

<li> Conectar un par de cables 10GE SFP+ a las interfaces de la tarjeta. Cada cable conecta dos puertos adyacentes formando dos “bucles”(ver figura &nbsp;<a href="#fig:Img6">I.6</a>):

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg6">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 6: Cables samtec twinax conectados</div>
<a name="fig:Img6">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>

<li> Conectar un cable serial RS232 al puerto serial RS232 en la tarjeta. Conectar el otro extremo en el puerto serial COM de la PC o a un adaptador USB por ejemplo(ver figura &nbsp;<a href="#fig:Img7">I.7</a>).

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg7">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 7: Cable seria puerto COM, Imagen extra&#237;da de [<a href="#NetFPGA6">Net, 2015g</a>]</div>
<a name="fig:Img7">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>

<li> Conectar el cable de programación Xilinx JTAG al conector JTAG en la tarjeta. Luego conectar el otro extremo a la PC a través de un puerto USB por ejemplo(figura &nbsp;<a href="#fig:Img8">I.8</a>).

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg8">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 8: Cable JTAG, Imagen extra&#237;da de [<a href="#NetFPGA6">Net, 2015g</a>]</div>
<a name="fig:Img8">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Al finalizar, la tarjeta NetFPGA debería estar instalada como se muestra en la siguientes figuras&nbsp;<a href="#fig:Img9">I.9</a>&nbsp;<a href="#fig:Img10">I.10</a>:

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg9">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 9: Tarjeta NetFPGA esquema de instalación, Imagen extra&#237;da de [<a href="#NetFPGA6">Net, 2015g</a>]</div>
<a name="fig:Img9">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg10">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 10: Tarjeta NetFPGA instalada en una PC</div>
<a name="fig:Img10">
</a>
</div>
<div class="p"><!----></div>
Para ejecutar el Test de Producción seguir los siguientes pasos:

<div class="p"><!----></div>

<ul>
<li> Encender el PC y como usuario root abrir el archivo /boot/grub/grub.cfg con un editor de texto. Realizar las siguientes modificaciones:

<div class="p"><!----></div>

<ul>
<li> Localizar la entrada particular para la distribución del kernel que se tenga
<div class="p"><!----></div>
</li>

<li> Encontrar la línea que comienza con Kernel
<div class="p"><!----></div>
</li>

<li> Concatenar <b>memmap=256M$0x5f700000</b> al final de la línea(ver figura &nbsp;<a href="#fig:Img11">I.11</a>).<br />
El parámetro memmap=256M$0x5f700000 es pasado al kernel al momento de iniciar el sistema. El mismo reserva una región de 256MB de memoria disponible comenzando en la dirección 0x5f700000. Este bloque es utilizado posteriormente en el Test PCIe DMA.
<div class="p"><!----></div>
</li>

<li> Guardar los cambios realizados al archivo

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg11">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 11: Reserva de bloque de memoria</div>
<a name="fig:Img11">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>
</ul>
<div class="p"><!----></div>
</li>

<li> Iniciar la aplicación IMPACT de Xilinx indicando la opción “Yes” cuando se pregunte si queremos que la herramienta cree y guarde automáticamente un proyecto por nosotros.
<div class="p"><!----></div>
</li>

<li> Programar la CPLD con el diseño localizado en el directorio(ver figura &nbsp;<a href="#fig:Img12">I.12</a>)<br />
/NF_ROOT/projects/production_test/bitfiles/cpld.jed

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg12">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 12: Programación Impact</div>
<a name="fig:Img12">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>

<li> Programar la FPGA con el bitstream localizado en el directorio<br />
/NF_ROOT/projects/production_test/bitfiles/prod_test_no_rldram.bit

<div class="p"><!----></div>
IMPACT posiblemente nos pregunte si queremos asociar un dispositivo PROM, seleccionar qué no. Programar la tarjeta puede llevar algún tiempo; de 10 a 20 minutos dependiendo del proyecto. Este proyecto en particular no debería tomar más de 10 minutos.
<div class="p"><!----></div>
</li>

<li> Reiniciar la PC
<div class="p"><!----></div>
</li>

<li> Verificar que el bloque de memoria DMA ha sido reservado correctamente a través de los siguientes pasos:

<div class="p"><!----></div>

<ul>
<li> Ejecutar en una consola:
    
# cat /proc/cmdline

<div class="p"><!----></div>
</li>

<li> La salida debería ser similar a la figura &nbsp;<a href="#fig:Img13">I.13</a>

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg13">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 13: Reserva bloque de memoria</div>
<a name="fig:Img13">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>
</ul>
<div class="p"><!----></div>
</li>

<li> Verificar que la tarjeta ha sido correctamente instalada siguiendo los siguientes pasos:

<div class="p"><!----></div>

<ul>
<li> Desde una consola verificar el dispositivo NetFPGA-10G ejecutando:

#lspci  -  grep Xilinx

<div class="p"><!----></div>
</li>

<li> La salida debería ser similar a:

# 01:00.0 Ethernet controller: Xilinx Corporation Device 4244

<div class="p"><!----></div>
</li>
</ul>
<div class="p"><!----></div>
</li>

<li> Desde la PC ejecutar el test de producción a través de los siguientes pasos:

<div class="p"><!----></div>

<ul>
<li> Situarse en la carpeta del Test de Producción

# cd projects/production_test/sw/

<div class="p"><!----></div>
</li>

<li> Compilar el test

# make

<div class="p"><!----></div>
</li>

<li> Posicionarse en la carpeta de scripts

# cd scripts

<div class="p"><!----></div>
</li>

<li> Ejecutar el Test de Producción

# sudo ./production_test.py


<div class="p"><!----></div>
La salida debería ser como la que se muestra a continuación(ver figura &nbsp;<a href="#fig:Img14">I.14</a>):

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg14">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 14: Test de Producción salida esperada</div>
<a name="fig:Img14">
</a>
</div>
<div class="p"><!----></div>

<div class="p"><!----></div>
</li>
</ul>
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Para ejecutar el RLDRAM Test recomendamos utilizar la guía [<a href="#NetFPGA8">Net, 2015b</a>].

<div class="p"><!----></div>
     <a name="tth_sEc3.6"></a><h3>
3.6&nbsp;&nbsp;Programación de la tarjeta</h3>

<div class="p"><!----></div>
A continuación se describen los pasos necesarios para programar las tarjetas NetFPGA en particular con el proyecto ReferenceNIC. En lo que prosigue se utiliza NF_ROOT para referirse al directorio donde se encuentra descargado el código fuente de NetFPGA.

<div class="p"><!----></div>

<h4>Configuración de la tarjeta</h4>
Antes de comenzar, es necesario realizar algunas modificaciones en el hardware NetFPGA para habilitar la programación pcie del mismo, utilizada en la programación persistente.<br />

<div class="p"><!----></div>
Para habilitar la programación PCI, es necesario cambiar la posición del switch Disp SW3 desde la posición por defecto (M0:On, M1:On, M2:On) a la posición de programación PCIE(M0:Off, M1:On, M2:On). Esta configuración habilita tanto la programación de la tarjeta por el cable de programación JTAG como por la interfaz PCIe.<br />

<div class="p"><!----></div>
La siguiente figura muestra la posición del Dip SW3 en la configuración PCIe(ver figura &nbsp;<a href="#fig:Img15">I.15</a>).

<div class="p"><!----></div>

<div class="p"><!----></div>
<a name="tth_fIg15">
</a>  <div style="text-align:center">    

<div style="text-align:center">Fig. 15: Configuración SW3 JTag/PCIe programing</div>
<a name="fig:Img15">
</a>
</div>
<div class="p"><!----></div>
Recordar que los cambios a la configuración de la tarjeta se deben realizar con la PC apagada y sin conexión a la corriente eléctrica para evitar daños en el hardware.<br />

<div class="p"><!----></div>
Una vez configurada la tarjeta conectar la PC a la corriente eléctrica y encender la misma.

<div class="p"><!----></div>

<h4>Programación de la tarjeta</h4>
Antes que nada es necesario realizar algunos cambios en archivos del código fuente asociados a los proyectos NetFPGA utilizados, arreglando algunos errores reportados para la version de código fuente utilizada en este manual. Entonces en caso de trabajar con la versión 5.0.5 (recomendada) se deben realizar las siguientes modificaciones:

<div class="p"><!----></div>

<ol type="1">
<li> En el directorio NF_ROOT/projects/reference_nic/hw/ cambiar en el archivo <i>system.mhs</i> lo siguiente:

<div class="p"><!----></div>
PORT axi_emc_0_Mem_DQ_pin = axi_emc_0_Mem_DQ, DIR = IO, 
VEC = [*7*:0]


<div class="p"><!----></div>
cambiar por

<div class="p"><!----></div>
PORT axi_emc_0_Mem_DQ_pin = axi_emc_0_Mem_DQ, DIR = IO, 
VEC = [*31*:0]

<div class="p"><!----></div>
</li>

<li> En el directorio NF_ROOT/projects/reference_nic/hw/nf10 cambiar en el archivo <i>xflow.opt</i> lo siguiente: 

<div class="p"><!----></div>
-t *1*


<div class="p"><!----></div>
cambiar por

<div class="p"><!----></div>
-t *5*


<div class="p"><!----></div>
(*) Los “*” no van
<div class="p"><!----></div>
</li>

<li> Modificar el driver utilizado por el proyecto ReferenceNIC:

<ol type="a">
<li> Posicionarse en el directorio NF_ROOT/projects/reference_nic/sw/host/driver
<div class="p"><!----></div>
</li>

<li> Abrir el archivo nf10_phy_conf.c y comentar las líneas 217,219 y 240
<div class="p"><!----></div>
</li>
</ol>
<div class="p"><!----></div>
</li>
</ol>

<div class="p"><!----></div>
Finalmente se debe compilar el proyecto. Para ello en una consola ejecutamos

<div class="p"><!----></div>
# cd NetFPGA_ROOT/projects/reference_nic
# make
# cd cpld
# make


<div class="p"><!----></div>
Este proceso puede llevar de 10 a 20 minutos.<br />

<div class="p"><!----></div>
Ahora programamos la tarjeta NetFPGA con el proyecto ReferenceNIC, para lo cual es necesario conectar el cable de programación JTag a la tarjeta y a la PC.<br />

<div class="p"><!----></div>
Luego en una consola se ejecuta lo siguiente:

<div class="p"><!----></div>
# cd NetFPGA_ROOT
# cd tools/scripts
# ./impact_run 
  ../../projects/reference_nic/bit/reference_nic.bit 
  ../../projects/reference_nic/cpld/cpld.jed 


<div class="p"><!----></div>
(*) En caso de que aparezca el error “NF_ROOT or NF_DESIGN_DIR variable is not defined”
abrir el script impact_run.sh y definir una variable de nombre NF_ROOT con el valor del directorio donde esta instalado el código fuente de NetFPGA.<br />

<div class="p"><!----></div>
Luego de finalizada la ejecución del comando reiniciar la máquina. Es sumamente importante que se reinicie y no que se apague, puesto que hasta el momento solo se ha programado el hardware en forma volátil. Esto quiere decir que producirse un corte de corriente el misma se desprogramaría.<br />

<div class="p"><!----></div>
Una vez reiniciada la PC es necesario cargar el driver del ReferenceNIC en el kernel del sistema. Para ello posiblemente primero se deba compilar el driver. Para compilar el driver ejecutar en una consola:<br />

<div class="p"><!----></div>
# cd NetFPGA_ROOT/projects/reference_nic/sw/host/driver
# make
# insmod nf10.ko


<div class="p"><!----></div>
Ahora vamos a programar una de las memorias flashes de la tarjeta (en particular utilizaremos la unidad A con el proyecto ReferenceNIC, de forma de hacer persistente la programación de la tarjeta.

<div class="p"><!----></div>
Para ello en una consola ejecutamos:

<div class="p"><!----></div>
# cd NF_ROOT/projects/reference_nic/bitfiles
# ./bit2bin.sh reference_nic.bit


<div class="p"><!----></div>
Con lo anterior se genera un archivo binario a partir del bitfile del proyecto, con el que programaremos la memoria. Luego de este comando se deberían de haber creado 3 archivos

<div class="p"><!----></div>

<ul>
<li> reference_nic.prm
<div class="p"><!----></div>
</li>

<li> reference_nic.bin
<div class="p"><!----></div>
</li>

<li> reference_nic.cfi
<div class="p"><!----></div>
</li>
</ul>

<div class="p"><!----></div>
Luego ejecutamos lo siguiente:

<div class="p"><!----></div>
# cd ../sw/host/pcieprog
# ./nf10_configure -b ../../../bitfiles/reference_nic.bin -f a


<div class="p"><!----></div>
Apagamos la PC y la volvemos a prender. Ahora el chip FPGA de la tarjeta se encuentra programado a partir del contenido de la memoria flash A, con el proyecto que allí grabamos.<br />

<div class="p"><!----></div>
Para no cargar el driver del ReferenceNIC cada vez que se reinicie la máquina podemos modificar el archivo /etc/rc.local agregando la siguiente línea:

<div class="p"><!----></div>
insmod NF_ROOT/preojects/reference_nic/sw/host/driver/nf10.ko


<div class="p"><!----></div>
     <a name="tth_sEc3.7"></a><h3>
3.7&nbsp;&nbsp;Instalación de Open vSwitch</h3>
Open vSwitch(ovs) lo instalaremos también desde su código fuente, por ejemplo ejecutando lo siguiente en una consola:

<div class="p"><!----></div>
# wget http://openvswitch.org/releases/openvswitch-2.3.0.tar.gz
# tar zxvf openvswitch-2.3.0.tar.gz
# cd openvswitch-2.3.0
#./boot.sh
#./configure -with-linux=/lib/modules/$(uname -r)/build


<div class="p"><!----></div>
Luego se compila el código fuente:

<div class="p"><!----></div>
# make </td><td width="150">
</td><td width="150">
 make install


<div class="p"><!----></div>
Se inserta en el kernel de linux el módulo de ovs

<div class="p"><!----></div>
# cd datapath/linux
# modprobe openvswitch


<div class="p"><!----></div>
Podemos verificar que el módulo se insertó correctamente de la siguiente forma:

<div class="p"><!----></div>
# lsmod  -  grep openvswitch

<div class="p"><!----></div>
openvswitch    57291      0  
gre         14236         1     openvswitch


<div class="p"><!----></div>
Luego creamos el siguiente directorio necesario para la configuración del ovs:

<div class="p"><!----></div>
# mkdir -p /usr/local/etc/openvswitch


<div class="p"><!----></div>
Finalmente creamos la base de datos de configuración de ovs:

<div class="p"><!----></div>
# ovsdb-tool create /usr/local/etc/openvswitch/conf.db 
vswitchd/vswitch.ovsschema


<div class="p"><!----></div>
Ya se esta en condiciones de levantar cualquiera de los demonios de ovs. En particular para iniciar ovs debemos indicar una serie de parámetros como la base de datos donde se encuentra la configuración, en caso de utilizar una conexión segura con el controlador los certificados SSL correspondiente entre otras cosas. Para ello es conveniente crear un script como el siguiente:

<div class="p"><!----></div>
ovsdb-server /usr/local/etc/openvswitch/conf.db &nbsp;-remote=punix:/usr/local/var/run/openvswitch/db.sock &nbsp;-remote=db:Open_vSwitch,Open_vSwitch,manager_options &nbsp;-private-key=db:Open_vSwitch,SSL,private_key &nbsp;-certificate=db:Open_vSwitch,SSL,certificate &nbsp;-bootstrap-ca-cert=db:Open_vSwitch,SSL,ca_cert 
-pidfile -detach -log-file

<div class="p"><!----></div>
ovs-vsctl -no-wait init
ovs-vswitchd -pidfile -detach
ovs-vsctl show

<div class="p"><!----></div>
Por simplicidad hemos comentado las líneas relacionadas a la parte de seguridad en la comunicación entre el ovs y el controlador.<br />

<div class="p"><!----></div>
Finalmente para automatizar la carga de los drivers de Open vSwitch al encender la PC, crear una carpeta de nombre ovs dentro del directorio /lib/modules/$(uname -r)/kernel/drivers
como se muestra a continuación.

<div class="p"><!----></div>
# mkdir /lib/modules/(uname &#8722;r)/kernel/drivers/ovs# cd /lib/modules/(uname -r)/kernel/drivers/ovs    


<div class="p"><!----></div>
Posteriormente copiar los archivos de extensión .o y .ko dentro de la carpeta de instalaci&#243;n de ovs(llamemos le OVS_ROOT) en el siguiente directorio <br />OVS_ROOT/datapath/linux al directorio anteriormente creado.<br />

<div class="p"><!----></div>
Finalmente ejecutamos el siguiente comando para actualizar las listas de dependencias para cada módulo agregado.

<div class="p"><!----></div>
# depmod


<div class="p"><!----></div>
     <a name="tth_sEc3.8"></a><h3>
3.8&nbsp;&nbsp;Instalación de software de ruteo Quagga</h3>
El software de ruteo Quagga puede instalarse con un gestor de aplicaciones(apt-get) o desde su codigo fuente; en este tutorial utilizamos esta ultima opción. 

<div class="p"><!----></div>
El código fuente podemos obtenerlo desde el sitio oficial de quagga:

<div class="p"><!----></div>

<div style="text-align:center">http://www.nongnu.org/quagga/
</div>

<div class="p"><!----></div>
o desde el siguiente link:

<div class="p"><!----></div>

<div style="text-align:center">http://www.nongnu.org/quagga/docs/docs-info.html#OSPFv2
</div>

<div class="p"><!----></div>
Una vez descargado el código fuente descomprimimos el archivo quagga.public-master.zip en el directorio deseado. En este caso utilizaremos el directorio /home/mina como venimos haciendo para el resto del software instalado. Luego abrimos una consola y ejecutamos los siguientes comandos para compilar el código fuente:

<div class="p"><!----></div>
# ./configure
# make 



<div class="p"><!----></div>
 
<div class="p"><!----></div>
<hr /><h3>Footnotes:</h3>

<div class="p"><!----></div>
<a name="tthFtNtAAB"></a><a href=" # t t h F r e f A A B ">  <sup> 1 </sup>  </a> P a r a   l a   a r q u i t e c t u r a   d e   l a   m   q u i n a   s e   d e b e   c o m p i l a r   l a   v e r s i   n   d e   6 4   b i t s   d e l   d r i v e r ;   l o   c u a l   s e   h a c e   p o r   d e f e c t o .   E n   c a s o   d e   t r a b a j a r   c o n   u n a   a r q u i t e c t u r a   d e   3 2 b i t s   s e   p u e d e   e j e c u t a r   e l   m a k e   p a r a   3 2 b i t s . 
<br /><br /><hr /><small>File translated from
T<sub><span class="small">E</span></sub>X
by <a href="http://hutchinson.belmont.ma.us/tth/">
T<sub><span class="small">T</span></sub>H</a>,
version 4.03.<br />On 13 Jun 2015, 16:22.</small>
</div></body></html>
